<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 üîß Discrete Random Variables | üÉè Probability I</title>
  <meta name="description" content="Course Materials" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 üîß Discrete Random Variables | üÉè Probability I" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course Materials" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 üîß Discrete Random Variables | üÉè Probability I" />
  
  <meta name="twitter:description" content="Course Materials" />
  

<meta name="author" content="Dr.¬†Daniel Flores Agreda (based on the Lecture by Prof.¬†Davide La Vecchia)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="axioms.html"/>
<link rel="next" href="continuousrv.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="https://www.unige.ch/gsem/fr/"><img src="img/gsem_en.png" alt="UNIGE Logo" width="200" class ="center"></a></li>
<li><a href="https://moodle.unige.ch/course/view.php?id=7133"><strong>Probability I (Spring 2021)</strong></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About this lecture</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#contents"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#practical-information"><i class="fa fa-check"></i>Practical information</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#who-we-are"><i class="fa fa-check"></i>Who we are</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#lectures"><i class="fa fa-check"></i>Lectures</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#tools"><i class="fa fa-check"></i>Tools</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#a-world-of-data"><i class="fa fa-check"></i><b>1.1</b> A World of Data</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#what-to-expect-from-this-lecture"><i class="fa fa-check"></i><b>1.2</b> What to expect from this Lecture?</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#one-intuitive-illustration"><i class="fa fa-check"></i><b>1.2.1</b> One intuitive illustration</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#a-quick-reminder-of-mathematics"><i class="fa fa-check"></i><b>1.3</b> A quick reminder of Mathematics</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#powers-and-logarithms"><i class="fa fa-check"></i><b>1.3.1</b> Powers and Logarithms</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#differentiation"><i class="fa fa-check"></i><b>1.3.2</b> Differentiation</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#integration"><i class="fa fa-check"></i><b>1.3.3</b> Integration</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction.html"><a href="introduction.html#sums"><i class="fa fa-check"></i><b>1.3.4</b> Sums</a></li>
<li class="chapter" data-level="1.3.5" data-path="introduction.html"><a href="introduction.html#combinatorics"><i class="fa fa-check"></i><b>1.3.5</b> Combinatorics</a></li>
<li class="chapter" data-level="1.3.6" data-path="introduction.html"><a href="introduction.html#limits"><i class="fa fa-check"></i><b>1.3.6</b> Limits</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="settheory.html"><a href="settheory.html"><i class="fa fa-check"></i><b>2</b> Elements of Set Theory for Probability</a>
<ul>
<li class="chapter" data-level="2.1" data-path="settheory.html"><a href="settheory.html#random-experiments-events-and-sample-spaces"><i class="fa fa-check"></i><b>2.1</b> Random Experiments, Events and Sample Spaces</a></li>
<li class="chapter" data-level="2.2" data-path="settheory.html"><a href="settheory.html#some-definitions-from-set-theory"><i class="fa fa-check"></i><b>2.2</b> Some definitions from set theory</a></li>
<li class="chapter" data-level="2.3" data-path="settheory.html"><a href="settheory.html#the-venn-diagram"><i class="fa fa-check"></i><b>2.3</b> The Venn diagram</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="settheory.html"><a href="settheory.html#sample-space-and-events"><i class="fa fa-check"></i><b>2.3.1</b> Sample Space and Events</a></li>
<li class="chapter" data-level="2.3.2" data-path="settheory.html"><a href="settheory.html#exclusive-and-non-exclusive-events"><i class="fa fa-check"></i><b>2.3.2</b> Exclusive and Non-Exclusive Events</a></li>
<li class="chapter" data-level="2.3.3" data-path="settheory.html"><a href="settheory.html#union-and-intersection-of-events"><i class="fa fa-check"></i><b>2.3.3</b> Union and Intersection of Events</a></li>
<li class="chapter" data-level="2.3.4" data-path="settheory.html"><a href="settheory.html#complement"><i class="fa fa-check"></i><b>2.3.4</b> Complement</a></li>
<li class="chapter" data-level="2.3.5" data-path="settheory.html"><a href="settheory.html#some-properties-of-union-and-intersection"><i class="fa fa-check"></i><b>2.3.5</b> Some Properties of union and intersection</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="settheory.html"><a href="settheory.html#countable-and-uncountable-sets"><i class="fa fa-check"></i><b>2.4</b> Countable and Uncountable sets</a></li>
<li class="chapter" data-level="2.5" data-path="settheory.html"><a href="settheory.html#de-morgans-laws"><i class="fa fa-check"></i><b>2.5</b> De Morgan‚Äôs Laws</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="settheory.html"><a href="settheory.html#first-law"><i class="fa fa-check"></i><b>2.5.1</b> First Law</a></li>
<li class="chapter" data-level="2.5.2" data-path="settheory.html"><a href="settheory.html#second-law"><i class="fa fa-check"></i><b>2.5.2</b> Second Law</a></li>
<li class="chapter" data-level="2.5.3" data-path="settheory.html"><a href="settheory.html#de-morgans-theorem"><i class="fa fa-check"></i><b>2.5.3</b> De Morgan‚Äôs Theorem</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="settheory.html"><a href="settheory.html#probability-as-frequency"><i class="fa fa-check"></i><b>2.6</b> Probability as Frequency</a></li>
<li class="chapter" data-level="2.7" data-path="settheory.html"><a href="settheory.html#some-references"><i class="fa fa-check"></i><b>2.7</b> Some references</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="axioms.html"><a href="axioms.html"><i class="fa fa-check"></i><b>3</b> Probability Axioms</a>
<ul>
<li class="chapter" data-level="3.1" data-path="axioms.html"><a href="axioms.html#an-axiomatic-definition-of-probability"><i class="fa fa-check"></i><b>3.1</b> An Axiomatic Definition of Probability</a></li>
<li class="chapter" data-level="3.2" data-path="axioms.html"><a href="axioms.html#properties-of-pcdot"><i class="fa fa-check"></i><b>3.2</b> Properties of <span class="math inline">\(P(\cdot)\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="axioms.html"><a href="axioms.html#examples-and-illustrations"><i class="fa fa-check"></i><b>3.3</b> Examples and Illustrations</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="axioms.html"><a href="axioms.html#flipping-coins"><i class="fa fa-check"></i><b>3.3.1</b> Flipping coins</a></li>
<li class="chapter" data-level="3.3.2" data-path="axioms.html"><a href="axioms.html#detecting-shoppers"><i class="fa fa-check"></i><b>3.3.2</b> Detecting shoppers</a></li>
<li class="chapter" data-level="3.3.3" data-path="axioms.html"><a href="axioms.html#de-morgans-law"><i class="fa fa-check"></i><b>3.3.3</b> De Morgan‚Äôs Law</a></li>
<li class="chapter" data-level="3.3.4" data-path="axioms.html"><a href="axioms.html#probability-union-and-complement"><i class="fa fa-check"></i><b>3.3.4</b> Probability, union, and complement</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="axioms.html"><a href="axioms.html#conditional-probability"><i class="fa fa-check"></i><b>3.4</b> Conditional probability</a></li>
<li class="chapter" data-level="3.5" data-path="axioms.html"><a href="axioms.html#independence"><i class="fa fa-check"></i><b>3.5</b> Independence</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="axioms.html"><a href="axioms.html#another-characterisation"><i class="fa fa-check"></i><b>3.5.1</b> Another characterisation</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="axioms.html"><a href="axioms.html#theorem-i-the-theorem-of-total-probabilities"><i class="fa fa-check"></i><b>3.6</b> Theorem I: The Theorem of Total Probabilities</a></li>
<li class="chapter" data-level="3.7" data-path="axioms.html"><a href="axioms.html#theorem-ii-bayes-theorem"><i class="fa fa-check"></i><b>3.7</b> Theorem II: Bayes‚Äô Theorem</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="axioms.html"><a href="axioms.html#guessing-in-a-multiple-choice-exam"><i class="fa fa-check"></i><b>3.7.1</b> Guessing in a multiple choice exam</a></li>
<li class="chapter" data-level="3.7.2" data-path="axioms.html"><a href="axioms.html#rent-car-maintenance"><i class="fa fa-check"></i><b>3.7.2</b> Rent car maintenance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discreterv.html"><a href="discreterv.html"><i class="fa fa-check"></i><b>4</b> üîß Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="4.1" data-path="discreterv.html"><a href="discreterv.html#what-is-a-random-variable"><i class="fa fa-check"></i><b>4.1</b> What is a Random Variable?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="discreterv.html"><a href="discreterv.html#formal-definition-of-a-random-variable"><i class="fa fa-check"></i><b>4.1.1</b> Formal definition of a random variable</a></li>
<li class="chapter" data-level="4.1.2" data-path="discreterv.html"><a href="discreterv.html#example-from-s-to-d-via-xcdot"><i class="fa fa-check"></i><b>4.1.2</b> Example: from <span class="math inline">\(S\)</span> to <span class="math inline">\(D\)</span>, via <span class="math inline">\(X(\cdot)\)</span></a></li>
<li class="chapter" data-level="4.1.3" data-path="discreterv.html"><a href="discreterv.html#an-example-from-gambling"><i class="fa fa-check"></i><b>4.1.3</b> An Example from gambling</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="discreterv.html"><a href="discreterv.html#discrete-random-variables"><i class="fa fa-check"></i><b>4.2</b> Discrete random variables</a></li>
<li class="chapter" data-level="4.3" data-path="discreterv.html"><a href="discreterv.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>4.3</b> Cumulative Distribution Function</a></li>
<li class="chapter" data-level="4.4" data-path="discreterv.html"><a href="discreterv.html#distributional-summaries-for-discrete-random-variables"><i class="fa fa-check"></i><b>4.4</b> Distributional summaries for discrete random variables</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="discreterv.html"><a href="discreterv.html#properties"><i class="fa fa-check"></i><b>4.4.1</b> Properties</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="discreterv.html"><a href="discreterv.html#dependenceindependence"><i class="fa fa-check"></i><b>4.5</b> Dependence/Independence</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="discreterv.html"><a href="discreterv.html#more-important-properties"><i class="fa fa-check"></i><b>4.5.1</b> More important properties</a></li>
<li class="chapter" data-level="4.5.2" data-path="discreterv.html"><a href="discreterv.html#more-on-expectations"><i class="fa fa-check"></i><b>4.5.2</b> More on expectations</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="discreterv.html"><a href="discreterv.html#some-discrete-distributions-of-interest"><i class="fa fa-check"></i><b>4.6</b> Some discrete distributions of interest</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="discreterv.html"><a href="discreterv.html#discrete-uniform-distribution"><i class="fa fa-check"></i><b>4.6.1</b> Discrete uniform distribution</a></li>
<li class="chapter" data-level="4.6.2" data-path="discreterv.html"><a href="discreterv.html#bernoulli-trials"><i class="fa fa-check"></i><b>4.6.2</b> Bernoulli Trials</a></li>
<li class="chapter" data-level="4.6.3" data-path="discreterv.html"><a href="discreterv.html#the-binomial-distribution"><i class="fa fa-check"></i><b>4.6.3</b> The Binomial Distribution</a></li>
<li class="chapter" data-level="4.6.4" data-path="discreterv.html"><a href="discreterv.html#poisson-distribution"><i class="fa fa-check"></i><b>4.6.4</b> Poisson Distribution</a></li>
<li class="chapter" data-level="4.6.5" data-path="discreterv.html"><a href="discreterv.html#the-hypergeometric-distribution"><i class="fa fa-check"></i><b>4.6.5</b> The Hypergeometric Distribution</a></li>
<li class="chapter" data-level="4.6.6" data-path="discreterv.html"><a href="discreterv.html#the-negative-binomial-distribution"><i class="fa fa-check"></i><b>4.6.6</b> The Negative Binomial Distribution</a></li>
<li class="chapter" data-level="4.6.7" data-path="discreterv.html"><a href="discreterv.html#illustrations-4"><i class="fa fa-check"></i><b>4.6.7</b> Illustrations</a></li>
<li class="chapter" data-level="4.6.8" data-path="discreterv.html"><a href="discreterv.html#the-geometric-distribution"><i class="fa fa-check"></i><b>4.6.8</b> The Geometric Distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="continuousrv.html"><a href="continuousrv.html"><i class="fa fa-check"></i><b>5</b> üîß Continuous Random Variable</a>
<ul>
<li class="chapter" data-level="5.1" data-path="continuousrv.html"><a href="continuousrv.html#two-motivating-examples"><i class="fa fa-check"></i><b>5.1</b> Two Motivating Examples</a></li>
<li class="chapter" data-level="5.2" data-path="continuousrv.html"><a href="continuousrv.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.2</b> Cumulative Distribution Function (CDF)</a></li>
<li class="chapter" data-level="5.3" data-path="continuousrv.html"><a href="continuousrv.html#distributional-summaries"><i class="fa fa-check"></i><b>5.3</b> Distributional Summaries</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="continuousrv.html"><a href="continuousrv.html#the-expectation"><i class="fa fa-check"></i><b>5.3.1</b> The Expectation</a></li>
<li class="chapter" data-level="5.3.2" data-path="continuousrv.html"><a href="continuousrv.html#the-variance"><i class="fa fa-check"></i><b>5.3.2</b> The Variance</a></li>
<li class="chapter" data-level="5.3.3" data-path="continuousrv.html"><a href="continuousrv.html#important-properties-of-expectations"><i class="fa fa-check"></i><b>5.3.3</b> Important properties of expectations</a></li>
<li class="chapter" data-level="5.3.4" data-path="continuousrv.html"><a href="continuousrv.html#mode-and-median"><i class="fa fa-check"></i><b>5.3.4</b> Mode and Median</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="continuousrv.html"><a href="continuousrv.html#some-important-continuous-distributions"><i class="fa fa-check"></i><b>5.4</b> Some Important Continuous Distributions</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="continuousrv.html"><a href="continuousrv.html#continuous-uniform-distribution"><i class="fa fa-check"></i><b>5.4.1</b> Continuous Uniform Distribution</a></li>
<li class="chapter" data-level="5.4.2" data-path="continuousrv.html"><a href="continuousrv.html#normal-gaussian-distribution"><i class="fa fa-check"></i><b>5.4.2</b> Normal (Gaussian) distribution</a></li>
<li class="chapter" data-level="5.4.3" data-path="continuousrv.html"><a href="continuousrv.html#the-chi-squared-distribution"><i class="fa fa-check"></i><b>5.4.3</b> The Chi-squared distribution</a></li>
<li class="chapter" data-level="5.4.4" data-path="continuousrv.html"><a href="continuousrv.html#the-student-t-distribution"><i class="fa fa-check"></i><b>5.4.4</b> The Student-t distribution</a></li>
<li class="chapter" data-level="5.4.5" data-path="continuousrv.html"><a href="continuousrv.html#the-f-distribution"><i class="fa fa-check"></i><b>5.4.5</b> The F distribution</a></li>
<li class="chapter" data-level="5.4.6" data-path="continuousrv.html"><a href="continuousrv.html#the-lognormal-distribution"><i class="fa fa-check"></i><b>5.4.6</b> The lognormal distribution</a></li>
<li class="chapter" data-level="5.4.7" data-path="continuousrv.html"><a href="continuousrv.html#exponential-distribution"><i class="fa fa-check"></i><b>5.4.7</b> Exponential distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="continuousrv.html"><a href="continuousrv.html#transformation-of-variables"><i class="fa fa-check"></i><b>5.5</b> Transformation of variables</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="continuousrv.html"><a href="continuousrv.html#transformation-of-discrete-random-variables"><i class="fa fa-check"></i><b>5.5.1</b> Transformation of discrete random variables</a></li>
<li class="chapter" data-level="5.5.2" data-path="continuousrv.html"><a href="continuousrv.html#transformation-of-variables-using-the-cdf"><i class="fa fa-check"></i><b>5.5.2</b> Transformation of variables using the CDF</a></li>
<li class="chapter" data-level="5.5.3" data-path="continuousrv.html"><a href="continuousrv.html#function-1-to-1-and-monotone-decreasing"><i class="fa fa-check"></i><b>5.5.3</b> Function 1-to-1 and monotone decreasing</a></li>
<li class="chapter" data-level="5.5.4" data-path="continuousrv.html"><a href="continuousrv.html#transformation-of-continuous-rv-through-pdf"><i class="fa fa-check"></i><b>5.5.4</b> Transformation of continuous RV through pdf</a></li>
<li class="chapter" data-level="5.5.5" data-path="continuousrv.html"><a href="continuousrv.html#example-of-transformation-using-pdf"><i class="fa fa-check"></i><b>5.5.5</b> Example of transformation using pdf</a></li>
<li class="chapter" data-level="5.5.6" data-path="continuousrv.html"><a href="continuousrv.html#a-caveat"><i class="fa fa-check"></i><b>5.5.6</b> A caveat</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="continuousrv.html"><a href="continuousrv.html#the-big-picture"><i class="fa fa-check"></i><b>5.6</b> The big picture</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="limittheorems.html"><a href="limittheorems.html"><i class="fa fa-check"></i><b>6</b> üìù Limit Theorems</a>
<ul>
<li class="chapter" data-level="6.1" data-path="limittheorems.html"><a href="limittheorems.html#sequences-of-random-variables"><i class="fa fa-check"></i><b>6.1</b> Sequences of Random Variables</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="limittheorems.html"><a href="limittheorems.html#example-bernoulli-trials-and-their-sum"><i class="fa fa-check"></i><b>6.1.1</b> Example: Bernoulli Trials and their sum</a></li>
<li class="chapter" data-level="6.1.2" data-path="limittheorems.html"><a href="limittheorems.html#example-bernoulli-trials-and-limit-behaviour"><i class="fa fa-check"></i><b>6.1.2</b> Example: Bernoulli Trials and limit behaviour</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="limittheorems.html"><a href="limittheorems.html#convergence-in-probability-oversetprightarrow"><i class="fa fa-check"></i><b>6.2</b> Convergence in Probability (<span class="math inline">\(\overset{p}{\rightarrow }\)</span>)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="limittheorems.html"><a href="limittheorems.html#operational-rules-for-oversetprightarrow"><i class="fa fa-check"></i><b>6.2.1</b> Operational Rules for <span class="math inline">\(\overset{p}{\rightarrow }\)</span></a></li>
<li class="chapter" data-level="6.2.2" data-path="limittheorems.html"><a href="limittheorems.html#convergence-of-sample-moments-as-a-motivation"><i class="fa fa-check"></i><b>6.2.2</b> Convergence of Sample Moments as a motivation‚Ä¶</a></li>
<li class="chapter" data-level="6.2.3" data-path="limittheorems.html"><a href="limittheorems.html#the-weak-law-of-large-numbers-wlln"><i class="fa fa-check"></i><b>6.2.3</b> The Weak Law of Large Numbers (WLLN)</a></li>
<li class="chapter" data-level="6.2.4" data-path="limittheorems.html"><a href="limittheorems.html#the-wlln-and-chebyshevs-inequality"><i class="fa fa-check"></i><b>6.2.4</b> The WLLN and Chebyshev‚Äôs Inequality</a></li>
<li class="chapter" data-level="6.2.5" data-path="limittheorems.html"><a href="limittheorems.html#chebyshevs-and-markovs-inequality"><i class="fa fa-check"></i><b>6.2.5</b> Chebyshev‚Äôs (and Markov‚Äôs) Inequality</a></li>
<li class="chapter" data-level="6.2.6" data-path="limittheorems.html"><a href="limittheorems.html#example-markovs-inequality"><i class="fa fa-check"></i><b>6.2.6</b> Example: Markov‚Äôs Inequality</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html"><i class="fa fa-check"></i><b>7</b> üìù Bivariate Discrete Random Variables</a>
<ul>
<li class="chapter" data-level="7.1" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#joint-probability-functions"><i class="fa fa-check"></i><b>7.1</b> Joint Probability Functions</a></li>
<li class="chapter" data-level="7.2" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#marginal-probability-mass-functions"><i class="fa fa-check"></i><b>7.2</b> Marginal probability (mass) functions</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#first-example"><i class="fa fa-check"></i><b>7.2.1</b> First Example</a></li>
<li class="chapter" data-level="7.2.2" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#empirical-example"><i class="fa fa-check"></i><b>7.2.2</b> Empirical Example</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#conditional-probability-mass-function"><i class="fa fa-check"></i><b>7.3</b> Conditional probability mass function</a></li>
<li class="chapter" data-level="7.4" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#independence-1"><i class="fa fa-check"></i><b>7.4</b> Independence</a></li>
<li class="chapter" data-level="7.5" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#expectations-for-jointly-distributed-discrete-rvs"><i class="fa fa-check"></i><b>7.5</b> Expectations for Jointly Distributed Discrete RVs</a></li>
<li class="chapter" data-level="7.6" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#iterated-expectations"><i class="fa fa-check"></i><b>7.6</b> Iterated Expectations</a></li>
<li class="chapter" data-level="7.7" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#expectations-for-jointly-distributed-discrete-rvs-1"><i class="fa fa-check"></i><b>7.7</b> Expectations for Jointly Distributed Discrete RVs</a></li>
<li class="chapter" data-level="7.8" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#some-properties-of-covariances"><i class="fa fa-check"></i><b>7.8</b> Some Properties of Covariances</a>
<ul>
<li class="chapter" data-level="7.8.1" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#a-remark"><i class="fa fa-check"></i><b>7.8.1</b> A remark</a></li>
<li class="chapter" data-level="7.8.2" data-path="bivariatediscreterv.html"><a href="bivariatediscreterv.html#an-important-property-of-correlation"><i class="fa fa-check"></i><b>7.8.2</b> An important property of correlation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="numericalmethods.html"><a href="numericalmethods.html"><i class="fa fa-check"></i><b>8</b> üìù Numerical Methods</a>
<ul>
<li class="chapter" data-level="8.1" data-path="numericalmethods.html"><a href="numericalmethods.html#introduction-to-simulation"><i class="fa fa-check"></i><b>8.1</b> Introduction to simulation</a></li>
<li class="chapter" data-level="8.2" data-path="numericalmethods.html"><a href="numericalmethods.html#simulation-procedure"><i class="fa fa-check"></i><b>8.2</b> Simulation procedure</a></li>
<li class="chapter" data-level="8.3" data-path="numericalmethods.html"><a href="numericalmethods.html#simulation-in-r"><i class="fa fa-check"></i><b>8.3</b> Simulation in R</a></li>
<li class="chapter" data-level="8.4" data-path="numericalmethods.html"><a href="numericalmethods.html#coin-tossing"><i class="fa fa-check"></i><b>8.4</b> Coin tossing</a></li>
<li class="chapter" data-level="8.5" data-path="numericalmethods.html"><a href="numericalmethods.html#summarizing"><i class="fa fa-check"></i><b>8.5</b> Summarizing</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="exercise-solutions.html"><a href="exercise-solutions.html"><i class="fa fa-check"></i><b>9</b> üìù Exercise Solutions</a>
<ul>
<li class="chapter" data-level="9.1" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-1"><i class="fa fa-check"></i><b>9.1</b> Chapter 1</a></li>
<li class="chapter" data-level="9.2" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-2"><i class="fa fa-check"></i><b>9.2</b> Chapter 2</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.1"><i class="fa fa-check"></i><b>9.2.1</b> Exercise 2.1</a></li>
<li class="chapter" data-level="9.2.2" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.2"><i class="fa fa-check"></i><b>9.2.2</b> Exercise 2.2</a></li>
<li class="chapter" data-level="9.2.3" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.3"><i class="fa fa-check"></i><b>9.2.3</b> Exercise 2.3</a></li>
<li class="chapter" data-level="9.2.4" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.4"><i class="fa fa-check"></i><b>9.2.4</b> Exercise 2.4</a></li>
<li class="chapter" data-level="9.2.5" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.5"><i class="fa fa-check"></i><b>9.2.5</b> Exercise 2.5</a></li>
<li class="chapter" data-level="9.2.6" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.6"><i class="fa fa-check"></i><b>9.2.6</b> Exercise 2.6</a></li>
<li class="chapter" data-level="9.2.7" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.7"><i class="fa fa-check"></i><b>9.2.7</b> Exercise 2.7</a></li>
<li class="chapter" data-level="9.2.8" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.8"><i class="fa fa-check"></i><b>9.2.8</b> Exercise 2.8</a></li>
<li class="chapter" data-level="9.2.9" data-path="exercise-solutions.html"><a href="exercise-solutions.html#exercise-2.8-1"><i class="fa fa-check"></i><b>9.2.9</b> Exercise 2.8</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-3"><i class="fa fa-check"></i><b>9.3</b> Chapter 3</a></li>
<li class="chapter" data-level="9.4" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-4"><i class="fa fa-check"></i><b>9.4</b> Chapter 4</a></li>
<li class="chapter" data-level="9.5" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-5"><i class="fa fa-check"></i><b>9.5</b> Chapter 5</a></li>
<li class="chapter" data-level="9.6" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-6"><i class="fa fa-check"></i><b>9.6</b> Chapter 6</a></li>
<li class="chapter" data-level="9.7" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-7"><i class="fa fa-check"></i><b>9.7</b> Chapter 7</a></li>
<li class="chapter" data-level="9.8" data-path="exercise-solutions.html"><a href="exercise-solutions.html#chapter-8"><i class="fa fa-check"></i><b>9.8</b> Chapter 8</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">üÉè Probability I</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="discreterv" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> üîß Discrete Random Variables</h1>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-1"></span>
<img src="img/fun/EC_Latin.png" alt="'Alea Acta Est' by Enrico Chavez" width="80%" />
<p class="caption">
Figure 4.1: ‚ÄòAlea Acta Est‚Äô by Enrico Chavez
</p>
</div>
<div id="what-is-a-random-variable" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> What is a Random Variable?</h2>
<p>Up until now, we have considered probabilities associated with random experiments characterised by different types of events. For instance, we‚Äôve illustrated events associated with experiments such as drawing a card (e.g.¬†the card may be ‚Äòhearts or diamonds‚Äô) or tossing a coin (e.g.¬†the coins may show two heads ‚Äò<span class="math inline">\(HH\)</span>‚Äô). This has led us to characterise events as sets and, using set theory, compute the probability of combinations of sets, (e.g.¬†an event in `<span class="math inline">\(A\cup B^{c}\)</span>‚Äô).</p>
<p>To continue further in our path of formalising the theory of probability, we shall introduce the very important notion of Random Variable and start exploring <em>Discrete</em> Random Variables.</p>

<div class="definition">
<span id="def:unnamed-chunk-2" class="definition"><strong>Definition 4.1  (Informal definition)  </strong></span>Broadly speaking, a <strong>Random Variable</strong> is a variable that takes on different <strong>numerical</strong> values with various probabilities of occurrence associated with each different outcome.
</div>
<p>Hence, to define a random variable, we need:</p>
<ol style="list-style-type: decimal">
<li>a list of all possible numerical outcomes, and</li>
<li>the probability for each numerical outcome</li>
</ol>

<div class="example">
<p><span id="exm:unnamed-chunk-3" class="example"><strong>Example 4.1  (Rolling the dice - again)  </strong></span>When we roll a single die, and record the number of dots on the top side, we can consider this the result of our draw as a Random Variable.</p>
In this case, the <strong>list of all possible outcomes</strong> of this random process is the number shown on the die i.e.¬†the possible outcomes are 1, 2, 3, 4, 5 and 6. If we say each outcome is equally likely, then the <strong>probability</strong> of each outcome must be 1/6
</div>

<div class="example">
<p><span id="exm:unnamed-chunk-4" class="example"><strong>Example 4.2  (Flipping Coins - Again)  </strong></span>If we flip a coin 10 times, and record the number of times T (tail) occurs, then the <strong>possible outcomes</strong> of the random process are:</p>
<p><span class="math display">\[\begin{equation*}
\text{0, 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10}
\end{equation*}\]</span></p>
<p>We can associate a probability to each of these numbers and the probabilities are determined by the assumptions made about the coin flips, e.g.</p>
<ul>
<li>the value probability of a ‚Äòtail‚Äô appearing on a single coin flip</li>
<li>whether this probability is the same for every coin flip</li>
<li>whether the 10 coin flips are ‚Äòindependent‚Äô of each other
</div></li>
</ul>

<div class="example">
<p><span id="exm:unnamed-chunk-5" class="example"><strong>Example 4.3  (Completing a test)  </strong></span>Suppose we want to study the time taken by school students to complete
a test. Let us assume that no student is given more than 2 hours to finish the test.</p>
<p>Here, we can define <span class="math inline">\(X=\)</span> as the completion time (in minutes), and the <strong>possible values</strong> of the random variable <span class="math inline">\(X\)</span> are contained in the interval</p>
<p><span class="math display">\[(0,120]=\{x:0&lt;x\leq 120\}.\]</span></p>
We then need to <strong>associate probabilities</strong> with all events we may wish to consider, such as
<span class="math display">\[P\left(\{ X\leq 15\}\right) \quad  \text{or} \quad P\left(\{ X&gt;60\}\right).\]</span>
</div>
<div id="formal-definition-of-a-random-variable" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Formal definition of a random variable</h3>
<p>Suppose we have:</p>
<ol style="list-style-type: lower-alpha">
<li>A sample space <span class="math inline">\(\color{green}{S}\)</span></li>
</ol>
<!-- %\item[b.] A $\sigma $-algebra generated by $S$ and denoted by $% -->
<!-- %\mathcal{B}$ -->
<ol start="2" style="list-style-type: lower-alpha">
<li>A probability measure (<span class="math inline">\(\color{green}{Pr}\)</span>) defined ‚Äúusing the events‚Äù of <span class="math inline">\(\color{green}{S}\)</span></li>
</ol>
<p>Let <span class="math inline">\(\color{blue}{X}(\color{green}{s})\)</span> be a function that takes an element <span class="math inline">\(\color{green}{s}\in S\)</span> and maps it to a number <span class="math inline">\(x\)</span></p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-6"></span>
<img src="img/04_discrete_rv/charts.010.png" alt="Schematic representation of mapping with a Random Variable" width="80%" />
<p class="caption">
Figure 4.2: Schematic representation of mapping with a Random Variable
</p>
</div>
<!-- <mark> -->
<!--   Improve this chart. -->
<!-- </mark> -->
</div>
<div id="example-from-s-to-d-via-xcdot" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Example: from <span class="math inline">\(S\)</span> to <span class="math inline">\(D\)</span>, via <span class="math inline">\(X(\cdot)\)</span></h3>

<div class="example">
<p><span id="exm:unnamed-chunk-7" class="example"><strong>Example 4.4  (Rolling two dice)  </strong></span>Consider the following <strong>Experiment</strong>: We roll two dice and we consider the number of points in the first die, and the number of points in the second die.</p>
We already know that the sample space <span class="math inline">\({\color{green}S}\)</span> is given by:
</div>
<p><img src="img/04_discrete_rv/C.png" width="554" style="display: block; margin: auto;" /></p>
<p>For the elements related to <span class="math inline">\(\color{green}S\)</span> we have a probability <span class="math inline">\(\color{green}{Pr}\)</span></p>
<p>Now define <span class="math inline">\(X(\color{green}{s_{ij}})\)</span> as the sum of the outcome <span class="math inline">\(i\)</span> of the first die and the outcome <span class="math inline">\(j\)</span> of the second die. Thus:</p>
<p><span class="math display">\[\begin{eqnarray*}
X(\color{green}{s_{ij}})= X(i,j)= i+j, &amp; \text{ for } &amp; i=1,...,6,  \text{ and }   j=1,...,6 
\end{eqnarray*}\]</span></p>
<p>In this notation <span class="math inline">\(\color{green}{s_{ij}=(i,j)}\)</span> and <span class="math inline">\(\color{green}{s_{ij}\in S}\)</span>, each having probability <span class="math inline">\(1/36\)</span>.</p>
<p>Let us proceed to formalise this setting with a <strong>Random Variable</strong> and make the mapping explicit:</p>
<ul>
<li><span class="math inline">\(X(\cdot)\)</span> maps <span class="math inline">\(\color{green}{S}\)</span> into <span class="math inline">\(\color{blue}D\)</span>. The (new) sample space <span class="math inline">\(\color{blue}{D}\)</span> is given by:
<span class="math display">\[\begin{equation*}
\color{blue}{D=\left\{2,3,4,5,6,7,8,9,10,11,12\right\}}
\end{equation*}\]</span>
where, e.g., <span class="math inline">\(\color{blue}{2}\)</span> is related to the pair <span class="math inline">\((1,1)\)</span>, <span class="math inline">\(\color{blue}{3}\)</span> is related to the pairs <span class="math inline">\((1,2)\)</span> and <span class="math inline">\((2,1)\)</span>, etc etc. So <span class="math inline">\(\color{blue}{D}\)</span> is related the new <span class="math inline">\(\color{blue}{P}\)</span>
<!-- triplet $(\color{blue}{D},\mathcal{B}_{\color{blue}{D}},P)$ . --></li>
<li>To each element (event) in <span class="math inline">\(\color{blue}{D}\)</span> we can attach a probability, using the probability of the corresponding event(s) in <span class="math inline">\(S\)</span>. For instance,
<span class="math display">\[P(\color{blue}{2})=Pr(1,1)=1/36, \quad \text{or} \quad P(\color{blue}{3})=Pr(1,2)+Pr(2,1)=2/36.\]</span></li>
<li>How about the <span class="math inline">\(P(\color{blue}{7})\)</span>?<br />
<span class="math display">\[\begin{equation*}
P(\color{blue}7)=Pr(3,4)+Pr(2,5)+Pr(1,6)+Pr(4,3)+Pr(5,2)+Pr(6,1)=6/36. 
\end{equation*}\]</span></li>
<li>The latter equality can also be re-written as
<span class="math display">\[P(\color{blue}7)=2(Pr(3,4)+Pr(2,5)+Pr(1,6))=6 \ Pr(3,4),\]</span></li>
</ul>

<div class="exercise">
<span id="exr:unnamed-chunk-9" class="exercise"><strong>Exercise 4.1  </strong></span>1. What is <span class="math inline">\(P(\color{blue}{9})\)</span>?
2. What is <span class="math inline">\(P(\color{blue} 13 )\)</span>? [Hint: does <span class="math inline">\(\color{blue}{13}\)</span> belong to <span class="math inline">\(\color{blue}{D}\)</span>?]
</div>
<!-- ## Formal definition of a random variable (II) -->
<p>Let us formalise all these ideas:</p>

<div class="definition">
<p><span id="def:unnamed-chunk-10" class="definition"><strong>Definition 4.2  (A more formal definition)  </strong></span>
Let <span class="math inline">\(D\)</span> be the set of all values <span class="math inline">\(x\)</span> that can be obtained by <span class="math inline">\(X\left(s\right)\)</span>, for all <span class="math inline">\(s\in S\)</span>:
<span class="math display">\[\begin{equation*}
D=\left\{ x:x=X\left( s\right) ,\text{ }s\in S\right\} 
\end{equation*}\]</span>
<span class="math inline">\(D\)</span> is a list of all possible numbers <span class="math inline">\(x\)</span> that can be obtained, and thus is a <strong>sample space</strong> for <strong><span class="math inline">\(X\)</span></strong>.</p>
<blockquote>
<p>Notice that the random variable is <span class="math inline">\(X\)</span> while <span class="math inline">\(x\)</span> represents its realization i.e.¬†‚Äúthe value it takes‚Äù</p>
</blockquote>
<p><span class="math inline">\(D\)</span> can be either:</p>
<ul>
<li>an <strong>uncountable interval</strong>, in which case,<span class="math inline">\(X\)</span> is a <strong>continuous</strong> random variable, or</li>
<li>a <strong>discrete</strong> or <strong>countable</strong>, in which case, <span class="math inline">\(X\)</span> is a <strong>discrete</strong> random variable</li>
</ul>
</div>
<p>For X to be a random variable it is required that for each event <span class="math inline">\(A\)</span> consisting, if you will, of elements in <span class="math inline">\(D\)</span>:
<span class="math display">\[\begin{equation*}
\color{blue}P\left( A\right) = \color{green} {Pr} ( \left\{ s\in S :X\left(  s  \right) \in A\right\}) 
\end{equation*}\]</span>
where <span class="math inline">\(\color{blue}{P}\)</span> and <span class="math inline">\(\color{green}{Pr}\)</span> stand for ‚Äúprobability‚Äù on <span class="math inline">\(\color{blue}{D}\)</span>
and on <span class="math inline">\(\color{green}{S}\)</span>, respectively, we assess the following properties (See Chapter <a href="axioms.html#axioms">3</a>):</p>
<ul>
<li><span class="math inline">\(P \left( A\right) \geq 0\)</span> %for all <span class="math inline">\(A\in \mathcal{B}_{D}\)</span></li>
<li><span class="math inline">\(\color{blue}P \left( D\right) =\color{green}Pr (\left\{ s\in S:X\left( s\right) \in D\right\}) =Pr \left( S\right) =1\)</span></li>
<li>If <span class="math inline">\(A_{1},A_{2},A_{3}...\)</span> is a sequence of events such that: <span class="math display">\[A_{i}\cap A_{j}=\varnothing\]</span> for all <span class="math inline">\(i\neq j\)</span> then:
<span class="math display">\[\color{blue}P  \left(
\bigcup _{i=1}^{\infty }A_{i}\right) =\sum_{i=1}^{\infty } \color{blue} P\left(
A_{i}\right).\]</span></li>
</ul>
<p>In what follows we will be dropping the colors.</p>
</div>
<div id="an-example-from-gambling" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> An Example from gambling</h3>

<div class="example">
<p><span id="exm:unnamed-chunk-11" class="example"><strong>Example 4.5  (Geometric random variable)  </strong></span>
Let us imagine we are playing a game consisting of rolling a die until a 6 appears. Let us use <span class="math inline">\(X\)</span> to denote the <strong>number of rolls required to obtain a 6</strong>. Hence, the possible values of <span class="math inline">\(X\)</span> are: <span class="math inline">\(1, 2, 3,\ldots,n,\ldots\)</span> (<span class="math inline">\(\equiv \mathbb{N}\)</span>). Moreover, we can list the following probabilities associated with these values and the respective events:</p>
<ul>
<li><span class="math inline">\(P(\{X=1\}) =\Pr (\text{obtain a 6  on the 1st roll})= \frac{1}{6}\)</span></li>
<li><span class="math inline">\(P (\{X=2\})=\Pr \left( \text{no }6\text{ on the 1st roll and }6\text{ on the 2nd roll}\right) =\frac{5}{6}\cdot \frac{1}{6}=\frac{5}{36}\)</span></li>
<li><span class="math inline">\(P(\{X=3\})=\Pr \left( \text{no }6\text{ on the 1st nor 2nd roll and &#39;6&#39; on the third roll}\right)\)</span>
<span class="math inline">\(=\frac{5}{6}\cdot \frac{5}{6}\cdot \frac{1}{6}=\frac{25}{216}\)</span> and so on.</li>
</ul>
<p>Here we start seeing a pattern or a <em>recurrence</em> and we can thus infer that the probability that <strong>it will take us <span class="math inline">\(n\)</span> throws to obtain a 6</strong> is given by:</p>
<p><span class="math display">\[\begin{align*}
P(\{X=n\})&amp;=\Pr(\text{no }6\text{ on the first } n-1 \text{ rolls and 6 on the last roll})\\
&amp;=\left(\frac{5}{6}\right)^{n-1}\cdot \frac{1}{6}
\end{align*}\]</span></p>
<p>This example also allows to see that rather than listing all the possible values of <span class="math inline">\(X\)</span> along with the associated probabilities in a table, we can provide <strong>a formula that gives the required probabilities</strong> for a value <span class="math inline">\(X=n\)</span>. Hence, the <strong>probability function</strong> (a.k.a the <strong>Probability Mass Function (PMF)</strong>) of the random variable <span class="math inline">\(X\)</span> is given by:</p>
<p><span class="math display">\[\begin{equation*}
P(\left\{ X=n \right\})=\left(\frac{5}{6}\right)^{n-1}\frac{1}{6}\quad\text{for} \quad n=1,2,\ldots
\end{equation*}\]</span></p>
Finally, let‚Äôs also notice that this function fulfills the conditions to be a probability function. Using properties of geometric series, we can verify that:<br />
<span class="math display">\[\begin{equation*}
\sum_{n=1}^\infty\left(\frac{5}{6}\right)^{n-1}\frac{1}{6}=1.
\end{equation*}\]</span>
</div>
</div>
</div>
<div id="discrete-random-variables" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Discrete random variables</h2>
<p>Discrete random variables are often associated with the process of counting. The previous example is a good illustration of that use. More generally, we can characterise the probability of any random variable as follows:</p>

<div class="definition">
<p><span id="def:unnamed-chunk-12" class="definition"><strong>Definition 4.3  (Probability of a Random Variable)  </strong></span>Suppose <span class="math inline">\(X\)</span> can take the values <span class="math inline">\(x_{1},x_{2},x_{3},\ldots ,x_{n}\)</span>. The probability of <span class="math inline">\(x_{i}\)</span> is
<span class="math display">\[p_{i}= P(\left\{ X=x_i\right\})\]</span></p>
<p>and we must have <span class="math inline">\(p_{1}+p_{2}+p_{3}+\cdots +p_{n}=1\)</span> and all <span class="math inline">\(p_{i}\geq 0\)</span>. These probabilities may be put in a table:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center"><span class="math inline">\(P(\left\{ X=x_i\right\})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{1}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_{2}\)</span></td>
<td align="center"><span class="math inline">\(p_{2}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{3}\)</span></td>
<td align="center"><span class="math inline">\(p_{3}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{n}\)</span></td>
<td align="center"><span class="math inline">\(p_{n}\)</span></td>
</tr>
<tr class="even">
<td align="center">Total</td>
<td align="center"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>For a <strong>discrete random variable <span class="math inline">\(X\)</span></strong>, any table listing all possible nonzero probabilities provides the entire <strong>probability distribution</strong>.</p>
<p>And the <strong>probability mass function</strong> <span class="math inline">\(p(a)\)</span> of <span class="math inline">\(X\)</span> is defined by:
<span class="math display">\[ 
p_a = p(a)= P(\{X=a \}),
\]</span>
and this is positive for at most a countable number of values of <span class="math inline">\(a\)</span>. For instance, <span class="math inline">\(p_{1} = P(\left\{ X=x_1\right\})\)</span>, <span class="math inline">\(p_{2} = P(\left\{ X=x_2\right\})\)</span>, and so on.</p>
<p>That is, if <span class="math inline">\(X\)</span> must assume one of the values <span class="math inline">\(x_1,x_2,...\)</span>, then
<span class="math display">\[\begin{eqnarray}
 p(x_i) \geq 0 &amp; \text{for \ \ } i=1,2,...  \\
 p(x) = 0 &amp; \text{otherwise.}
\end{eqnarray}\]</span></p>
<p>Clearly, we must have
<span class="math display">\[
\sum_{i=1}^{\infty} p(x_i) = 1.
\]</span></p>
<!-- %The probabilities $p_{i}=P\left( X=x_{i}\right)$ may be given by an appropriate mathematical formula: i.e.  -->
<!-- %$$ -->
<!-- %p_{i}=P\left( X=x_{i}\right)=f(x_{i})  -->
<!-- %$$  -->
</div>
<div id="cumulative-distribution-function" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Cumulative Distribution Function</h2>
<p>The <strong>cumulative distribution function (CDF)</strong> is a table listing the values that <span class="math inline">\(X\)</span> can take, alongside the the cumulative probability, i.e.
<span class="math display">\[F_X(a) = P \left(\{ X\leq a\}\right)= \sum_{\text{all } x \leq a } p(x).\]</span></p>
<p>If the random variable <span class="math inline">\(X\)</span> takes on values <span class="math inline">\(x_{1},x_{2},x_{3},\ldots .,x_{n}\)</span> <em>listed in
increasing order</em> <span class="math inline">\(x_{1}&lt;x_{2}&lt;x_{3}&lt;\cdots &lt;x_{n}\)</span>, the CDF is a step function, that it its value is constant in the intervals <span class="math inline">\((x_{i-1},x_i]\)</span> and takes a step/jump of size <span class="math inline">\(p_i\)</span>
at each <span class="math inline">\(x_i\)</span>:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center"><span class="math inline">\(F_X(x_i)=P\left(\{ X\leq x_i\}\right)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{1}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_{2}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}+p_{2}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{3}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}+p_{2}+p_{3}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{n}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}+p_{2}+\cdots +p_{n}=1\)</span></td>
</tr>
</tbody>
</table>

<div class="example">
<p><span id="exm:unnamed-chunk-13" class="example"><strong>Example 4.6  </strong></span>
Let us conside a random variable <span class="math inline">\(X_i\)</span> taking values <span class="math inline">\(\{0,1,2,3\}\)</span> with the probabilities listed as follows. We can display the values of the PDF and the PMF at the same time:</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center"><span class="math inline">\(P(\{X=x_i\})\)</span></th>
<th align="center"><span class="math inline">\(P(\{X\leq x_i\})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0</td>
<td align="center">4/35</td>
<td align="center">4/35</td>
</tr>
<tr class="even">
<td align="center">1</td>
<td align="center">18/35</td>
<td align="center">22/35</td>
</tr>
<tr class="odd">
<td align="center">2</td>
<td align="center">12/35</td>
<td align="center">34/35</td>
</tr>
<tr class="even">
<td align="center">3</td>
<td align="center">1/35</td>
<td align="center">35/35</td>
</tr>
<tr class="odd">
<td align="center">Total</td>
<td align="center">1</td>
<td align="center"></td>
</tr>
</tbody>
</table>
Moreover, we can provide the expression of the CDF as a function defined on intervals:
<span class="math display">\[F_X(x) = \left\{ 
\begin{array}{ll}
0 &amp; x&lt;0 \\
4/35 &amp; 0 \leq x &lt; 1\\
22/35 &amp; 1 \leq x &lt; 2\\
34/35 &amp; 2 \leq x &lt; 3 \\
1 &amp; x \geq 3.
\end{array} \right.,\]</span>
which can be displayed graphically as <em>a step function</em>
</div>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-14"></span>
<img src="img/04_discrete_rv/repartbis.png" alt="Step function" width="50%" />
<p class="caption">
Figure 4.3: Step function
</p>
</div>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Suppose <span class="math inline">\(a\leq b\)</span>. Then, because the event <span class="math inline">\(\{X\leq a \}\)</span> is contained in the event <span class="math inline">\(\{X\leq b \}\)</span>, namely
<span class="math display">\[\{X\leq a \} \subseteq \{X\leq b \},\]</span> it follows that
<span class="math display">\[F_X(a) \leq F_X(b),\]</span>
so, the probability of the former is less than or equal to the probability of the latter.
In other words, <strong><span class="math inline">\(F_X(x)\)</span> is a nondecreasing function of <span class="math inline">\(x\)</span>.</strong>
</div>

<div class="example">
<p><span id="exm:unnamed-chunk-16" class="example"><strong>Example 4.7  (Quantiles)  </strong></span>Since the CDF is monotonous, it can be inverted to define the value <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span> that corresponds to a given probability <span class="math inline">\(\alpha\)</span>, namely <span class="math inline">\(\alpha = P (X \leq x )\)</span>, for <span class="math inline">\(\alpha \in [0,1]\)</span>.</p>
The inverse CDF <span class="math inline">\(F_X^{-1}(\alpha)\)</span> or quantile of order <span class="math inline">\(\alpha\)</span>, and labelled as <span class="math inline">\(Q(\alpha)\)</span>, is the smallest realisation of <span class="math inline">\(X\)</span> associated to a CDF greater or equal to <span class="math inline">\(\alpha\)</span>; in formula, the
<span class="math inline">\(\alpha\)</span>-quantile <span class="math inline">\(Q(\alpha)\)</span> is the smallest number satisfying:
<span class="math display">\[F_X [F^{-1}_X (\alpha)] = P[X \leq \underbrace{F^{-1}_X (\alpha)}_{Q(\alpha)}] \geq \alpha, \quad \text{for} \quad \alpha\in[0,1].\]</span>
By construction, a quantile of a discrete random variable is a realization of <span class="math inline">\(X\)</span>.
</div>
<p>If we denote the random variable as <span class="math inline">\(R\)</span>, its realisations with <span class="math inline">\(r\)</span> and the CDF evaluated in <span class="math inline">\(r\)</span> as <span class="math inline">\(F_R(r)\)</span>, we can see graphically:
<img src="img/04_discrete_rv/Quantiles_Discr.png" width="1170" style="display: block; margin: auto;" /></p>
</div>
<div id="distributional-summaries-for-discrete-random-variables" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Distributional summaries for discrete random variables</h2>
<p>In many applications, it is useful to describe some attributes or properties of the distribution of a Random Variable, for instance, to have an overview of how ‚Äúcentral‚Äù a realisation is or how ‚Äúspread‚Äù or variable the distribution really is. In this section, we will define two of these summaries:</p>
<ul>
<li><p>The <strong>Expectation</strong>, or <strong>Mean</strong> of the distribution is an indicator of ‚Äúlocation.‚Äù It is defined as the mean of the realisations weighted by their probabilities, i.e.¬†
<span class="math display">\[\begin{equation*}
E\left[ X\right] =p_{1}x_{1}+p_{2}x_{2}+\cdots + p_{n}x_{n} = \sum_{i=1}^{n} p_i x_i
\end{equation*}\]</span>
Roughly speaking the mean represents the <em>center of gravity</em> of the distribution.</p></li>
<li><p>The <strong>square root of the variance</strong>, or <strong>standard deviation</strong>, of the distribution is a measure of <em>spread</em> and is computed as the average squared distance between the observations with respect to the Expectation.
<span class="math display">\[\begin{eqnarray*}
s.d\left( X\right) &amp;=&amp;\sqrt{Var\left( X\right) } \\
&amp;=&amp;\sqrt{p_{1}\left( x_{1}-E\left[ X\right] \right) ^{2}+p_{2}\left( x_{2}-E
\left[ X\right] \right) ^{2}+\cdots + p_{n}\left( x_{n}-E\left[ X\right]
\right) ^{2}}
\end{eqnarray*}\]</span>
Roughly <em>spread (or ‚Äòvariability‚Äô or ‚Äòdispersion‚Äô).</em></p></li>
</ul>
<div id="properties" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> Properties</h3>
<p>If <span class="math inline">\(X\)</span> is a discrete random variable and <span class="math inline">\(a\)</span> is any real number, then</p>
<ul>
<li><span class="math inline">\(E\left[ \alpha X\right] =\alpha E\left[ X\right]\)</span></li>
<li><span class="math inline">\(E\left[ \alpha+X\right] =\alpha+E\left[ X\right]\)</span></li>
<li><span class="math inline">\(Var\left( \alpha X\right) =\alpha^{2}Var\left( X\right)\)</span></li>
<li><span class="math inline">\(Var\left( \alpha+X\right) =Var\left( X\right)\)</span></li>
</ul>

<div class="exercise">
<p><span id="exr:unnamed-chunk-18" class="exercise"><strong>Exercise 4.2  </strong></span>Let us verify the first property:
<span class="math inline">\(E\left[ \alpha X \right] =\alpha E\left[ X\right]\)</span>.</p>
<p>From the Intro lecture we know that, for every <span class="math inline">\(\alpha_i \in \mathbb{R}\)</span>,<br />
<span class="math display">\[\sum_{i=1}^{n} \alpha_i X_{i} = \alpha_1 X_1 + \alpha_2 X_2 +....+ \alpha_n X_n.\]</span> So, the
required result follows as a special case, setting <span class="math inline">\(\alpha_i= \alpha\)</span>, for every <span class="math inline">\(i\)</span>, and applying the definition of expected value.</p>
Verify this and the other properties as an exercise. [Hint: set <span class="math inline">\(\alpha_i = \alpha p_i\)</span>.]
</div>
</div>
</div>
<div id="dependenceindependence" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Dependence/Independence</h2>

<div class="definition">
<span id="def:unnamed-chunk-19" class="definition"><strong>Definition 4.4  </strong></span>Consider two discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Then, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent</strong> if
<span class="math display">\[\begin{equation*}
P \left(\left\{ \ X=x\right\} \cap \left\{ Y=y\right\} \right) =P \left(\{
X=x\}\right) \cdot P \left(\{ Y=y \}\right)
\end{equation*}\]</span>
for all values <span class="math inline">\(x\)</span> that <span class="math inline">\(X\)</span> can take and all values <span class="math inline">\(y\)</span> that <span class="math inline">\(Y\)</span> can take.
</div>
<div id="more-important-properties" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> More important properties</h3>
<ul>
<li><p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two discrete random variables, then%
<span class="math display">\[\begin{equation*}
E\left[ X+Y\right] =E\left[ X\right] +E\left[ Y\right]
\end{equation*}\]</span></p></li>
<li><p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are also <em>independent</em>, then
<span class="math display">\[\begin{equation}
Var\left( X+Y\right) =Var\left( X\right) +Var\left( Y\right) \label{Eq. Var}
\end{equation}\]</span></p></li>
</ul>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Note that Eq. () does not (typically) hold if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are NOT independent‚Äîmore to come on this later on‚Ä¶
</div>
</div>
<div id="more-on-expectations" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> More on expectations</h3>
<p>Recall that the expectation of X was defined as
<span class="math display">\[\begin{equation*}
E\left[ X\right] = \sum_{i=1}^{n} p_i x_i
\end{equation*}\]</span></p>
<p>Now, suppose we are interested in a function <span class="math inline">\(m\)</span> of the random variable <span class="math inline">\(X\)</span>, say <span class="math inline">\(m(X)\)</span>. We define
<span class="math display">\[\begin{equation*}
E\left[ m\left( X\right) \right] =p_{1}m\left( x_{1}\right) +p_{2}m\left(
x_{2}\right) +\cdots p_{n}m\left( x_{n}\right).
\end{equation*}\]</span></p>
<p>Notice that the variance is a special case of expectation where,
<span class="math display">\[\begin{equation*}
m(X)=(X-E\left[ X\right] )^{2}.
\end{equation*}\]</span>
Indeed,
<span class="math display">\[\begin{equation*}
Var\left( X\right) =E\left[ (X-E\left[ X\right] )^{2}\right].
\end{equation*}\]</span></p>
</div>
</div>
<div id="some-discrete-distributions-of-interest" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Some discrete distributions of interest</h2>
<ul>
<li>Discrete Uniform</li>
<li>Bernoulli</li>
<li>Binomial</li>
<li>Poisson</li>
<li>Hypergeometric</li>
<li>Negative binomial</li>
</ul>
<p>Their main characteristic is that the probability <span class="math inline">\(P\left(\left\{ X=x_i\right\}\right)\)</span> is given by an appropriate mathematical formula: i.e.¬†
<span class="math display">\[p_{i}=P\left(\left\{ X=x_i\right\}\right)=h(x_{i})\]</span>
for a suitably specified function <span class="math inline">\(h(\cdot)\)</span>.</p>
<div id="discrete-uniform-distribution" class="section level3" number="4.6.1">
<h3><span class="header-section-number">4.6.1</span> Discrete uniform distribution</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-21" class="definition"><strong>Definition 4.5  </strong></span>We say <span class="math inline">\(X\)</span> has a <strong>discrete uniform distribution</strong> when</p>
<ul>
<li><span class="math inline">\(X\)</span> can take the values <span class="math inline">\(x=0,1,2,...,k\)</span> (for some specified finite value <span class="math inline">\(k\in \mathbb{N}\)</span>)</li>
<li>The probability that <span class="math inline">\(X=x\)</span> is <span class="math inline">\(1/\left( k+1\right)\)</span>, namely</li>
</ul>
<p><span class="math display">\[P\left(\left\{ X=x\right\}\right) = \frac{1}{\left( k+1\right)}.\]</span></p>
<p>The probability distribution is given by</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center"><span class="math inline">\(P \left(\left\{ X=x_i\right\}\right)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{\left( k+1\right) }\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{\left( k+1\right) }\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(k\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{\left( k+1\right) }\)</span></td>
</tr>
<tr class="odd">
<td align="center">Total</td>
<td align="center"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="expectation" class="section level4" number="4.6.1.1">
<h4><span class="header-section-number">4.6.1.1</span> Expectation</h4>
<ul>
<li>The expected value of <span class="math inline">\(X\)</span> is
<span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;  x_1 p_1 + ... +  x_k p_k\\
&amp;=&amp; 0\cdot \frac{1}{\left( k+1\right) }+1\cdot \frac{1}{%
\left( k+1\right) }+\cdots +k\cdot \frac{1}{\left( k+1\right) } \\
&amp;=&amp;\frac{1}{\left( k+1\right) }\cdot\left( 0+1+\cdots +k\right) \\
&amp;=&amp;\frac{1}{\left( k+1\right) }\cdot \frac{k\left( k+1\right) }{2} \\
&amp;=&amp;\frac{k}{2}.
\end{eqnarray*}\]</span></li>
</ul>
<p>E.g. when <span class="math inline">\(k=6\)</span>, then <span class="math inline">\(X\)</span> can take on one of the seven distinct values
<span class="math inline">\(x=0,1,2,3,4,5,6,\)</span> each with equal probability <span class="math inline">\(\frac{1}{7}\)</span>, but the
expected value of <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(3\)</span>, which is one of the possible outcomes!!!</p>
</div>
<div id="variance" class="section level4" number="4.6.1.2">
<h4><span class="header-section-number">4.6.1.2</span> Variance</h4>
<ul>
<li>The variance of <span class="math inline">\(X\)</span> ‚Äì we will be denoting it as <span class="math inline">\(Var(X)\)</span> ‚Äì is%
<span class="math display">\[\begin{eqnarray*}
Var\left( X\right) &amp;=&amp;\left( 0-\frac{k}{2}\right) ^{2}\cdot \frac{1}{\left(
k+1\right) }+\left( 1-\frac{k}{2}\right) ^{2}\cdot \frac{1}{\left(
k+1\right) }+ \\
&amp;&amp;\cdots +\left( k-\frac{k}{2}\right) ^{2}\cdot \frac{1}{\left( k+1\right) }
\\
&amp;=&amp;\frac{1}{\left( k+1\right) }\cdot\left\{ \left( 0-\frac{k}{2}\right)
^{2}+\left( 1-\frac{k}{2}\right) ^{2}+\cdots +\left( k-\frac{k}{2}\right)
^{2}\right\} \\
&amp;=&amp;\frac{1}{\left( k+1\right) }\cdot \frac{k\left( k+1\right) \left(
k+2\right) }{12} \\
&amp;=&amp;\frac{k\left( k+2\right) }{12}
\end{eqnarray*}\]</span></li>
</ul>
<p>E.g. when <span class="math inline">\(k=6\)</span>, the variance of <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(4,\)</span> and the standard
deviation of <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(\sqrt{4}=2.\)</span></p>
</div>
<div id="illustrations" class="section level4" number="4.6.1.3">
<h4><span class="header-section-number">4.6.1.3</span> Illustrations</h4>
<p><img src="img/04_discrete_rv/discrete_uniforms__1.png" width="80%" style="display: block; margin: auto;" /></p>

<div class="example">
<p><span id="exm:unnamed-chunk-23" class="example"><strong>Example 4.8  </strong></span>
An example of discrete uniform is related to the experiment of rolling a die- with the important remark that the outcome zero is not allowed in this specific example.</p>
<p>Let us call <span class="math inline">\(X\)</span> the corresponding random variable and <span class="math inline">\(\{x_1,x_2,...,x_6\}\)</span> its realizations.</p>
<p>The possible outcomes are:
<span class="math display">\[\{1,2,3,4,5,6\}\]</span>
each having probability <span class="math inline">\(\frac{1}{6}\)</span>.</p>
Moreover,
<span class="math display">\[E(X) = (1+2+3+4+5+6) \cdot \frac{1}{6} = 3.5,\]</span>
which is not one of the possible outcomes(!)
</div>
</div>
</div>
<div id="bernoulli-trials" class="section level3" number="4.6.2">
<h3><span class="header-section-number">4.6.2</span> Bernoulli Trials</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-24" class="definition"><strong>Definition 4.6  </strong></span>
<em>Bernoulli trial</em> is the name given to the random variable <span class="math inline">\(X\)</span> having probability distribution given by</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center"><span class="math inline">\(P(\left\{ X=x_i\right\})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(p\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(1-p\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Often we write the probability mass function (PMF) as:</p>
<p><span class="math display">\[\begin{equation*}
P(\left\{ X=x\right\})=p^{x}\left( 1-p\right) ^{1-x}, \quad \text{ for }x=0,1
\end{equation*}\]</span></p>
<p>A Bernoulli trial represents the most primitive form of all random variables. It derives from a random experiment having only two possible mutually exclusive outcomes. These are often labelled Success and Failure and</p>
<ul>
<li>Success occurs with probability <span class="math inline">\(p\)</span></li>
<li>Failure occurs with probability <span class="math inline">\(1-p\)</span>.</li>
</ul>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Just for the sake of notation, let us set <span class="math inline">\(X=1\)</span> if <em>Success</em> occurs, and <span class="math inline">\(X=0\)</span> if <em>Failure</em> occurs
</div>

<div class="example">
<p><span id="exm:unnamed-chunk-26" class="example"><strong>Example 4.9  </strong></span>Coin tossing: we can define a random variable</p>
<table>
<thead>
<tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center"><span class="math inline">\(P(\left\{ X=x_i\right\})\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(1\)</span></td>
<td align="center"><span class="math inline">\(p\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(0\)</span></td>
<td align="center"><span class="math inline">\(1-p\)</span></td>
</tr>
</tbody>
</table>
and say that <span class="math inline">\(X=1\)</span> if <span class="math inline">\(H\)</span> and <span class="math inline">\(X=0\)</span> if <span class="math inline">\(T\)</span>.
</div>
<div id="expectation-1" class="section level4" number="4.6.2.1">
<h4><span class="header-section-number">4.6.2.1</span> Expectation</h4>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;1 \cdot p+0 \cdot (1-p) \\
&amp;=&amp;p
\end{eqnarray*}\]</span></p>
</div>
<div id="variance-1" class="section level4" number="4.6.2.2">
<h4><span class="header-section-number">4.6.2.2</span> Variance</h4>
<p><span class="math display">\[\begin{eqnarray*}
Var\left( X\right) &amp;=&amp;\left( 1-p\right) ^{2} \cdot p+\left( 0-p\right)
^{2} \cdot \left( 1-p\right) \\
&amp;=&amp;p\left( 1-p\right).
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="the-binomial-distribution" class="section level3" number="4.6.3">
<h3><span class="header-section-number">4.6.3</span> The Binomial Distribution</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-27" class="definition"><strong>Definition 4.7  </strong></span>Let us consider the random experiment consisting in a series of <span class="math inline">\(n\)</span> trials
having 3 characteristics</p>
<ul>
<li>Only two mutually exclusive outcomes are possible in each trial:
<em>success</em> (<em>S</em>) and <em>failure</em> (<em>F</em>)</li>
<li>The outcomes in the series of <span class="math inline">\(n\)</span> trials constitute independent events</li>
<li>The probability of success <span class="math inline">\(p\)</span> in each trial is constant from trial to
trial</li>
</ul>
<span class="math inline">\(X\)</span> is the <em>number of successes</em> occurring in <span class="math inline">\(n\)</span> (Bernoulli) trials. Binomial
probability distribution given by:
<span class="math display" id="eq:binom">\[\begin{eqnarray}
P(\left\{ X=x\right\})&amp;=&amp;{n \choose k} p^{x}\left(1-p\right)^{n-x} \\
&amp;=&amp;\frac{n!}{x!\left( n-x\right) !}p^{x}\left( 1-p\right) ^{n-x},\text{ for }%
x=0,1,2,...,n 
\tag{4.1}
\end{eqnarray}\]</span>
</div>
<p>You might recall from Chapter <a href="introduction.html#introduction">1</a> that Combinations are defined as:
<span class="math display">\[\begin{equation*}
{n \choose k} =\frac{n!}{k!\left( n-k\right) !}=C^{k}_{n}
\end{equation*}\]</span>
and, for <span class="math inline">\(n \geq k\)</span>, we say ``<span class="math inline">\(n\)</span> choose <span class="math inline">\(k\)</span>‚Äô‚Äô.</p>
<p>The binomial coefficient <span class="math inline">\(n \choose k\)</span> represents the number of possible combinations of <span class="math inline">\(n\)</span> objects taken <span class="math inline">\(k\)</span> at a time, without regard of the order. Thus, <span class="math inline">\(C^{k}_{n}\)</span> represents the number of different groups of size <span class="math inline">\(k\)</span> that could be selected from a set of <span class="math inline">\(n\)</span> objects
when the order of selection is not relevant.</p>
<p>So, ‚ÄúWhat is the interpretation of the formula?‚Äù</p>
<ul>
<li>The first factor <span class="math display">\[{n \choose k} =\frac{n!}{x!\left( n-x\right)!}\]</span> is the number of different combinations of individual ‚Äúsuccesses‚Äù and ‚Äúfailures‚Äù in <span class="math inline">\(n\)</span> (Bernoulli) trials that result in a sequence containing a total of <span class="math inline">\(x\)</span> ‚Äòsuccesses‚Äô and <span class="math inline">\(n-x\)</span> ‚Äúfailures.‚Äù</li>
<li>The second factor <span class="math display">\[p^{x}\left( 1-p\right) ^{n-x}\]</span> is the probability associated with any one
sequence of <span class="math inline">\(x\)</span> ‚Äòsuccesses‚Äô and <span class="math inline">\((n-x)\)</span> `failures‚Äô.</li>
</ul>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Short-hand notation: <span class="math display">\[X \sim \mathcal{B}(x,n,p)\]</span> or, occasionally, simply $ X (n,p)$ (no <span class="math inline">\(x\)</span> in the formula).
</div>
<div id="expectation-2" class="section level4" number="4.6.3.1">
<h4><span class="header-section-number">4.6.3.1</span> Expectation</h4>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\sum_{x=0}^{n}x\Pr \left\{ X=x\right\} \\
&amp;=&amp;\sum_{x=0}^{n}x {n\choose k} p^{x}\left(1-p\right) ^{n-x} = np
\end{eqnarray*}\]</span></p>
</div>
<div id="variance-2" class="section level4" number="4.6.3.2">
<h4><span class="header-section-number">4.6.3.2</span> Variance</h4>
<p><span class="math display">\[\begin{eqnarray*}
Var\left( X\right) &amp;=&amp;\sum_{x=0}^{n}\left( x-np\right) ^{2} P (\left\{
X=x\right\}) \\
&amp;=&amp;np\left( 1-p\right)
\end{eqnarray*}\]</span></p>

<div class="remark">
 <span class="remark"><em>Remark. </em></span> Looking at (@ref{eq:Binom}), we remark that the Bernoulli distribution is a special case (<span class="math inline">\(n=1\)</span>)
of the Binomial distribution. Roughly speaking, ``a Binomial random variable arises when we sum <span class="math inline">\(n\)</span> independent
Bernoulli trails.‚Äô‚Äô
</div>
<!-- %%EndExpansion -->
<!-- % -->
<!-- %\frametitle{Some illustrations of Binomial \begin{small}(introducing a tool)\end{small}} -->
<!-- %Let us provide a graphical illustration of $X\sim B(x,n,p)$ via some numerical method. Specifically, we first  \color{blue}simulate  a large number of  -->
<!-- %realizations of $X$, then we draw their \color{blue}histogram  (or barplot). \\ -->
<!-- % -->
<!-- %%```{remark} -->
<!-- %A **histogram is a representation of a distribution by means of rectangles whose widths represent class intervals and whose areas are proportional  -->
<!-- %to the corresponding frequencies. The purpose of a histogram is to graphically summarize the distribution of a data set -- roughly speaking, a histogram is a graphical representation of a table of frequencies.  -->
<!-- % -->
<!-- % -->
<!-- %```{remark}[How to draw it?] -->
<!-- %The most common form of  -->
<!-- %the histogram is obtained by splitting the range of the data into equal-sized bins. Then for each bin, the number of points from the data set  -->
<!-- %that fall into each bin are counted. That is: on the \color{blue} vertical axis we read the (relative) frequency  (i.e., counts for each bin); on the \color{red} horizontal axis we read the observed values of $X$. The classes can either be defined arbitrarily by the user or via some systematic rule.   -->
<!-- %``` -->
<!-- % -->
<!-- % -->
<!-- %EndExpansion -->
</div>
<div id="illustrations-1" class="section level4" number="4.6.3.3">
<h4><span class="header-section-number">4.6.3.3</span> Illustrations</h4>
<p>The visualisation shows some similiarities to the Discrete Uniform but some values seem more probable than others. Moreover, the shape of the distribution seems to vary according to the values of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, i.e the <em>parameters</em> of the distribution.</p>
<p><img src="img/04_discrete_rv/distbin.png" width="821" style="display: block; margin: auto;" /></p>

<div class="example">
<p><span id="exm:unnamed-chunk-31" class="example"><strong>Example 4.10  (cherry trees)  </strong></span>
One night a storm washes three cherries ashore on an island. For each cherry, there is a probability <span class="math inline">\(p=0.8\)</span> that its seed will
produce a tree. What is the probability that these three cherries will produce two
trees?</p>
<p>First, we notice that this can be determined using a <strong>Bernoulli distribution</strong>. To this end, consider whether each seed will produce a tree as a sequence of <span class="math inline">\(n=3\)</span>
trials. For each cherry:</p>
<ul>
<li>either the cherry produces a tree (Success) or it does not (Failure);</li>
<li>the event that a cherry produces a tree is independent from the event that any of the other two cherries produces a tree.</li>
<li>The probability that a cherry produces a tree is the same for all three cherries
</div></li>
</ul>
<p><img src="img/04_discrete_rv/BINOMIALpic__1.png" width="80%" style="display: block; margin: auto;" /></p>

<div class="example">
<p><span id="exm:unnamed-chunk-33" class="example"><strong>Example 4.11  </strong></span>
- There are <span class="math inline">\(2^{3}=8\)</span> possible outcomes from the <span class="math inline">\(3\)</span> individual trials</p>
<ul>
<li><p>It does not matter which of the three cherries produce a tree</p></li>
<li><p>Consider all of the possible sequences of outcomes (S=success, F=failure)</p></li>
</ul>
<p><span class="math display">\[SSS, \color{red}{SSF}, \color{red}{SFS}, SFF, \color{red}{FSS, FSF, FFS, FFF}\]</span></p>
<ul>
<li><p>We are interested in <span class="math inline">\(\color{red}{SSF}\)</span> , <span class="math inline">\(\color{red}{SFS}\)</span>, <span class="math inline">\(\color{red}{FSS}\)</span></p></li>
<li><p>These possible events are <em>mutually exclusive</em>, so</p></li>
</ul>
<p><span class="math display">\[\begin{equation*}
\Pr(\left\{\color{red}{SSF} \cup \color{red}{SFS} \cup \color{red}{FSS} \right\}) = 
  \Pr (\left\{\color{red}{SSF}\right\}) +\Pr (\left\{\color{red}{SFS}\right\}) +
  \Pr (\left\{\color{red}{FSS}\right\})
\end{equation*}\]</span></p>
<p>The three trials are assumed to be <em>independent</em>, so each of the three seed events corresponding to two trees growing has the same probability</p>
<p><span class="math display">\[\begin{eqnarray*}
\Pr (\left\{\color{red}SSF \right\})  &amp;=&amp;\Pr (\left\{ \color{red}S \right\}) \cdot \Pr (\left\{\color{red} S\right\} ) \cdot (\Pr \left\{\color{red} F \right\}  ) \\
&amp;=&amp;0.8\cdot 0.8\cdot (1-0.8) \\
&amp;=&amp;0.8\cdot (1-0.8)\cdot 0.8 =\Pr (\left\{\color{red}{SFS} \right\})  \\
&amp;=&amp;(1-0.8)\cdot 0.8\cdot 0.8= \Pr (\left\{\color{red}{FSS} \right\} ) \\
&amp;=&amp;0.128
\end{eqnarray*}\]</span></p>
<p>So the probability of two trees resulting from the three seeds must be</p>
<span class="math display">\[\begin{eqnarray*}
\Pr (\left\{ \color{red}{SSF} \cup \color{red}{SFS} \cup \color{red}{FSS} \right\} ) &amp;=&amp;3\cdot 0.128 \\
&amp;=&amp;0.384.
\end{eqnarray*}\]</span>
</div>

<div class="example">
<p><span id="exm:unnamed-chunk-34" class="example"><strong>Example 4.12  </strong></span>Finally, we notice that we can obtain the same result (in a more direct way), using the <strong>binomial probability</strong> for the random variable
<span class="math display">\[X= \text{number of trees that grows from 3 seeds}.\]</span></p>
<p>Indeed</p>
<span class="math display">\[\begin{eqnarray*}
\Pr (\left\{ X=2\right\})  &amp;=&amp;\frac{3!}{2!\left( 3-2\right) !}\cdot \left(
0.8\right) ^{2} \cdot \left( 1-0.8\right) ^{3-2} \\
&amp;=&amp;3 \cdot \left( 0.8\right) ^{2} \cdot \left( 0.2\right)  \\
&amp;=&amp;0.384.
\end{eqnarray*}\]</span>
</div>
</div>
</div>
<div id="poisson-distribution" class="section level3" number="4.6.4">
<h3><span class="header-section-number">4.6.4</span> Poisson Distribution</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-35" class="definition"><strong>Definition 4.8  </strong></span>
Let us consider random variable <span class="math inline">\(X\)</span> which takes values <span class="math inline">\(0,1,2,...\)</span>, namely the nonnegative integers in <span class="math inline">\(\mathbb{N}\)</span>. <span class="math inline">\(X\)</span> is said to be a Poisson random variable if its probability mass function, with <span class="math inline">\(\lambda &gt;0\)</span> fixed and providing info on the intensity, is
<span class="math display">\[\begin{equation}
p(x)=\Pr \left( \{ X = x \}\right) =\frac{\lambda ^{x}e^{-\lambda }}{x!}\text{,\qquad }%
x=0,1,2,... \label{Eq. Poisson}
\end{equation}\]</span>
and we write <span class="math inline">\(X\sim \text{Poisson}(\lambda)\)</span>.</p>
</div>
<p>The Eq. () defines a genuine probability mass function, since <span class="math inline">\(p(x) \geq 0\)</span> and</p>
<p><span class="math display">\[\begin{eqnarray}
\sum_{x=0}^{\infty} p(x) &amp;=&amp; \sum_{x=0}^{\infty}  \frac{\lambda ^{x}e^{-\lambda }}{x!}   \\
&amp; = &amp; e^{-\lambda } \sum_{x=0}^{\infty}   \frac{\lambda ^{x}}{x!}  \\
&amp; = &amp; e^{-\lambda } e^{\lambda } = 1  \quad \text{(see Intro Lecture).}
\end{eqnarray}\]</span></p>
<p>Moreover, for a given value of $$ also the CDF can be easily defined. E.g.</p>
<p><span class="math display">\[\begin{equation*}
F_X(2)=\Pr \left( \{X\leq 2\}\right) =e^{-\lambda }+\lambda e^{-\lambda }+\frac{\lambda
^{2}e^{-\lambda }}{2},
\end{equation*}\]</span></p>
<p>and the Expected value and Variance for Poisson distribution (see tutorial) can be obtained by ‚Äò‚Äôsum algebra‚Äô‚Äô (and/or some algebra)</p>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\lambda \\
Var\left( X\right) &amp;=&amp;\lambda.
\end{eqnarray*}\]</span></p>
<div id="illustrations-2" class="section level4" number="4.6.4.1">
<h4><span class="header-section-number">4.6.4.1</span> Illustrations</h4>
<p>‚Ä¶ same barplot as in slide 30, just a bit fancier‚Ä¶</p>
<p><img src="img/fun/distpois.png" width="80%" style="display: block; margin: auto;" /></p>

<div class="example">
<p><span id="exm:unnamed-chunk-37" class="example"><strong>Example 4.13  </strong></span>The average number of newspapers sold by Alfred is 5 per minute. What is the probability that Alfred will sell at least 1 newspaper in a minute?</p>
<p>To answer, let <span class="math inline">\(X\)</span> be the <span class="math inline">\(\#\)</span> of newspapers sold by Alfred in a minute. We have</p>
<p><span class="math display">\[ X \sim \text{Poisson}(\lambda) \]</span></p>
with <span class="math inline">\(\lambda = 5\)</span>, so
<span class="math display">\[\begin{eqnarray*}
P(X \geq 1) &amp; = &amp; 1- P(\{X=0\}) \\
&amp; = &amp; 1 - \exp^{-5} \frac{5^0}{0!} \\
%&amp; = &amp; 1-\exp^{-5} \\
&amp; \approx &amp; 1- 0.0067 \approx 99.33\%.
\end{eqnarray*}\]</span>
How about <span class="math inline">\(P(X \geq 2)\)</span>? Is it <span class="math inline">\(P(X \geq 2) \geq P(X \geq 1)\)</span> or not? Answer the question‚Ä¶
</div>

<div class="example">
<p><span id="exm:unnamed-chunk-38" class="example"><strong>Example 4.14  </strong></span>A telephone switchboard handles 300 calls, on the average, during one hour. The board
can make maximum 10 connections per minute. Use the Poisson
distribution to evaluate the probability that the board will be overtaxed during a given minute.</p>
To answer, let us set <span class="math inline">\(\lambda = 300\)</span> per hour, which is equivalent to 5 calls per minute. Noe let us define
<span class="math display">\[
X = \text{\# of connections in a minute}
\]</span>
and by assumption we have <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>. Thus,
<span class="math display">\[\begin{eqnarray}
P[\text{overtaxed}] &amp;=&amp; P(\{X &gt; 10\})  \\ 
&amp;=&amp; 1 \quad - \underbrace{P(\{X \leq 10\})}_{\text{using $\lambda=5$,  minute base}}  \\ 
&amp;\approx&amp; 0.0137. 
\end{eqnarray}\]</span>
</div>
</div>
<div id="link-to-binomial" class="section level4" number="4.6.4.2">
<h4><span class="header-section-number">4.6.4.2</span> Link to Binomial</h4>
<p>Let us consider <span class="math inline">\(X \sim B(x,n,p)\)</span>,  where <span class="math inline">\(n\)</span> is large, <span class="math inline">\(p\)</span> is small, and the product <span class="math inline">\(np\)</span> is appreciable. Setting, <span class="math inline">\(\lambda=np\)</span>, we
then have that, for the Binomial probability as in Eq.(), it is a good approximation to write:
<span class="math display">\[
p(k) = P(\{X=k\}) \approx \frac{\lambda^k}{k!} e^{-\lambda}.
\]</span>
To see this, remember that<br />
<span class="math display">\[
\lim_{n\rightarrow\infty} \left( 1- \frac{\lambda}{n} \right)^n = e^{-\lambda}.
\]</span>
Then, let us consider that in our setting, we have <span class="math inline">\(p=\lambda/n\)</span>. From the formula of the binomial probability mass function we have:
<span class="math display">\[
p(0) = (1-p)^{n}=\left( 1- \frac{\lambda}{n} \right)^{n} \approx e^{-\lambda}, \quad \text{\ as \ \ } n\rightarrow\infty.
\]</span></p>
<p>Moreover, it is easily found that</p>
<p><span class="math display">\[\begin{eqnarray}
\frac{p(k)}{p(k-1)} &amp;=&amp; \frac{np-(k-1)p}{k(1-p)} \approx \frac{\lambda}{k}, \quad \text{\ as \ \ } n\rightarrow\infty. 
\end{eqnarray}\]</span></p>
<p>Therefore, we have</p>
<p><span class="math display">\[\begin{eqnarray}
p(1) &amp;\approx&amp; \frac{\lambda}{1!}p(0) \approx \lambda e^{-\lambda}  \\
p(2) &amp;\approx&amp; \frac{\lambda}{2!}p(1) \approx \frac{\lambda^2}{2} e^{-\lambda}  \\
\dotsm &amp; \dotsm &amp;  \dotsm   \\
p(k) &amp;\approx&amp; \frac{\lambda}{k!}p(k-1) \approx \underbrace{\frac{\lambda^k}{k!} e^{-\lambda}}_{\text{\ see \ \ Eq. (\ref{Eq. Poisson})  }} 
\end{eqnarray}\]</span></p>
<p>thus, we remark that <span class="math inline">\(p(k)\)</span> can be approximated by the probability mass function of a Poisson‚Äîwhich is easier to implement.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-39" class="example"><strong>Example 4.15  (two-fold use of Poisson)  </strong></span>Suppose a certain high-speed printer makes errors at random on printed paper. Assuming that the Poisson
distribution with parameter <span class="math inline">\(\lambda = 4\)</span> is appropriate to model the number of errors per page (say, <span class="math inline">\(X\)</span>), what is the probability that in a book containing 300 pages (produced by the printer) at least 7 will have no errors?</p>
<p>Let <span class="math inline">\(X\)</span> denote the number of errors per page, so that
<span class="math display">\[
p(x) = \exp^{-4}\frac{4^x}{x!}, \quad \text{for} \quad x = 0,1,2,....
\]</span>
The probability of any page to be error free is then
<span class="math display">\[p(0) = \exp^{-4}\frac{4^0}{0!} = \exp^{-4} \approx 0.018.\]</span></p>
<p>Having no errors on a page is a success, and there are 300 independent pages. Hence, let us define</p>
<p><span class="math display">\[
Y = \text{the number of pages without any errors}. 
\]</span></p>
<p><span class="math inline">\(Y\)</span> is binomially distributed with parameters <span class="math inline">\(n = 300\)</span>
and <span class="math inline">\(p = 0.018\)</span>, namely
<span class="math display">\[Y\sim B(n,p).\]</span></p>
But here we have
<p>thus, we can compute <span class="math inline">\(P(\{Y \geq 7\})\)</span> using either the exact Binomial or its Poisson approximation. So</p>
<ul>
<li><p>using <span class="math inline">\(B(300,0.018)\)</span>, we get: <span class="math inline">\(P(\{Y \geq 7\}) \approx 0.297\)</span></p></li>
<li><p>using Poisson(5.4), we get <span class="math inline">\(P(\{Y \geq 7\}) \approx 0.298.\)</span></p></li>
</ul>
</div>
<!--  %The two outcomes differ at the 3rd digits after the comma. -->
</div>
</div>
<div id="the-hypergeometric-distribution" class="section level3" number="4.6.5">
<h3><span class="header-section-number">4.6.5</span> The Hypergeometric Distribution</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-40" class="definition"><strong>Definition 4.9  </strong></span>Let us consider a random experiment consisting of a series of <span class="math inline">\(n\)</span> trials,
having the following properties</p>
<ul>
<li><p>Only two mutually exclusive outcomes are possible in each trials:
success (S) and failure (F)</p></li>
<li><p>The population has <span class="math inline">\(N\)</span> elements in which <span class="math inline">\(k\)</span> are looked upon as
S and the other <span class="math inline">\(N-k\)</span> are looked upon as F</p></li>
<li><p>Sampling from the population is done <strong>without</strong> replacement (so
that the trials are not independent).</p></li>
</ul>
<p>The random variable
<span class="math display">\[X= \text{number of successes in $n$ such trials}\]</span>
has an hypergeometric distribution and the probability that <span class="math inline">\(X=x\)</span> is</p>
<span class="math display">\[\begin{equation*}
\Pr (\left\{ X=x\right\}) =\frac{\left(
\begin{array}{c}
k \\
x%
\end{array}
\right) \left(
\begin{array}{c}
N-k \\
n-x
\end{array}
\right) }{\left(
\begin{array}{c}
N \\
n
\end{array}
\right)}.
\end{equation*}\]</span>
</div>
<p>Moreover,</p>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\frac{nk}{N} \\
Var\left( X\right) &amp;=&amp;\frac{nk\left( N-k\right) \left( N-n\right) }{%
N^{2}\left( N-1\right) }
\end{eqnarray*}\]</span></p>
<div id="illustrations-3" class="section level4" number="4.6.5.1">
<h4><span class="header-section-number">4.6.5.1</span> Illustrations</h4>

<div class="example">
<p><span id="exm:unnamed-chunk-41" class="example"><strong>Example 4.16  </strong></span>[Psychological experiment]</p>
A group of 8 students includes 5 women and 3 men: 3 students are randomly chosen to participate in a psychological
experiment. What is the probability that <em>exactly</em> 2 women will be included
in the sample?%
</div>
<p><img src="img/04_discrete_rv/URNpic_2.png" width="220" style="display: block; margin: auto;" /></p>
<p>Consider each of the three participants being selected as a separate
trial $$ there are <span class="math inline">\(n=3\)</span> trials. Consider a woman being selected in a trial as a `success‚Äô
\
Then here <span class="math inline">\(N=8\)</span>, <span class="math inline">\(k=5\)</span>, <span class="math inline">\(n=3\)</span>, and <span class="math inline">\(x=2\)</span>, so that%
<span class="math display">\[\begin{eqnarray*}
\Pr (\left\{ X=2\right\})  &amp;=&amp;\frac{\left(
\begin{array}{c}
5 \\
2%
\end{array}%
\right) \left(
\begin{array}{c}
8-5 \\
3-2%
\end{array}%
\right) }{\left(
\begin{array}{c}
8 \\
3%
\end{array}%
\right) } \\
&amp;&amp; \\
&amp;=&amp;\frac{\frac{5!}{2!3!}\frac{3!}{1!2!}}{\frac{8!}{5!3!}} \\
&amp;&amp; \\
&amp;=&amp;0.53571
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="the-negative-binomial-distribution" class="section level3" number="4.6.6">
<h3><span class="header-section-number">4.6.6</span> The Negative Binomial Distribution</h3>
<p>Let us consider a random experiment consisting of a series of trials, having
the following properties</p>
<ul>
<li><p>Only two mutually exclusive outcomes are possible in each trial:
<code>success' (S) and</code>failure‚Äô (F)</p></li>
<li><p>The outcomes in the series of trials constitute <em>independent events</em></p></li>
<li><p>The probability of success <span class="math inline">\(p\)</span> in each trial is <em>constant</em> from trial to trial</p></li>
</ul>
<p>What is the probability of having exactly <span class="math inline">\(y\)</span> F‚Äôs before the <span class="math inline">\(r^{th}\)</span> S?</p>
<p>Equivalently: What is the probability that in a sequence of <span class="math inline">\(y+r\)</span> (Bernoulli) trials the last trial yields the <span class="math inline">\(r^{th}\)</span> S?</p>

<div class="definition">
<span id="def:unnamed-chunk-43" class="definition"><strong>Definition 4.10  </strong></span>Let
<span class="math display">\[X= \text{the total number of trials required until a total of $r$ successes is accumulated}.\]</span>
Then <span class="math inline">\(X\)</span> is said to be a Negative Binomial random variable and its probability mass function<br />
<span class="math inline">\(\Pr (\left\{ X=n\right\})\)</span> equals the probability of <span class="math inline">\(r-1\)</span> ‚Äòsuccesses‚Äô in the first <span class="math inline">\(n-1\)</span> trials, times the probability of a ‚Äòsuccess‚Äô on
the last trial. These probabilities are given by%
<span class="math display">\[\begin{equation*}
\Pr (\left\{ X=n\right\}) =\left(
\begin{array}{c}
n-1 \\
r-1
\end{array}
\right) p^{r}\left( 1-p\right) ^{n-r}\quad \text{ for }n=r,r+1,...
\end{equation*}\]</span>
</div>
<p>The mean and variance for <span class="math inline">\(X\)</span> are, respectively,%</p>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\frac{r}{p} \\
Var\left( X\right) &amp;=&amp;\frac{r\left( 1-p\right) }{p^{2}}
\end{eqnarray*}\]</span></p>
</div>
<div id="illustrations-4" class="section level3" number="4.6.7">
<h3><span class="header-section-number">4.6.7</span> Illustrations</h3>

<div class="example">
<p><span id="exm:unnamed-chunk-44" class="example"><strong>Example 4.17  </strong></span>
[marketing research]</p>
<ul>
<li><p>A marketing researcher wants to find 5 people to join her focus group</p></li>
<li><p>Let <span class="math inline">\(p\)</span> denote the probability that a randomly selected individual
agrees to participate in the focus group</p></li>
<li><p>If <span class="math inline">\(p=0.2\)</span>, what is the probability that the researcher must ask 15
individuals before 5 are found who agree to participate?</p></li>
</ul>
<p>%- That is, what is the probability that 10 people will decline the
%request to participate before a 5<span class="math inline">\(^{th}\)</span> person agrees?</p>
<ul>
<li>In this case, <span class="math inline">\(p=0.2\)</span>, <span class="math inline">\(r=5\)</span>, <span class="math inline">\(n=15\)</span>: we are looking for <span class="math inline">\(\Pr (\left\{ X=15\right\}).\)</span> By the negative binomial formula we have</li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
\Pr (\left\{ X=15\right\}) &amp;=&amp;\left(
\begin{array}{c}
14 \\
4%
\end{array}%
\right) \left( 0.2\right) ^{5}\left( 0.8\right) ^{10} \\
&amp;=&amp;0.034
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="the-geometric-distribution" class="section level3" number="4.6.8">
<h3><span class="header-section-number">4.6.8</span> The Geometric Distribution</h3>

<div class="definition">
<p><span id="def:unnamed-chunk-45" class="definition"><strong>Definition 4.11  (a special case)  </strong></span>
When <span class="math inline">\(r=1\)</span>, the negative binomial distribution is equivalent to the <strong>Geometric distribution</strong></p>
In this case, probabilities are given by
<span class="math display">\[\begin{equation*}
\Pr (\left\{ X=n\right\}) =p\left( 1-p\right) ^{n-1}\text{, for }n=1,2,...
\end{equation*}\]</span>
</div>
<p>The corresponding mean and variance for <span class="math inline">\(X\)</span> are, respectively,</p>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\frac{ 1 }{p} \\
Var\left( X\right) &amp;=&amp;\frac{\left( 1-p\right) }{p^{2}}
\end{eqnarray*}\]</span></p>

<div class="example">
<p><span id="exm:unnamed-chunk-46" class="example"><strong>Example 4.18  (failure of a machine)  </strong></span>
Items are produced by a machine having a 3% defective rate.</p>
<ul>
<li>What is the probability that the first defective occurs in the fifth item inspected?
<span class="math display">\[\begin{eqnarray}
P(\{X   =   5\})    &amp;=&amp; P (\text{first  4   non-defective}) P (\text{5th defective})
&amp;=&amp; (0.97)^4(0.03) \approx 0.026 
\end{eqnarray}\]</span></li>
<li>What is the probability that the first defective occurs in the first five inspections?
<span class="math display">\[\begin{eqnarray}
P(\{X   \leq 5  \}) = P(\{X &lt; 6 \}) &amp;=&amp;  P (\{X=1\})+ ... + P(\{X=5\})
&amp;=&amp; 1- P(\text{first 5 non-defective}) = 0.1412.
%&amp;=&amp; 1- (0.97)^5 \approx 0.1412 
\end{eqnarray}\]</span>
</div></li>
</ul>
<p>More generally, for a geometric random variable we have:</p>
<p><span class="math display">\[P(\{X \geq k \}) = (1-p)^{k-1}\]</span></p>
<p>Thus, in the example we have <span class="math inline">\(P( \{X \geq 6 \}) = (1-0.03)^{6-1}\approx 0.8587\)</span></p>
<p><span class="math display">\[\begin{eqnarray}
P(\{X   \leq 5\}) = 1-P( \{X    \geq 6  \}) \approx 1- 0.8587 \approx 0.1412. 
\end{eqnarray}\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="axioms.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="continuousrv.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-discrete_rv.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
