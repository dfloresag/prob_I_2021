<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 🔧 Discrete Random Variables | 🃏 Probability I</title>
<meta name="author" content="Dr. Daniel Flores Agreda (based on the Lecture by Prof. Davide La Vecchia)">
<meta name="description" content="Figure 4.1: ‘Alea Acta Est’ by Enrico Chavez   4.1 What is a Random Variable? Up until now, we have considered probabilities associated with random experiments characterised by different types of...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 4 🔧 Discrete Random Variables | 🃏 Probability I">
<meta property="og:type" content="book">
<meta property="og:description" content="Figure 4.1: ‘Alea Acta Est’ by Enrico Chavez   4.1 What is a Random Variable? Up until now, we have considered probabilities associated with random experiments characterised by different types of...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 🔧 Discrete Random Variables | 🃏 Probability I">
<meta name="twitter:description" content="Figure 4.1: ‘Alea Acta Est’ by Enrico Chavez   4.1 What is a Random Variable? Up until now, we have considered probabilities associated with random experiments characterised by different types of...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="Course Notes">🃏 Probability I</a>:
        <small class="text-muted">Course Notes</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">About this lecture</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="settheory.html"><span class="header-section-number">2</span> Elements of Set Theory for Probability</a></li>
<li><a class="" href="axioms.html"><span class="header-section-number">3</span> Probability Axioms</a></li>
<li><a class="active" href="discreterv.html"><span class="header-section-number">4</span> 🔧 Discrete Random Variables</a></li>
<li><a class="" href="continuousrv.html"><span class="header-section-number">5</span> 🔧 Continuous Random Variable</a></li>
<li><a class="" href="limittheorems.html"><span class="header-section-number">6</span> 📝 Limit Theorems</a></li>
<li><a class="" href="bivariatediscreterv.html"><span class="header-section-number">7</span> 📝 Bivariate Discrete Random Variables</a></li>
<li><a class="" href="numericalmethods.html"><span class="header-section-number">8</span> 📝 Numerical Methods</a></li>
<li><a class="" href="exercise-solutions.html"><span class="header-section-number">9</span> 📝 Exercise Solutions</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="discreterv" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> 🔧 Discrete Random Variables<a class="anchor" aria-label="anchor" href="#discreterv"><i class="fas fa-link"></i></a>
</h1>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-72"></span>
<img src="img/fun/EC_Latin.png" alt="'Alea Acta Est' by Enrico Chavez" width="80%"><p class="caption">
Figure 4.1: ‘Alea Acta Est’ by Enrico Chavez
</p>
</div>
<div id="what-is-a-random-variable" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> What is a Random Variable?<a class="anchor" aria-label="anchor" href="#what-is-a-random-variable"><i class="fas fa-link"></i></a>
</h2>
<p>Up until now, we have considered probabilities associated with random experiments characterised by different types of events. For instance, we’ve illustrated events associated with experiments such as drawing a card (e.g. the card may be ‘hearts or diamonds’) or tossing a coin (e.g. the coins may show two heads ‘<span class="math inline">\(HH\)</span>’). This has led us to characterise events as sets and, using set theory, compute the probability of combinations of sets, (e.g. an event in `<span class="math inline">\(A\cup B^{c}\)</span>’).</p>
<p>To continue further in our path of formalising the theory of probability, we shall introduce the very important notion of Random Variable and start exploring <em>Discrete</em> Random Variables.</p>
<p>Hence, to define a random variable, we need:</p>
<ol style="list-style-type: decimal">
<li>a list of all possible numerical outcomes, and</li>
<li>the probability for each numerical outcome</li>
</ol>
<div id="formal-definition-of-a-random-variable" class="section level3" number="4.1.1">
<h3>
<span class="header-section-number">4.1.1</span> Formal definition of a random variable<a class="anchor" aria-label="anchor" href="#formal-definition-of-a-random-variable"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose we have:</p>
<ol style="list-style-type: lower-alpha">
<li>A sample space <span class="math inline">\(\color{green}{S}\)</span>
</li>
</ol>
<!-- %\item[b.] A $\sigma $-algebra generated by $S$ and denoted by $% --><!-- %\mathcal{B}$ --><ol start="2" style="list-style-type: lower-alpha">
<li>A probability measure (<span class="math inline">\(\color{green}{Pr}\)</span>) defined “using the events” of <span class="math inline">\(\color{green}{S}\)</span>
</li>
</ol>
<p>Let <span class="math inline">\(\color{blue}{X}(\color{green}{s})\)</span> be a function that takes an element <span class="math inline">\(\color{green}{s}\in S\)</span> and maps it to a number <span class="math inline">\(x\)</span></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-77"></span>
<img src="img/04_discrete_rv/charts.010.png" alt="Schematic representation of mapping with a Random Variable" width="80%"><p class="caption">
Figure 4.2: Schematic representation of mapping with a Random Variable
</p>
</div>
<!-- <mark> -->
<!--   Improve this chart. -->
<!-- </mark> -->
</div>
<div id="example-from-s-to-d-via-xcdot" class="section level3" number="4.1.2">
<h3>
<span class="header-section-number">4.1.2</span> Example: from <span class="math inline">\(S\)</span> to <span class="math inline">\(D\)</span>, via <span class="math inline">\(X(\cdot)\)</span><a class="anchor" aria-label="anchor" href="#example-from-s-to-d-via-xcdot"><i class="fas fa-link"></i></a>
</h3>
<div class="inline-figure"><img src="img/04_discrete_rv/C.png" width="554" style="display: block; margin: auto;"></div>
<p>For the elements related to <span class="math inline">\(\color{green}S\)</span> we have a probability <span class="math inline">\(\color{green}{Pr}\)</span></p>
<p>Now define <span class="math inline">\(X(\color{green}{s_{ij}})\)</span> as the sum of the outcome <span class="math inline">\(i\)</span> of the first die and the outcome <span class="math inline">\(j\)</span> of the second die. Thus:</p>
<p><span class="math display">\[\begin{eqnarray*}
X(\color{green}{s_{ij}})= X(i,j)= i+j, &amp; \text{ for } &amp; i=1,...,6,  \text{ and }   j=1,...,6 
\end{eqnarray*}\]</span></p>
<p>In this notation <span class="math inline">\(\color{green}{s_{ij}=(i,j)}\)</span> and <span class="math inline">\(\color{green}{s_{ij}\in S}\)</span>, each having probability <span class="math inline">\(1/36\)</span>.</p>
<p>Let us proceed to formalise this setting with a <strong>Random Variable</strong> and make the mapping explicit:</p>
<ul>
<li>
<span class="math inline">\(X(\cdot)\)</span> maps <span class="math inline">\(\color{green}{S}\)</span> into <span class="math inline">\(\color{blue}D\)</span>. The (new) sample space <span class="math inline">\(\color{blue}{D}\)</span> is given by:
<span class="math display">\[\begin{equation*}
\color{blue}{D=\left\{2,3,4,5,6,7,8,9,10,11,12\right\}}
\end{equation*}\]</span>
where, e.g., <span class="math inline">\(\color{blue}{2}\)</span> is related to the pair <span class="math inline">\((1,1)\)</span>, <span class="math inline">\(\color{blue}{3}\)</span> is related to the pairs <span class="math inline">\((1,2)\)</span> and <span class="math inline">\((2,1)\)</span>, etc etc. So <span class="math inline">\(\color{blue}{D}\)</span> is related the new <span class="math inline">\(\color{blue}{P}\)</span>
<!-- triplet $(\color{blue}{D},\mathcal{B}_{\color{blue}{D}},P)$ . -->
</li>
<li>To each element (event) in <span class="math inline">\(\color{blue}{D}\)</span> we can attach a probability, using the probability of the corresponding event(s) in <span class="math inline">\(S\)</span>. For instance,
<span class="math display">\[P(\color{blue}{2})=Pr(1,1)=1/36, \quad \text{or} \quad P(\color{blue}{3})=Pr(1,2)+Pr(2,1)=2/36.\]</span>
</li>
<li>How about the <span class="math inline">\(P(\color{blue}{7})\)</span>?<br><span class="math display">\[\begin{equation*}
P(\color{blue}7)=Pr(3,4)+Pr(2,5)+Pr(1,6)+Pr(4,3)+Pr(5,2)+Pr(6,1)=6/36. 
\end{equation*}\]</span>
</li>
<li>The latter equality can also be re-written as
<span class="math display">\[P(\color{blue}7)=2(Pr(3,4)+Pr(2,5)+Pr(1,6))=6 \ Pr(3,4),\]</span>
</li>
</ul>
<!-- ## Formal definition of a random variable (II) --><p>Let us formalise all these ideas:</p>
<p>For X to be a random variable it is required that for each event <span class="math inline">\(A\)</span> consisting, if you will, of elements in <span class="math inline">\(D\)</span>:
<span class="math display">\[\begin{equation*}
\color{blue}P\left( A\right) = \color{green} {Pr} ( \left\{ s\in S :X\left(  s  \right) \in A\right\}) 
\end{equation*}\]</span>
where <span class="math inline">\(\color{blue}{P}\)</span> and <span class="math inline">\(\color{green}{Pr}\)</span> stand for “probability” on <span class="math inline">\(\color{blue}{D}\)</span>
and on <span class="math inline">\(\color{green}{S}\)</span>, respectively, we assess the following properties (See Chapter <a href="axioms.html#axioms">3</a>):</p>
<ul>
<li>
<span class="math inline">\(P \left( A\right) \geq 0\)</span> %for all <span class="math inline">\(A\in \mathcal{B}_{D}\)</span>
</li>
<li><span class="math inline">\(\color{blue}P \left( D\right) =\color{green}Pr (\left\{ s\in S:X\left( s\right) \in D\right\}) =Pr \left( S\right) =1\)</span></li>
<li>If <span class="math inline">\(A_{1},A_{2},A_{3}...\)</span> is a sequence of events such that: <span class="math display">\[A_{i}\cap A_{j}=\varnothing\]</span> for all <span class="math inline">\(i\neq j\)</span> then:
<span class="math display">\[\color{blue}P  \left(
\bigcup _{i=1}^{\infty }A_{i}\right) =\sum_{i=1}^{\infty } \color{blue} P\left(
A_{i}\right).\]</span>
</li>
</ul>
<p>In what follows we will be dropping the colors.</p>
</div>
<div id="an-example-from-gambling" class="section level3" number="4.1.3">
<h3>
<span class="header-section-number">4.1.3</span> An Example from gambling<a class="anchor" aria-label="anchor" href="#an-example-from-gambling"><i class="fas fa-link"></i></a>
</h3>
</div>
</div>
<div id="discrete-random-variables" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Discrete random variables<a class="anchor" aria-label="anchor" href="#discrete-random-variables"><i class="fas fa-link"></i></a>
</h2>
<p>Discrete random variables are often associated with the process of counting. The previous example is a good illustration of that use. More generally, we can characterise the probability of any random variable as follows:</p>
<p>For a <strong>discrete random variable <span class="math inline">\(X\)</span></strong>, any table listing all possible nonzero probabilities provides the entire <strong>probability distribution</strong>.</p>
<p>And the <strong>probability mass function</strong> <span class="math inline">\(p(a)\)</span> of <span class="math inline">\(X\)</span> is defined by:
<span class="math display">\[ 
p_a = p(a)= P(\{X=a \}),
\]</span>
and this is positive for at most a countable number of values of <span class="math inline">\(a\)</span>. For instance, <span class="math inline">\(p_{1} = P(\left\{ X=x_1\right\})\)</span>, <span class="math inline">\(p_{2} = P(\left\{ X=x_2\right\})\)</span>, and so on.</p>
<p>That is, if <span class="math inline">\(X\)</span> must assume one of the values <span class="math inline">\(x_1,x_2,...\)</span>, then
<span class="math display">\[\begin{eqnarray}
 p(x_i) \geq 0 &amp; \text{for \ \ } i=1,2,...  \\
 p(x) = 0 &amp; \text{otherwise.}
\end{eqnarray}\]</span></p>
<p>Clearly, we must have
<span class="math display">\[
\sum_{i=1}^{\infty} p(x_i) = 1.
\]</span></p>
<!-- %The probabilities $p_{i}=P\left( X=x_{i}\right)$ may be given by an appropriate mathematical formula: i.e.  -->
<!-- %$$ -->
<!-- %p_{i}=P\left( X=x_{i}\right)=f(x_{i})  -->
<!-- %$$  -->
</div>
<div id="cumulative-distribution-function" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Cumulative Distribution Function<a class="anchor" aria-label="anchor" href="#cumulative-distribution-function"><i class="fas fa-link"></i></a>
</h2>
<p>The <strong>cumulative distribution function (CDF)</strong> is a table listing the values that <span class="math inline">\(X\)</span> can take, alongside the the cumulative probability, i.e.
<span class="math display">\[F_X(a) = P \left(\{ X\leq a\}\right)= \sum_{\text{all } x \leq a } p(x).\]</span></p>
<p>If the random variable <span class="math inline">\(X\)</span> takes on values <span class="math inline">\(x_{1},x_{2},x_{3},\ldots .,x_{n}\)</span> <em>listed in
increasing order</em> <span class="math inline">\(x_{1}&lt;x_{2}&lt;x_{3}&lt;\cdots &lt;x_{n}\)</span>, the CDF is a step function, that it its value is constant in the intervals <span class="math inline">\((x_{i-1},x_i]\)</span> and takes a step/jump of size <span class="math inline">\(p_i\)</span>
at each <span class="math inline">\(x_i\)</span>:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="center"><span class="math inline">\(x_i\)</span></th>
<th align="center"><span class="math inline">\(F_X(x_i)=P\left(\{ X\leq x_i\}\right)\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{1}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_{2}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}+p_{2}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{3}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}+p_{2}+p_{3}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{n}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}+p_{2}+\cdots +p_{n}=1\)</span></td>
</tr>
</tbody>
</table></div>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-85"></span>
<img src="img/04_discrete_rv/repartbis.png" alt="Step function" width="50%"><p class="caption">
Figure 4.3: Step function
</p>
</div>
<p>If we denote the random variable as <span class="math inline">\(R\)</span>, its realisations with <span class="math inline">\(r\)</span> and the CDF evaluated in <span class="math inline">\(r\)</span> as <span class="math inline">\(F_R(r)\)</span>, we can see graphically:
<img src="img/04_discrete_rv/Quantiles_Discr.png" width="1170" style="display: block; margin: auto;"></p>
</div>
<div id="distributional-summaries-for-discrete-random-variables" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Distributional summaries for discrete random variables<a class="anchor" aria-label="anchor" href="#distributional-summaries-for-discrete-random-variables"><i class="fas fa-link"></i></a>
</h2>
<p>In many applications, it is useful to describe some attributes or properties of the distribution of a Random Variable, for instance, to have an overview of how “central” a realisation is or how “spread” or variable the distribution really is. In this section, we will define two of these summaries:</p>
<ul>
<li><p>The <strong>Expectation</strong>, or <strong>Mean</strong> of the distribution is an indicator of “location.” It is defined as the mean of the realisations weighted by their probabilities, i.e. 
<span class="math display">\[\begin{equation*}
E\left[ X\right] =p_{1}x_{1}+p_{2}x_{2}+\cdots + p_{n}x_{n} = \sum_{i=1}^{n} p_i x_i
\end{equation*}\]</span>
Roughly speaking the mean represents the <em>center of gravity</em> of the distribution.</p></li>
<li><p>The <strong>square root of the variance</strong>, or <strong>standard deviation</strong>, of the distribution is a measure of <em>spread</em> and is computed as the average squared distance between the observations with respect to the Expectation.
<span class="math display">\[\begin{eqnarray*}
s.d\left( X\right) &amp;=&amp;\sqrt{Var\left( X\right) } \\
&amp;=&amp;\sqrt{p_{1}\left( x_{1}-E\left[ X\right] \right) ^{2}+p_{2}\left( x_{2}-E
\left[ X\right] \right) ^{2}+\cdots + p_{n}\left( x_{n}-E\left[ X\right]
\right) ^{2}}
\end{eqnarray*}\]</span>
Roughly <em>spread (or ‘variability’ or ‘dispersion’).</em></p></li>
</ul>
<div id="properties" class="section level3" number="4.4.1">
<h3>
<span class="header-section-number">4.4.1</span> Properties<a class="anchor" aria-label="anchor" href="#properties"><i class="fas fa-link"></i></a>
</h3>
<p>If <span class="math inline">\(X\)</span> is a discrete random variable and <span class="math inline">\(a\)</span> is any real number, then</p>
<ul>
<li><span class="math inline">\(E\left[ \alpha X\right] =\alpha E\left[ X\right]\)</span></li>
<li><span class="math inline">\(E\left[ \alpha+X\right] =\alpha+E\left[ X\right]\)</span></li>
<li><span class="math inline">\(Var\left( \alpha X\right) =\alpha^{2}Var\left( X\right)\)</span></li>
<li><span class="math inline">\(Var\left( \alpha+X\right) =Var\left( X\right)\)</span></li>
</ul>
</div>
</div>
<div id="dependenceindependence" class="section level2" number="4.5">
<h2>
<span class="header-section-number">4.5</span> Dependence/Independence<a class="anchor" aria-label="anchor" href="#dependenceindependence"><i class="fas fa-link"></i></a>
</h2>
<div id="more-important-properties" class="section level3" number="4.5.1">
<h3>
<span class="header-section-number">4.5.1</span> More important properties<a class="anchor" aria-label="anchor" href="#more-important-properties"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two discrete random variables, then%
<span class="math display">\[\begin{equation*}
E\left[ X+Y\right] =E\left[ X\right] +E\left[ Y\right]
\end{equation*}\]</span></p></li>
<li><p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are also <em>independent</em>, then
<span class="math display">\[\begin{equation}
Var\left( X+Y\right) =Var\left( X\right) +Var\left( Y\right) \label{Eq. Var}
\end{equation}\]</span></p></li>
</ul>
</div>
<div id="more-on-expectations" class="section level3" number="4.5.2">
<h3>
<span class="header-section-number">4.5.2</span> More on expectations<a class="anchor" aria-label="anchor" href="#more-on-expectations"><i class="fas fa-link"></i></a>
</h3>
<p>Recall that the expectation of X was defined as
<span class="math display">\[\begin{equation*}
E\left[ X\right] = \sum_{i=1}^{n} p_i x_i
\end{equation*}\]</span></p>
<p>Now, suppose we are interested in a function <span class="math inline">\(m\)</span> of the random variable <span class="math inline">\(X\)</span>, say <span class="math inline">\(m(X)\)</span>. We define
<span class="math display">\[\begin{equation*}
E\left[ m\left( X\right) \right] =p_{1}m\left( x_{1}\right) +p_{2}m\left(
x_{2}\right) +\cdots p_{n}m\left( x_{n}\right).
\end{equation*}\]</span></p>
<p>Notice that the variance is a special case of expectation where,
<span class="math display">\[\begin{equation*}
m(X)=(X-E\left[ X\right] )^{2}.
\end{equation*}\]</span>
Indeed,
<span class="math display">\[\begin{equation*}
Var\left( X\right) =E\left[ (X-E\left[ X\right] )^{2}\right].
\end{equation*}\]</span></p>
</div>
</div>
<div id="some-discrete-distributions-of-interest" class="section level2" number="4.6">
<h2>
<span class="header-section-number">4.6</span> Some discrete distributions of interest<a class="anchor" aria-label="anchor" href="#some-discrete-distributions-of-interest"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Discrete Uniform</li>
<li>Bernoulli</li>
<li>Binomial</li>
<li>Poisson</li>
<li>Hypergeometric</li>
<li>Negative binomial</li>
</ul>
<p>Their main characteristic is that the probability <span class="math inline">\(P\left(\left\{ X=x_i\right\}\right)\)</span> is given by an appropriate mathematical formula: i.e. 
<span class="math display">\[p_{i}=P\left(\left\{ X=x_i\right\}\right)=h(x_{i})\]</span>
for a suitably specified function <span class="math inline">\(h(\cdot)\)</span>.</p>
<div id="discrete-uniform-distribution" class="section level3" number="4.6.1">
<h3>
<span class="header-section-number">4.6.1</span> Discrete uniform distribution<a class="anchor" aria-label="anchor" href="#discrete-uniform-distribution"><i class="fas fa-link"></i></a>
</h3>
<div id="expectation" class="section level4" number="4.6.1.1">
<h4>
<span class="header-section-number">4.6.1.1</span> Expectation<a class="anchor" aria-label="anchor" href="#expectation"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>The expected value of <span class="math inline">\(X\)</span> is
<span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;  x_1 p_1 + ... +  x_k p_k\\
&amp;=&amp; 0\cdot \frac{1}{\left( k+1\right) }+1\cdot \frac{1}{%
\left( k+1\right) }+\cdots +k\cdot \frac{1}{\left( k+1\right) } \\
&amp;=&amp;\frac{1}{\left( k+1\right) }\cdot\left( 0+1+\cdots +k\right) \\
&amp;=&amp;\frac{1}{\left( k+1\right) }\cdot \frac{k\left( k+1\right) }{2} \\
&amp;=&amp;\frac{k}{2}.
\end{eqnarray*}\]</span>
</li>
</ul>
<p>E.g. when <span class="math inline">\(k=6\)</span>, then <span class="math inline">\(X\)</span> can take on one of the seven distinct values
<span class="math inline">\(x=0,1,2,3,4,5,6,\)</span> each with equal probability <span class="math inline">\(\frac{1}{7}\)</span>, but the
expected value of <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(3\)</span>, which is one of the possible outcomes!!!</p>
</div>
<div id="variance" class="section level4" number="4.6.1.2">
<h4>
<span class="header-section-number">4.6.1.2</span> Variance<a class="anchor" aria-label="anchor" href="#variance"><i class="fas fa-link"></i></a>
</h4>
<ul>
<li>The variance of <span class="math inline">\(X\)</span> – we will be denoting it as <span class="math inline">\(Var(X)\)</span> – is%
<span class="math display">\[\begin{eqnarray*}
Var\left( X\right) &amp;=&amp;\left( 0-\frac{k}{2}\right) ^{2}\cdot \frac{1}{\left(
k+1\right) }+\left( 1-\frac{k}{2}\right) ^{2}\cdot \frac{1}{\left(
k+1\right) }+ \\
&amp;&amp;\cdots +\left( k-\frac{k}{2}\right) ^{2}\cdot \frac{1}{\left( k+1\right) }
\\
&amp;=&amp;\frac{1}{\left( k+1\right) }\cdot\left\{ \left( 0-\frac{k}{2}\right)
^{2}+\left( 1-\frac{k}{2}\right) ^{2}+\cdots +\left( k-\frac{k}{2}\right)
^{2}\right\} \\
&amp;=&amp;\frac{1}{\left( k+1\right) }\cdot \frac{k\left( k+1\right) \left(
k+2\right) }{12} \\
&amp;=&amp;\frac{k\left( k+2\right) }{12}
\end{eqnarray*}\]</span>
</li>
</ul>
<p>E.g. when <span class="math inline">\(k=6\)</span>, the variance of <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(4,\)</span> and the standard
deviation of <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(\sqrt{4}=2.\)</span></p>
</div>
<div id="illustrations" class="section level4" number="4.6.1.3">
<h4>
<span class="header-section-number">4.6.1.3</span> Illustrations<a class="anchor" aria-label="anchor" href="#illustrations"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-figure"><img src="img/04_discrete_rv/discrete_uniforms__1.png" width="80%" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="bernoulli-trials" class="section level3" number="4.6.2">
<h3>
<span class="header-section-number">4.6.2</span> Bernoulli Trials<a class="anchor" aria-label="anchor" href="#bernoulli-trials"><i class="fas fa-link"></i></a>
</h3>
<p>Often we write the probability mass function (PMF) as:</p>
<p><span class="math display">\[\begin{equation*}
P(\left\{ X=x\right\})=p^{x}\left( 1-p\right) ^{1-x}, \quad \text{ for }x=0,1
\end{equation*}\]</span></p>
<p>A Bernoulli trial represents the most primitive form of all random variables. It derives from a random experiment having only two possible mutually exclusive outcomes. These are often labelled Success and Failure and</p>
<ul>
<li>Success occurs with probability <span class="math inline">\(p\)</span>
</li>
<li>Failure occurs with probability <span class="math inline">\(1-p\)</span>.</li>
</ul>
<div id="expectation-1" class="section level4" number="4.6.2.1">
<h4>
<span class="header-section-number">4.6.2.1</span> Expectation<a class="anchor" aria-label="anchor" href="#expectation-1"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;1 \cdot p+0 \cdot (1-p) \\
&amp;=&amp;p
\end{eqnarray*}\]</span></p>
</div>
<div id="variance-1" class="section level4" number="4.6.2.2">
<h4>
<span class="header-section-number">4.6.2.2</span> Variance<a class="anchor" aria-label="anchor" href="#variance-1"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\begin{eqnarray*}
Var\left( X\right) &amp;=&amp;\left( 1-p\right) ^{2} \cdot p+\left( 0-p\right)
^{2} \cdot \left( 1-p\right) \\
&amp;=&amp;p\left( 1-p\right).
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="the-binomial-distribution" class="section level3" number="4.6.3">
<h3>
<span class="header-section-number">4.6.3</span> The Binomial Distribution<a class="anchor" aria-label="anchor" href="#the-binomial-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>You might recall from Chapter <a href="introduction.html#introduction">1</a> that Combinations are defined as:
<span class="math display">\[\begin{equation*}
{n \choose k} =\frac{n!}{k!\left( n-k\right) !}=C^{k}_{n}
\end{equation*}\]</span>
and, for <span class="math inline">\(n \geq k\)</span>, we say ``<span class="math inline">\(n\)</span> choose <span class="math inline">\(k\)</span>’’.</p>
<p>The binomial coefficient <span class="math inline">\(n \choose k\)</span> represents the number of possible combinations of <span class="math inline">\(n\)</span> objects taken <span class="math inline">\(k\)</span> at a time, without regard of the order. Thus, <span class="math inline">\(C^{k}_{n}\)</span> represents the number of different groups of size <span class="math inline">\(k\)</span> that could be selected from a set of <span class="math inline">\(n\)</span> objects
when the order of selection is not relevant.</p>
<p>So, “What is the interpretation of the formula?”</p>
<ul>
<li>The first factor <span class="math display">\[{n \choose k} =\frac{n!}{x!\left( n-x\right)!}\]</span> is the number of different combinations of individual “successes” and “failures” in <span class="math inline">\(n\)</span> (Bernoulli) trials that result in a sequence containing a total of <span class="math inline">\(x\)</span> ‘successes’ and <span class="math inline">\(n-x\)</span> “failures.”</li>
<li>The second factor <span class="math display">\[p^{x}\left( 1-p\right) ^{n-x}\]</span> is the probability associated with any one
sequence of <span class="math inline">\(x\)</span> ‘successes’ and <span class="math inline">\((n-x)\)</span> `failures’.</li>
</ul>
<div id="expectation-2" class="section level4" number="4.6.3.1">
<h4>
<span class="header-section-number">4.6.3.1</span> Expectation<a class="anchor" aria-label="anchor" href="#expectation-2"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\sum_{x=0}^{n}x\Pr \left\{ X=x\right\} \\
&amp;=&amp;\sum_{x=0}^{n}x {n\choose k} p^{x}\left(1-p\right) ^{n-x} = np
\end{eqnarray*}\]</span></p>
</div>
<div id="variance-2" class="section level4" number="4.6.3.2">
<h4>
<span class="header-section-number">4.6.3.2</span> Variance<a class="anchor" aria-label="anchor" href="#variance-2"><i class="fas fa-link"></i></a>
</h4>
<p><span class="math display">\[\begin{eqnarray*}
Var\left( X\right) &amp;=&amp;\sum_{x=0}^{n}\left( x-np\right) ^{2} P (\left\{
X=x\right\}) \\
&amp;=&amp;np\left( 1-p\right)
\end{eqnarray*}\]</span></p>
<!-- %%EndExpansion -->
<!-- % -->
<!-- %\frametitle{Some illustrations of Binomial \begin{small}(introducing a tool)\end{small}} -->
<!-- %Let us provide a graphical illustration of $X\sim B(x,n,p)$ via some numerical method. Specifically, we first  \color{blue}simulate  a large number of  -->
<!-- %realizations of $X$, then we draw their \color{blue}histogram  (or barplot). \\ -->
<!-- % -->
<!-- %%```{remark} -->
<!-- %A **histogram is a representation of a distribution by means of rectangles whose widths represent class intervals and whose areas are proportional  -->
<!-- %to the corresponding frequencies. The purpose of a histogram is to graphically summarize the distribution of a data set -- roughly speaking, a histogram is a graphical representation of a table of frequencies.  -->
<!-- % -->
<!-- % -->
<!-- %```{remark}[How to draw it?] -->
<!-- %The most common form of  -->
<!-- %the histogram is obtained by splitting the range of the data into equal-sized bins. Then for each bin, the number of points from the data set  -->
<!-- %that fall into each bin are counted. That is: on the \color{blue} vertical axis we read the (relative) frequency  (i.e., counts for each bin); on the \color{red} horizontal axis we read the observed values of $X$. The classes can either be defined arbitrarily by the user or via some systematic rule.   -->
<!-- %``` -->
<!-- % -->
<!-- % -->
<!-- %EndExpansion -->
</div>
<div id="illustrations-1" class="section level4" number="4.6.3.3">
<h4>
<span class="header-section-number">4.6.3.3</span> Illustrations<a class="anchor" aria-label="anchor" href="#illustrations-1"><i class="fas fa-link"></i></a>
</h4>
<p>The visualisation shows some similiarities to the Discrete Uniform but some values seem more probable than others. Moreover, the shape of the distribution seems to vary according to the values of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, i.e the <em>parameters</em> of the distribution.</p>
<div class="inline-figure"><img src="img/04_discrete_rv/distbin.png" width="821" style="display: block; margin: auto;"></div>
<div class="inline-figure"><img src="img/04_discrete_rv/BINOMIALpic__1.png" width="80%" style="display: block; margin: auto;"></div>
</div>
</div>
<div id="poisson-distribution" class="section level3" number="4.6.4">
<h3>
<span class="header-section-number">4.6.4</span> Poisson Distribution<a class="anchor" aria-label="anchor" href="#poisson-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The Eq. () defines a genuine probability mass function, since <span class="math inline">\(p(x) \geq 0\)</span> and</p>
<p><span class="math display">\[\begin{eqnarray}
\sum_{x=0}^{\infty} p(x) &amp;=&amp; \sum_{x=0}^{\infty}  \frac{\lambda ^{x}e^{-\lambda }}{x!}   \\
&amp; = &amp; e^{-\lambda } \sum_{x=0}^{\infty}   \frac{\lambda ^{x}}{x!}  \\
&amp; = &amp; e^{-\lambda } e^{\lambda } = 1  \quad \text{(see Intro Lecture).}
\end{eqnarray}\]</span></p>
<p>Moreover, for a given value of $$ also the CDF can be easily defined. E.g.</p>
<p><span class="math display">\[\begin{equation*}
F_X(2)=\Pr \left( \{X\leq 2\}\right) =e^{-\lambda }+\lambda e^{-\lambda }+\frac{\lambda
^{2}e^{-\lambda }}{2},
\end{equation*}\]</span></p>
<p>and the Expected value and Variance for Poisson distribution (see tutorial) can be obtained by ‘’sum algebra’’ (and/or some algebra)</p>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\lambda \\
Var\left( X\right) &amp;=&amp;\lambda.
\end{eqnarray*}\]</span></p>
<div id="illustrations-2" class="section level4" number="4.6.4.1">
<h4>
<span class="header-section-number">4.6.4.1</span> Illustrations<a class="anchor" aria-label="anchor" href="#illustrations-2"><i class="fas fa-link"></i></a>
</h4>
<p>… same barplot as in slide 30, just a bit fancier…</p>
<div class="inline-figure"><img src="img/fun/distpois.png" width="80%" style="display: block; margin: auto;"></div>
</div>
<div id="link-to-binomial" class="section level4" number="4.6.4.2">
<h4>
<span class="header-section-number">4.6.4.2</span> Link to Binomial<a class="anchor" aria-label="anchor" href="#link-to-binomial"><i class="fas fa-link"></i></a>
</h4>
<p>Let us consider <span class="math inline">\(X \sim B(x,n,p)\)</span>,  where <span class="math inline">\(n\)</span> is large, <span class="math inline">\(p\)</span> is small, and the product <span class="math inline">\(np\)</span> is appreciable. Setting, <span class="math inline">\(\lambda=np\)</span>, we
then have that, for the Binomial probability as in Eq.(), it is a good approximation to write:
<span class="math display">\[
p(k) = P(\{X=k\}) \approx \frac{\lambda^k}{k!} e^{-\lambda}.
\]</span>
To see this, remember that<br><span class="math display">\[
\lim_{n\rightarrow\infty} \left( 1- \frac{\lambda}{n} \right)^n = e^{-\lambda}.
\]</span>
Then, let us consider that in our setting, we have <span class="math inline">\(p=\lambda/n\)</span>. From the formula of the binomial probability mass function we have:
<span class="math display">\[
p(0) = (1-p)^{n}=\left( 1- \frac{\lambda}{n} \right)^{n} \approx e^{-\lambda}, \quad \text{\ as \ \ } n\rightarrow\infty.
\]</span></p>
<p>Moreover, it is easily found that</p>
<p><span class="math display">\[\begin{eqnarray}
\frac{p(k)}{p(k-1)} &amp;=&amp; \frac{np-(k-1)p}{k(1-p)} \approx \frac{\lambda}{k}, \quad \text{\ as \ \ } n\rightarrow\infty. 
\end{eqnarray}\]</span></p>
<p>Therefore, we have</p>
<p><span class="math display">\[\begin{eqnarray}
p(1) &amp;\approx&amp; \frac{\lambda}{1!}p(0) \approx \lambda e^{-\lambda}  \\
p(2) &amp;\approx&amp; \frac{\lambda}{2!}p(1) \approx \frac{\lambda^2}{2} e^{-\lambda}  \\
\dotsm &amp; \dotsm &amp;  \dotsm   \\
p(k) &amp;\approx&amp; \frac{\lambda}{k!}p(k-1) \approx \underbrace{\frac{\lambda^k}{k!} e^{-\lambda}}_{\text{\ see \ \ Eq. (\ref{Eq. Poisson})  }} 
\end{eqnarray}\]</span></p>
<p>thus, we remark that <span class="math inline">\(p(k)\)</span> can be approximated by the probability mass function of a Poisson—which is easier to implement.</p>
<!--  %The two outcomes differ at the 3rd digits after the comma. -->
</div>
</div>
<div id="the-hypergeometric-distribution" class="section level3" number="4.6.5">
<h3>
<span class="header-section-number">4.6.5</span> The Hypergeometric Distribution<a class="anchor" aria-label="anchor" href="#the-hypergeometric-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Moreover,</p>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\frac{nk}{N} \\
Var\left( X\right) &amp;=&amp;\frac{nk\left( N-k\right) \left( N-n\right) }{%
N^{2}\left( N-1\right) }
\end{eqnarray*}\]</span></p>
<div id="illustrations-3" class="section level4" number="4.6.5.1">
<h4>
<span class="header-section-number">4.6.5.1</span> Illustrations<a class="anchor" aria-label="anchor" href="#illustrations-3"><i class="fas fa-link"></i></a>
</h4>
<div class="inline-figure"><img src="img/04_discrete_rv/URNpic_2.png" width="220" style="display: block; margin: auto;"></div>
<p>Consider each of the three participants being selected as a separate
trial $$ there are <span class="math inline">\(n=3\)</span> trials. Consider a woman being selected in a trial as a `success’
\
Then here <span class="math inline">\(N=8\)</span>, <span class="math inline">\(k=5\)</span>, <span class="math inline">\(n=3\)</span>, and <span class="math inline">\(x=2\)</span>, so that%
<span class="math display">\[\begin{eqnarray*}
\Pr (\left\{ X=2\right\})  &amp;=&amp;\frac{\left(
\begin{array}{c}
5 \\
2%
\end{array}%
\right) \left(
\begin{array}{c}
8-5 \\
3-2%
\end{array}%
\right) }{\left(
\begin{array}{c}
8 \\
3%
\end{array}%
\right) } \\
&amp;&amp; \\
&amp;=&amp;\frac{\frac{5!}{2!3!}\frac{3!}{1!2!}}{\frac{8!}{5!3!}} \\
&amp;&amp; \\
&amp;=&amp;0.53571
\end{eqnarray*}\]</span></p>
</div>
</div>
<div id="the-negative-binomial-distribution" class="section level3" number="4.6.6">
<h3>
<span class="header-section-number">4.6.6</span> The Negative Binomial Distribution<a class="anchor" aria-label="anchor" href="#the-negative-binomial-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>Let us consider a random experiment consisting of a series of trials, having
the following properties</p>
<ul>
<li><p>Only two mutually exclusive outcomes are possible in each trial:
<code>success' (S) and</code>failure’ (F)</p></li>
<li><p>The outcomes in the series of trials constitute <em>independent events</em></p></li>
<li><p>The probability of success <span class="math inline">\(p\)</span> in each trial is <em>constant</em> from trial to trial</p></li>
</ul>
<p>What is the probability of having exactly <span class="math inline">\(y\)</span> F’s before the <span class="math inline">\(r^{th}\)</span> S?</p>
<p>Equivalently: What is the probability that in a sequence of <span class="math inline">\(y+r\)</span> (Bernoulli) trials the last trial yields the <span class="math inline">\(r^{th}\)</span> S?</p>
<p>The mean and variance for <span class="math inline">\(X\)</span> are, respectively,%</p>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\frac{r}{p} \\
Var\left( X\right) &amp;=&amp;\frac{r\left( 1-p\right) }{p^{2}}
\end{eqnarray*}\]</span></p>
</div>
<div id="illustrations-4" class="section level3" number="4.6.7">
<h3>
<span class="header-section-number">4.6.7</span> Illustrations<a class="anchor" aria-label="anchor" href="#illustrations-4"><i class="fas fa-link"></i></a>
</h3>
</div>
<div id="the-geometric-distribution" class="section level3" number="4.6.8">
<h3>
<span class="header-section-number">4.6.8</span> The Geometric Distribution<a class="anchor" aria-label="anchor" href="#the-geometric-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The corresponding mean and variance for <span class="math inline">\(X\)</span> are, respectively,</p>
<p><span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\frac{ 1 }{p} \\
Var\left( X\right) &amp;=&amp;\frac{\left( 1-p\right) }{p^{2}}
\end{eqnarray*}\]</span></p>
<p>More generally, for a geometric random variable we have:</p>
<p><span class="math display">\[P(\{X \geq k \}) = (1-p)^{k-1}\]</span></p>
<p>Thus, in the example we have <span class="math inline">\(P( \{X \geq 6 \}) = (1-0.03)^{6-1}\approx 0.8587\)</span></p>
<p><span class="math display">\[\begin{eqnarray}
P(\{X   \leq 5\}) = 1-P( \{X    \geq 6  \}) \approx 1- 0.8587 \approx 0.1412. 
\end{eqnarray}\]</span></p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="axioms.html"><span class="header-section-number">3</span> Probability Axioms</a></div>
<div class="next"><a href="continuousrv.html"><span class="header-section-number">5</span> 🔧 Continuous Random Variable</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#discreterv"><span class="header-section-number">4</span> 🔧 Discrete Random Variables</a></li>
<li>
<a class="nav-link" href="#what-is-a-random-variable"><span class="header-section-number">4.1</span> What is a Random Variable?</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#formal-definition-of-a-random-variable"><span class="header-section-number">4.1.1</span> Formal definition of a random variable</a></li>
<li><a class="nav-link" href="#example-from-s-to-d-via-xcdot"><span class="header-section-number">4.1.2</span> Example: from \(S\) to \(D\), via \(X(\cdot)\)</a></li>
<li><a class="nav-link" href="#an-example-from-gambling"><span class="header-section-number">4.1.3</span> An Example from gambling</a></li>
</ul>
</li>
<li><a class="nav-link" href="#discrete-random-variables"><span class="header-section-number">4.2</span> Discrete random variables</a></li>
<li><a class="nav-link" href="#cumulative-distribution-function"><span class="header-section-number">4.3</span> Cumulative Distribution Function</a></li>
<li>
<a class="nav-link" href="#distributional-summaries-for-discrete-random-variables"><span class="header-section-number">4.4</span> Distributional summaries for discrete random variables</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#properties"><span class="header-section-number">4.4.1</span> Properties</a></li></ul>
</li>
<li>
<a class="nav-link" href="#dependenceindependence"><span class="header-section-number">4.5</span> Dependence/Independence</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#more-important-properties"><span class="header-section-number">4.5.1</span> More important properties</a></li>
<li><a class="nav-link" href="#more-on-expectations"><span class="header-section-number">4.5.2</span> More on expectations</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#some-discrete-distributions-of-interest"><span class="header-section-number">4.6</span> Some discrete distributions of interest</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#discrete-uniform-distribution"><span class="header-section-number">4.6.1</span> Discrete uniform distribution</a></li>
<li><a class="nav-link" href="#bernoulli-trials"><span class="header-section-number">4.6.2</span> Bernoulli Trials</a></li>
<li><a class="nav-link" href="#the-binomial-distribution"><span class="header-section-number">4.6.3</span> The Binomial Distribution</a></li>
<li><a class="nav-link" href="#poisson-distribution"><span class="header-section-number">4.6.4</span> Poisson Distribution</a></li>
<li><a class="nav-link" href="#the-hypergeometric-distribution"><span class="header-section-number">4.6.5</span> The Hypergeometric Distribution</a></li>
<li><a class="nav-link" href="#the-negative-binomial-distribution"><span class="header-section-number">4.6.6</span> The Negative Binomial Distribution</a></li>
<li><a class="nav-link" href="#illustrations-4"><span class="header-section-number">4.6.7</span> Illustrations</a></li>
<li><a class="nav-link" href="#the-geometric-distribution"><span class="header-section-number">4.6.8</span> The Geometric Distribution</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>🃏 Probability I</strong>: Course Notes" was written by Dr. Daniel Flores Agreda (based on the Lecture by Prof. Davide La Vecchia). </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
