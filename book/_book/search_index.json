[["axioms.html", "Chapter 3 ðŸ”§ Probability Axioms 3.1 An Axiomatic Definition of Probability 3.2 Properties of \\(P(\\cdot)\\) 3.3 Illustrations of use 3.4 Conditional probability 3.5 Independence 3.6 Theorem I 3.7 Theorem II", " Chapter 3 ðŸ”§ Probability Axioms 3.1 An Axiomatic Definition of Probability Figure 3.1: â€˜Is it clear to Everyone?â€™ - by Enrico Chavez In order to formalise probability as a branch of mathematics, Andrey Kolmogorov formulated a series of postulates. These axioms are crucial elements of the foundations on which all the mathematical theory of probability is built. Definition 3.1 (Probability Axioms) We define probability as a set function with values in \\([0,1]\\), which satisfies the following axioms: The probability of an event \\(A\\) in the Sample Space \\(S\\) is a non-negative real number \\[\\begin{equation} P(A) \\geq 0, \\text{ for every event } A \\subset S \\tag{3.1} \\end{equation}\\] The probability of the Sample Space is 1 \\[\\begin{equation} P(S)=1 \\tag{3.2} \\end{equation}\\] If \\(A_1,A_2,...\\) is - a sequence of mutually exclusive events, i.e. \\[A_{i}\\cap A_{j}=\\varnothing, \\ \\text{for} \\ i\\neq j,\\ \\text{and} \\ i,j=1,2,...,\\] - such that \\(A = \\bigcup_{i=1}^{\\infty} A_i\\), then: \\[\\begin{equation} P(A)=P\\left(\\bigcup_{i=1}^{\\infty}A_{i}\\right)=\\sum_{i=1}^{\\infty}P(A_i). \\tag{3.3} \\end{equation}\\] 3.2 Properties of \\(P(\\cdot)\\) These three axioms are the building block of other, more sophisticated statements. For instance: Theorem 3.1 (The Probability of the Empty Set) The probability of the empty set is 0 \\[P(\\varnothing)=0.\\] Proof. Consider the sequence of mutually exclusive empty sets. \\(A_1=A_2=A_3=....=\\varnothing\\). Then, by (3.3) in Axiom (ii) we have \\[P(\\varnothing)= P\\left( \\bigcup_{i=1}^{\\infty} A_i \\right) = \\sum_{i=1}^{\\infty} P(A_i) =\\sum_{i=1}^{\\infty} P(\\varnothing)\\] which is true only if the left hand side is an infinite sum of zeros. Thus: \\[P(\\varnothing) = 0.\\] Theorem 3.2 (The Addition Law of Probability) If \\(A_1, A_2,...\\) are mutually exclusive events, then the probability of their union is the sum of their probabilities, i.e. \\[\\begin{equation} P\\left( \\bigcup_{i=1}^{n} A_i \\right) = \\sum_{i=1}^{n} P(A_i). \\tag{3.4} \\end{equation}\\] Proof. Let \\(A_{n+1}=A_{n+2}=....=\\varnothing\\), then \\(\\bigcup_{i=1}^{n} A_i = \\bigcup_{i=1}^{\\infty} A_i,\\) and, from (3.3) (see Axiom (iii)) it follows that: \\[\\begin{eqnarray} P\\left( \\bigcup_{i=1}^{n} A_i \\right) &amp;=&amp; P\\left( \\bigcup_{i=1}^{\\infty} A_i \\right) = \\sum_{i=1}^{\\infty} P(A_i) = \\sum_{i=1}^{n} P(A_i) + \\underbrace{\\sum_{i=n+1}^{\\infty} P(A_i)}_{\\equiv 0}. \\end{eqnarray}\\] Theorem 3.3 (The Complement Rule) If \\(A\\) is an event, then \\(P(A^c) = 1- P(A).\\) Proof. By definition, \\(A\\) and its complement \\(A^c\\) are such that: - \\(A \\cup A^c = S\\) and - \\(A \\cap A^c = \\varnothing\\) Hence, from the addition law 3.2: \\[P( S ) = P\\left(A \\cup A^c \\right) =P(A) + P\\left(A^c \\right).\\] Finally, by Axiom (ii), \\(P(S)=1\\), and. \\[1 = P(A) + P\\left(A^c \\right).\\] The result follows. Theorem 3.4 (The Monotonicity Rule) For any two events \\(A\\) and \\(B\\), such that \\(B \\subset A\\), we have: \\[P(A) \\geq P(B).\\] Illustrate with an example. Insert diagram here Proof. Let us write \\[A = B \\cup (B^c \\cap A) \\] and notice that \\(B \\cap (B^c \\cap A) = \\phi\\), so that \\[\\begin{eqnarray} P(A) &amp;=&amp; P\\left\\{ B \\cup (B^c \\cap A) \\right\\} \\\\ &amp;=&amp; P(B) + P(B^c \\cap A) \\end{eqnarray}\\] which implies (since \\(P(B^c \\cap A) \\geq 0\\)) that \\[ P(A) \\geq P(B). \\] Theorem 3.5 (Booleâ€™s inequality) For the events \\(A_1,A_2,... A_n\\), \\[ P(A_1 \\cup A_2 \\cup....\\cup A_n) \\leq \\sum_{i=1}^{n}P(A_i). \\] To illustrate this property, consider for instance \\(n=2\\). Then we have: \\[ P(A_1 \\cup A_2 ) = P(A_1) + P(A_2) - P(A_1 \\cap A_2) \\leq P(A_1) + P(A_2) \\] since \\(P(A_1 \\cap A_2) \\geq 0\\) by definition. Remark. It is worth noticing that if \\(A_j \\cap A_i = \\varnothing\\), for every \\(i\\) and \\(j\\), with \\(i\\neq j\\), then \\(P(A_1 \\cup A_2 \\cup....\\cup A_n) = \\sum_{i=1}^{n}P(A_i),\\) as stated in (3.4). Theorem 3.6 For any two events \\(A\\) and \\(B\\) then \\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B). \\] Proof. Consider that \\(A\\cup B = A \\cup (A^c \\cap B)\\), and \\(A\\cap(A^c \\cap B) = \\phi\\). Now remember that \\(A^c \\cap B = B -(A \\cap B)\\), so, \\[\\begin{eqnarray} P(A\\cup B) &amp;=&amp; P(A) + P(A^c \\cap B) \\\\ &amp;=&amp; P(A) + P(B) - P(A\\cap B). \\end{eqnarray}\\] 3.3 Illustrations of use 3.3.1 Flipping coins Example 3.1 adapt example here 3.3.2 Detecting shoppers Example 3.2 (Real-life example II) adapt example here 3.3.3 De Morganâ€™s Law Example 3.3 (Application of De Morganâ€™s laws) Given \\(P(A\\cup B)=0.7\\) and \\(P(A\\cup {B}^c) = 0.9\\), find \\(P(A)\\). By De Morganâ€™s law, \\[P(A^c \\cap B^c) = P((A\\cup B )^c) = 1 - P(A\\cup B) = 1 - 0.7 = 0.3\\] and similarly: \\[P(A^c \\cap B) = 1 - P(A \\cup B^c) = 1- 0.9 = 0.1.\\] Thus, \\[P(A^c)=P(A^c \\cap B^c )+P(A^c \\cap B)= 0.3+ 0.1= 0.4,\\] so \\[P(A)=1 - 0.4= 0.6.\\] 3.3.4 Probability, union, and complement Example 3.4 John is taking two books along on his holiday vacation. With probability 0.5, he will like the first book; with probability 0.4, he will like the second book; and with probability 0.3, he will like both books. What is the probability that he likes neither book? \\ Let \\(A_i\\) be the event that John likes book \\(i\\), for \\(i=1,2\\). Then the probability that he likes at least one book is (remember the short hand notation \\(A_1 \\cap A_2 = A_1 A_2\\)) \\[\\begin{eqnarray} P(\\bigcup_{i=1}^2 A_i) &amp;=&amp; P(A_1 \\cup A_2) = P(A_1) + P(A_2) - P(A_1A_2) \\\\ &amp;=&amp; 0.5 + 0.4 -0.3 =0.6. \\end{eqnarray}\\] Because the event the John likes neither books is the complement of the event that he likes at leas one of them (namely \\(A_1 \\cup A_2\\)), we have \\[P(A^{c}_1 \\cap A^{c}_2 ) = P((A_1 \\cup A_2)^c) = 1- P (A_1 \\cup A_2) = 0.4.\\] Example 3.5 # [Real-life example III] Let \\(A\\) denote the event that the midtown temperature in Los Angeles (LA) is 70 F, and let \\(B\\) denote the event that the midtown temperature in New York (NY) is 70 F. Also, let \\(C\\) denote the event that the maximum of midtown temperatures in NY and LA is 70 F. If \\[P(A)=0.3, \\quad P(B)=0.4 \\quad \\text{and} \\quad P(C)=0.2,\\] find the probability that the minimum of the two midtown temperatures is 70 F. Let \\(D\\) denote the event that the minimum temperature is 70 F. Then \\[\\begin{eqnarray} P(A\\cup B) &amp;=&amp; P(A) + P(B) - P(AB) = 0.7 -P(AB) \\\\ P(C\\cup D) &amp;=&amp; P(C) + P(D) - P(CD) = 0.2 -P(D) - P(DC). \\end{eqnarray}\\] Since \\[A\\cup B = C \\cup D \\quad \\text{and} \\quad AB = CD,\\] subtracting one of the preceding equations from the other we get \\[\\begin{eqnarray*} P(A\\cup B)-P(C\\cup D) &amp; = &amp; 0.7 -P(AB) - [0.2 -P(D) - P(DC)] \\\\ &amp; = &amp; 0.5 -P(D) = 0 , \\\\ \\end{eqnarray*}\\] thus $ P(D) =0.5.$ 3.4 Conditional probability As a measure of uncertainty, the probability depends on the information available. Example 3.6 Suppose you have two dice and throw them; the possible outcomes are: knitr::include_graphics(&quot;img/03_axioms/c1.png&quot;) Let us define \\(A\\) = getting \\(5\\), or equivalently \\(A=\\{ 5\\}\\). What is \\(P(A)\\), namely, the probability of getting \\(5\\)? knitr::include_graphics(&quot;img/03_axioms/c2.png&quot;) The dice are fair so we can get 36 events with equal probability \\({1}/{36}\\). Namely: \\[Pr(i,j) = \\frac{1}{36}, \\quad \\text{for} \\quad i,j=1,..,6\\] Thus, we can make use of the blue guys as \\[\\begin{eqnarray} P(5) &amp;=&amp; Pr\\left\\{ (1,4) \\cup (2,3) \\cup (3,2) \\cup (4,1) \\right\\} \\\\ &amp;=&amp; Pr\\left\\{ (1,4) \\right\\} + Pr\\left\\{ (2,3) \\right\\} + Pr\\left\\{(3,2) \\right\\} + Pr\\left\\{ (4,1) \\right\\} \\\\ &amp;=&amp; {1} /{36} + {1} /{36} + {1} /{36} + {1} /{36} \\\\ &amp;=&amp; {4} /{36} \\\\ &amp;=&amp; 1 /{9}. \\end{eqnarray}\\] Now, suppose that we throw the die first and e we get 2.\\ What is the probability of getting 5 given that we have observed 2 in the first throw? knitr::include_graphics(&quot;img/03_axioms/c4.png&quot;) \\(\\text{Pr}\\{\\text{getting 5 given 2 in the first throw}\\}= \\text{Pr}\\{\\text{getting 3 in the second throw}\\}=1/6\\) Remark. Information changes the probability By knowing that we got 2 in the first throw, we have changed the sample space: Probability can change drastically â€” e.g., suppose that in our example we have 6 in the first throw \\(\\Rightarrow\\) the probability of observing 5 in two draws â€¦ is zero !!! Definition 3.2 Let \\(A\\) and \\(B\\) be two events. The conditional probability of event \\(A\\) given event \\(B\\), denoted by \\(P\\left(A\\vert B\\right)\\), is defined by: \\[ P\\left(A\\vert B\\right) = \\frac{P(A \\cap B)}{P(B)}, \\quad \\text{if} \\quad P(B) &gt;0, \\] and it is left undefined if \\(P(B)=0\\). 3.4.1 A check Example 3.7 Let us define the set \\(B\\) as: So we have \\[\\begin{eqnarray} P(B) &amp;=&amp; Pr\\left\\{ (2,1) \\cup (2,2) \\cup (2,3) \\cup (2,4) \\cup (2,5) \\cup (2,6) \\right\\} \\\\ &amp;=&amp; Pr(2,1) + Pr(2,2) + Pr(2,3) + Pr(2,4) + Pr(2,5) + Pr(2,6) \\\\ &amp;=&amp; 6/36 =1/6 \\end{eqnarray}\\] and consider \\(A \\cap B\\) So we have \\(P(A \\cap B) = Pr (2,3) = 1/36\\) so, \\[ P(A\\vert B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{1/36}{1/6} = \\frac{1}{6}. \\] 3.5 Independence Definition 3.3 Two events \\(A\\) and \\(B\\) are independent if the occurrence of one event has no effect on the probability of occurrence of the other event. Thus, \\[P(A\\vert B) = P(A)\\] or equivalently \\[P(B\\vert A) = P(B)\\] Clearly, if \\(P(A\\vert B) \\neq P(A)\\), then \\(A\\) and \\(B\\) are . 3.5.1 Independence â€“ another characterization Two events \\(A\\) and \\(B\\) are independent if \\[P(A \\vert B) = {P(A)},\\] now by definition of conditional probability we know that \\[P(A \\vert B) = \\frac{P(A \\cap B)}{P(B)},\\] so we have \\[P(A) = \\frac{P(A \\cap B)}{P(B)},\\] and rearranging the terms, we find that two events are independent iif \\[P(A\\cap B) = P(A) P(B).\\] Example 3.8 Figure 3.2: things 3.6 Theorem I Theorem 3.7 (Theorem of total probabilities) Let \\(B_1,B_2,...,B_k,...,B_n\\) be mutually disjoint events, satisfying \\(S=\\cup_{i=1}^{n} B_i,\\) and \\(P(B_i)&gt;0\\), for every \\(i=1,2,...,n\\) then for every \\(A\\) we have that: \\[\\begin{equation} P(A)=\\sum_{i=1}^n P(A\\vert B_i) P(B_i). \\tag{3.5} \\end{equation}\\] Proof. Write \\(A=A\\cap S = A\\cap (\\cup_{i=1}^{n} B_i) = \\cup_{i=1}^{n} (A\\cap B_i)\\). Since the \\(\\{B_i \\cap A \\}\\) are mutually disjoint, we have \\[\\begin{equation*} P(A)=P\\left( \\cup_{i=1}^{n} (A\\cap B_i) \\right)=\\sum_{i=1}^n P\\left( A\\cap B_i \\right ) =\\sum_{i=1}^n P(A\\vert B_i) P(B_i). \\end{equation*}\\] Remark. The theorem remains valid even if \\(n=\\infty\\) in Eq. (3.5). (Double check, and re-do the proof using \\(n=\\infty\\).) Corollary 3.1 Let \\(B\\) satisfy \\(0&lt;P(B)&lt;1\\); then for every event \\(A\\): \\[\\begin{equation*} P(A)=P(A\\vert B)P(B)+P(A\\vert B^c) P(B^c) \\end{equation*}\\] Proof. Exercise [Hint: \\(S=B \\cup B^c\\)]. 3.7 Theorem II Theorem I can be applied to derive the well-celebrated Bayesâ€™ Theorem. Theorem 3.8 (Bayesâ€™ Theorem) Let \\(B_1,B_2,...,B_k,...,B_n\\) be mutually disjoint events, satisfying \\[S=\\cup_{i=1}^{n} B_i,\\] and \\(P(B_i)&gt;0,\\) for every \\(i=1,2,...,n\\). Then for every event \\(A\\) for which \\(P(A)&gt;0\\), we have that \\[\\begin{equation} P(B_k\\vert A)=\\frac{P(A\\vert B_k)P(B_k)}{\\sum_{i=1}^n P(A\\vert B_i) P(B_i)}. \\tag{3.6} \\end{equation}\\] Proof. Let us write \\[\\begin{eqnarray} P(B_k\\vert A)&amp;=&amp;\\frac{P(A\\cap B_k)}{P(A)} \\\\ &amp;=&amp;\\frac{P(A\\cap B_k)}{\\sum_{i=1}^n P(A\\vert B_i) P(B_i)} \\\\ &amp;=&amp;\\frac{P(A\\vert B_k)P(B_k)}{\\sum_{i=1}^n P(A\\vert B_i) P(B_i)} \\end{eqnarray}\\] That concludes the proof. Example 3.9 Let us consider a special case, where we have only two events \\(A\\) and \\(B\\). â€¦ so thanks to Bayesâ€™ Theorem we can reverse the role of \\(A\\vert B\\) and \\(B \\vert A\\). 3.7.1 Guessing in a multiple choice exam Example 3.10 (Example 3c in Ross (2014)) In answering a question on a multiple-choice test, a student either knows the answer or guesses. Let p be the probability that the student knows the answer and 1 âˆ’ p be the probability that the student guesses. Assume that a student who guesses at the answer will be correct with probability 1/m, where m is the number of multiple choice alternatives. What is the conditional probability that a student knew the answer to a question given that he or she answered it correctly? Solution Let C and K denote, respectively, the events that the student answers the question correctly and the event that he or she actually knows the answer. Now, \\[\\begin{align*} P(K|C ) &amp; = \\frac{P(KC)}{P(C)} \\\\ &amp;= \\frac{P(C|K)P(K)}{P(C|K )P(K) + P(C|Kc)P(Kc)} \\\\ &amp;= \\frac{p}{p + (1/m)(1 âˆ’ p)} \\\\ &amp;= \\frac{mp}{1 + (m âˆ’ 1)p} \\end{align*}\\] For example, if \\(m = 5\\), \\(p = 12\\) , then the probability that the student knew the answer to a question he or she answered correctly is 5/6 . 3.7.2 Rent car maintenance Example 3.11 - 60% from AVIS - 40% from Mobility Now consider that 9% of the cars from AVIS need a tune-up 20% of the cars from Mobility need a tune-up If a car delivered to the consulting firm needs a tune-up, what is the probability that the care came from AVIS? Let us set: \\(A:=\\{\\text{car rented from AVIS}\\}\\) and \\(B:=\\{\\text{car needs a tune-up}\\}\\). We know \\(P(B\\vert A)\\) and we look for \\(P(A\\vert B)\\) \\(\\Rightarrow\\) Bayesâ€™ theorem!! "]]
