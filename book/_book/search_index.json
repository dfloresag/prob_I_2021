[["continuousrv.html", "Chapter 5 üîß Continuous Random Variable 5.1 Two Motivating Examples 5.2 Cumulative Distribution Function (CDF) 5.3 Distributional Summaries 5.4 Some Important Continuous Distributions 5.5 Transformation of variables 5.6 The big picture", " Chapter 5 üîß Continuous Random Variable Figure 5.1: ‚ÄòStatistical Cow‚Äô by Enrico Chavez 5.1 Two Motivating Examples Example 5.1 (Standard &amp; Poors 500 returns) Let us consider the returns of the S&amp;P 500 index for all the trading days in 1990, 1991,‚Ä¶,1999. Here below, the plot of the returns (in \\(\\%\\) on the y-axis) series over time: Let‚Äôs try to count the relative frequency of each of these returns, in an attempt to estimate the probability of each value of the return. We notice that there are too many different values for the returns. This yields a low value and ‚Äúuniform‚Äù relative frequency which does not help us gain any insight on the probability distribution of the values. Moreover, we could be almost sure that a future return would not have any of the values already obtained. However, we notice that there‚Äôs some concentration, i.e. there are more returns in the neighborhood of zero, as there are in the extreme corners of the possible values. To quantify this concentration, we can create a histogram by splitting the space of possible values into a given number of intervals, or ‚Äúbins,‚Äù and counting the number of observations within a bin. If we consider 30 bins, we obtain the following plot: If we consider separating the space of returns into 300 bins, we obtain the following plot: We could go further and affine the number of bins and the method of counting. One of the options is to estimate a so-called Density Curve, which among other considerations, chooses to split the space into an infinite amount of bins. This plot is much more interesting, as it clearly shows that the returns are quite concentrated around 0. Example 5.2 (Arrivals to the Cafeteria) Let us consider a serious/significant issue: the arrivals to the cafeteria UniMail, from 10AM to 2PM \\[\\begin{eqnarray*} \\mbox{relative freq}&amp;=&amp; \\frac{\\mbox{# customers incoming }}{\\mbox{# total of customers}} \\end{eqnarray*}\\] We want to study the distribution of this object over the considered time interval. E.g. we would like to know when the relative frequency has a peak and when that peak happens. Once again, we can display this phenomenon as a histogram: And we can affine our analysis by increasing the number of bins. In the last one, we see that there is clearly a peak around 12h. If we would like to allocate resources (e.g. hire more service workers to adapt to the demand), we‚Äôd like to call the workers to provide service around those hours. These examples are illustrations of a class of random variables which are different from what we have seen so far. Specifically, the examples emphasise that, unlike discrete random variables, the considered variables are continuous random variables i.e. they can take any value in an interval. This means we cannot simply list all possible values of the random variable, because there are (infinitely many) an uncountable number of possible outcomes that might occur. However, what we can do is to construct a probability distribution by assigning a positive probability to each and every possible interval of values that can occur. This is done by defining the Cumulative Distribution Function (CDF), which is sometimes called Probability Distribution Function. So, graphically, we have that the Discrete Random Variables have a distribution that allocates the probability to the values, represented by bars, whereas Continuous Random Variables have a distribution that allocates probability to intervals, represented by the area below the density curve enclosed by the interval. 5.2 Cumulative Distribution Function (CDF) Definition 5.1 Let \\(X\\) be a continuous random variable and let \\(x\\in \\mathbb{R}\\), here \\(x\\) denotes any number somewhere on the real line \\(\\mathbb{R}=(-\\infty,\\infty)\\). The ‚ÄúProbability Distribution Function‚Äù synonymously, the Cumulative Distribution Function (CDF) of \\(X\\) at the point \\(x\\) is a continuous function \\(\\color{red}{F_{X}\\left( x\\right)}\\) defined such that: \\(\\lim_{x\\rightarrow -\\infty}\\color{red}{F_{X}\\left( x\\right)}=0\\) and \\(\\lim_{x\\rightarrow +\\infty}\\color{red}{F_{X}\\left( x\\right)}=1\\), \\(0\\leq \\color{red}{F_{X}\\left( x\\right)} \\leq 1\\) for all \\(x\\in \\mathbb{R}\\) and the function is monotonically non-decreasing in \\(x\\), i.e. \\[\\color{red}{F_{X}\\left( x\\right)}\\geq \\color{red}{F_{X}\\left( x&#39;\\right)}\\quad\\mbox{for all}\\quad x&gt;x&#39;\\] and the value \\(\\color{red}{F_{X}\\left( x\\right)}\\) yields the probability that \\(X\\) lies in the interval \\((-\\infty,x]\\), i.e \\[\\color{red}{F_{X}\\left( x\\right)}=P\\left( X\\leq x\\right)\\] Definition 5.2 Let \\(X\\) be a random variable taking values in the interval \\((a,b]\\) since: \\(\\color{red}{F_{X}\\left( x\\right)}\\) is zero for all \\(x&lt;a\\) \\(0&lt;\\color{red}{F_{X}\\left( x\\right)}&lt;1\\) for all \\(x\\) in \\((a,b)\\) and \\(\\color{red}{F_{X}\\left( x\\right)}=1\\) for all \\(x\\geq b\\). Then, the Probability Density Function (pdf) of \\(X\\) at the point \\(x\\) is defined as \\[\\color{blue}{f_{X}\\left( x\\right)} =\\frac{d\\color{red}{F_{X}(x)}}{dx}.\\] Graphically, if we represent the values of the pdf with a curve, the CDF will be the area beneath it in the interval considered. Another way of seeing this relationship is with the following plot, where the value of the area undereneath the density is mapped into a new curve that will represent the CDF. From a formal mathematical standpoint, the relationship between PDF and CDF is given by the fundamental theorem of integral calculus. In the illustration, \\(X\\) is a random variable taking values in the interval \\((a,b]\\), and the pdf \\(f_{X}\\left(x\\right)\\) is non-zero only in \\((a,b)\\). More generally we have, for a variable taking values on the whole real line (\\(\\mathbb{R}\\)): the fundamental theorem of integral calculus yields \\[\\color{red}{F_{X}\\left( x\\right)} =P\\left( X\\leq x\\right) =\\int_{-\\infty}^{x}\\color{blue}{f_{X}\\left( t\\right)}dt,\\] the area under the CDF between \\(-\\infty\\) and \\(x\\) or in terms of derivative \\[\\color{blue}{f_{X}\\left( x\\right)} = \\frac{d\\color{red}{F_{X}(x)}}{dx}\\] Most of the PDF‚Äôs that we are going to consider are bell-shaped. So, typically, we will have a density that looks like a bell: and a CDF that has the shape of an S. 5.3 Distributional Summaries 5.3.1 The Expectation Recall that for Discrete random variables, the Expectation results from summing the product of \\(x_{i}\\) and \\(p_{i}=P(X=x_{i})\\), for all possible values \\(x_{i}\\) \\[\\begin{equation*} E\\left[ X\\right] =\\sum_{i}x_{i}p_{i} \\end{equation*}\\] Definition 5.3 (Expectation of a Continuous Random Variable) The Expectation of \\(X\\) results from integrating the product of \\(x\\) and its pdf \\(f_{X}\\left(x\\right)\\) over the range of possible values of \\(x\\). In other words, we obtain the Expectation via integration: \\[\\begin{equation*} E\\left[ X\\right] =\\int_{a}^{b}x\\,f_{X}\\left( x\\right)dx \\end{equation*}\\] 5.3.2 The Variance Recall that, for discrete random variables, we defined the variance as: \\[\\begin{equation*} Var\\left( X\\right) =\\sum_{i}\\left( x_{i}-E\\left[ X\\right] \\right) ^{2}P \\left( X=x_{i}\\right) \\end{equation*}\\] Definition 5.4 (Variance of a Continuous Random Variable) Similarly, for continuous random variables, we use integration to obtain the variance. \\[\\begin{equation*} Var\\left( X\\right) =\\int_{a}^{b}\\left( x-E\\left[ X\\right] \\right)^{2}\\,f_{X}\\left( x\\right) dx \\end{equation*}\\] Very roughly speaking, we could say that we are replacing the sum (\\(\\sum\\)) by its continuous counterpart, namely the integral (\\(\\int\\)). 5.3.3 Important properties of expectations As with discrete random variables, the following properties hold when \\(X\\) is a continuous random variable and \\(c\\) is any real number (namely, \\(c \\in \\mathbb{R}\\)): \\(E\\left[ cX\\right] =cE\\left[ X\\right]\\) \\(E\\left[ c+X\\right] =c+E\\left[ X\\right]\\) \\(Var\\left( cX\\right) =c^{2}Var\\left( X\\right)\\) \\(Var\\left( c+X\\right) =Var\\left( X\\right)\\) Let us consider, for instance, the following proofs for first two properties. To compute \\(E\\left[ cX\\right]\\) we take advantage of the linearity of the integral with respect to multiplication by a constant. \\[\\begin{eqnarray*} E\\left[ cX\\right] &amp;=&amp;\\int \\left( cx\\right) f_{X}\\left( x\\right) dx \\\\ &amp;=&amp;c\\int xf_{X}\\left( x\\right) dx \\\\ &amp;=&amp;cE\\left[ X\\right]. \\end{eqnarray*}\\] In the same way, to evaluate \\(E\\left[ c+X\\right]\\), we take advantage of the linearity of integration and of the fact that \\(f(x)\\) is a density and integrates to 1 over the whole domain of integration. \\[\\begin{eqnarray*} E\\left[ c+X\\right] &amp;=&amp;\\int \\left( c+x\\right) f_{X}\\left( x\\right) dx \\\\ &amp;=&amp;\\int cf_{X}\\left( x\\right) dx+\\int xf_{X}\\left( x\\right) dx \\\\ &amp;=&amp;c\\times 1+E\\left[ X\\right] \\\\ &amp;=&amp;c+E\\left[ X\\right]. \\end{eqnarray*}\\] 5.3.4 Mode and Median There are two other important distributional summaries that also characterise a sort of center for the distribution: the Mode and the Median. The Mode of a continuous random variable having density \\(f_{X}(x)\\) is the value of \\(x\\) for which the PDF \\(f_X(x)\\) attains its maximum, i.e.\\[\\text{Mode}(X) = \\text{argmax}_{x}\\{f_X(x)\\}.\\] Very roughly speaking, it is the point with the highest concentration of probability. On the other hand, the Median of a continuous random variable having CDF \\(F_{X}(x)\\) is the value \\(m\\) such that \\(F(m) = 1/2\\) \\[\\text{Median}(X) = \\text{arg}_{m}\\left\\{P(X\\leq m) = F_X(m) = \\frac{1}{2}\\right\\}.\\] Again, very roughly speaking, the median is the value that splits the sample space in two intervals with equal cumulative probability. Let‚Äôs illustrate these two location values in the pdf. We have purposefully chosen an asymmetric (right)skewed distribution to display the differences between these two values. Figure 5.2: Mode and Median of an asymmetric distribution 5.4 Some Important Continuous Distributions There is a wide variety of Probability Distributions. In this section, we will explore the properties of some of them: Continuous Uniform Gaussian a.k.a. ‚ÄúNormal‚Äù Chi-squared Student‚Äôs \\(t\\) Fisher‚Äôs \\(F\\) Log-Normal Exponential 5.4.1 Continuous Uniform Distribution Definition 5.5 (Continuous Uniform Distribution) We say \\(X\\) has a continuous uniform distribution over the interval \\([a,b]\\), denoted as \\(X\\sim \\mathcal{U}(a,b)\\), when the CDF and pdf are given by \\[ {F_X\\left( x\\right)}=\\left\\{ \\begin{array}{ll} 0, &amp; \\hbox{$x\\leq a$;} \\\\ \\frac{(x-a)}{(b-a)}, &amp; \\hbox{$a&lt;x\\leq b$;} \\\\ 1, &amp; \\hbox{$x&gt;b$.} \\end{array} \\right.\\mbox{and}~{f_{X}\\left( x\\right)} =\\left\\{ \\begin{array}{l} \\frac{1}{b-a}\\text{, for }a&lt;x&lt;b \\\\ 0\\text{, otherwise} \\end{array} \\right. , \\] respectively. As a graphical illustration, let us consider the case when \\(a=0\\) and \\(b=1\\). So, we have that the density is 1 on the interval \\((0,1)\\) and 0 everywhere else: Figure 5.3: Density of a Continuous Uniform Random Variable 5.4.1.1 Expectation The expected value of \\(X\\) can be computed via integration: \\[\\begin{eqnarray*} E\\left[ X\\right] &amp;=&amp;\\int_{a}^{b}\\frac{x}{\\left( b-a\\right) }dx \\\\ &amp;=&amp;\\frac{1}{\\left( b-a\\right)} \\int_{a}^{b} x dx \\\\ &amp;=&amp; \\frac{1}{\\left( b-a\\right)} \\left[\\frac{x^{2}}{2}\\right] _{a}^{b} \\\\ &amp;=&amp;\\frac{1}{\\left(b-a\\right)}\\left[\\frac{b^{2}}{2}-\\frac{a^{2}}{2}\\right]\\\\ &amp;=&amp;\\frac{1}{2\\left(b-a\\right)}\\left[(b-a)(b+a)\\right]\\\\ &amp;=&amp;\\frac{a+b}{2} \\end{eqnarray*}\\] Example 5.3 When \\(a=0\\) and \\(b=1\\), then \\(E\\left[ X\\right] =\\frac{1}{2}\\). 5.4.1.2 Variance By definition, the variance of \\(X\\) is given by: \\[\\begin{eqnarray*} Var\\left( X\\right) &amp;=&amp;\\int_{a}^{b}\\left[] x-\\left( \\frac{a+b}{2}\\right) \\right]^{2}\\frac{1}{b-a}dx \\\\ &amp;=&amp;E\\left[ X^{2}\\right] -E\\left[ X\\right] ^{2} \\end{eqnarray*}\\] Since we know the values of the second term: \\[\\begin{equation*} E\\left[ X\\right] ^{2}=\\left( \\frac{a+b}{2}\\right) ^{2}, \\end{equation*}\\] we only need to work out the first term, i.e. \\[\\begin{eqnarray*} E\\left[ X^{2}\\right] &amp;=&amp;\\int_{a}^{b}\\frac{x^{2}}{b-a}dx =\\left. \\frac{x^{3}}{3\\left( b-a\\right) }\\right\\vert _{a}^{b} \\\\ &amp;=&amp;\\frac{b^{3}-a^{3}}{3\\left( b-a\\right) } = \\frac{(b-a)\\left( ab+a^{2}+b^{2}\\right)}{3\\left( b-a\\right) } \\\\ &amp;=&amp;\\frac{\\left( ab+a^{2}+b^{2}\\right) }{3}. \\end{eqnarray*}\\] Putting both terms together, we get that the variance of \\(X\\) is given by: \\[\\begin{eqnarray*} Var\\left( X\\right) &amp;=&amp;\\frac{\\left( ab+a^{2}+b^{2}\\right) }{3}-\\left( \\frac{% a+b}{2}\\right) ^{2} \\\\ &amp;=&amp;\\frac{1}{12}\\left( a-b\\right) ^{2} \\end{eqnarray*}\\] Example 5.4 For instance, when \\(a=0\\) and \\(b=1\\), then \\(Var\\left( X\\right) =\\frac{1}{12}\\). Example 5.5 Let \\(X \\sim \\mathcal{U}(0,10)\\). Then its pdf is \\(f_X(x) = 1/10=0.1\\) for \\(x\\in(0,10)\\) and zero otherwise. The PDF plot is: Figure 5.4: Density of a Continuous Uniform Random Variable We can compute the probabilities of different intervals: \\[\\begin{align} P(0\\leq x \\leq 1) &amp;= \\int_{0}^{1} 0.1x dx &amp;= &amp;0.1 x \\left.\\right\\vert_{x=0}^{x=1} \\\\ &amp;= 0.1\\cdot(1-0) &amp;= &amp;0.1 \\\\ P(0\\leq x \\leq 2) &amp;= 0.1\\cdot (2-0) &amp;= &amp;0.2 \\\\ P(2\\leq x \\leq 4) &amp;= P(2\\leq x \\leq 4) &amp;= &amp;0.2 \\\\ P(x \\geq 2) &amp;= P(2 &lt; x \\leq 10) &amp;= &amp;0.1(10-2) = 0.8 \\end{align}\\] Let‚Äôs represent graphically the \\(P(X\\geq 2)\\). Indeed, we see that it is represented by the pink rectangle. If we compute the area, it is apparent that its value is \\(0.8\\), as we have obtained with the formula. Figure 5.5: Cumulated Probability between 2 and 10 5.4.2 Normal (Gaussian) distribution The Normal distribution was ‚Äúdiscovered‚Äù in the eighteenth century when scientists observed an astonishing degree of regularity in the behavior of measurement errors. They found that the patterns (distributions) that they observed, and which they attributed to chance, could be closely approximated by continuous curves which they christened the ‚Äúnormal curve of errors.‚Äù The mathematical properties of these curves were first studied by Abraham de Moivre (1667-1745), Pierre Laplace (1749-1827), and then Karl Gauss (1777-1855), who also lent his name to the distribution. Definition 5.6 (The Gaussian Distribution) A variable \\(X\\) is said to have a Gaussian or Normal distribution, with mean \\(\\mu\\) and variance \\(\\sigma ^{2}\\), if its pdf is given by \\[\\begin{equation*} \\phi_{(\\mu,\\sigma)}(x) =\\frac{1}{\\sqrt{2\\pi \\sigma ^{2}}}\\exp{ \\left\\{ -\\frac{1% }{2\\sigma ^{2}}\\left( x-\\mu \\right) ^{2}\\right\\}}~~-\\infty&lt;x&lt;\\infty\\,. \\end{equation*}\\] The short-hand notation is \\(X\\sim \\mathcal{N}\\left( \\mu ,\\sigma ^{2}\\right)\\). There are couple of important properties worth mentioning: A Normal random variable \\(X\\) can take any value \\(x\\in\\mathbb{R}\\). A Normal distribution is completely defined by its mean \\(\\mu\\) and its variance \\(\\sigma ^{2}\\). This means that infinitely many different normal distributions are obtained by varying the parameters \\(\\mu\\) and \\(\\sigma ^{2}\\). To illustrate this property, we plot a series of densities that result from replacing different values of \\(\\mu\\) and \\(\\sigma^2\\) in the following figure. Figure 5.6: Densities for \\(\\color{blue}{X_{1}\\sim\\mathcal{N}(0,1)}\\), \\(\\color{red}{X_{2}\\sim\\mathcal{N}(0,1.5^2)}\\), \\(\\color{forestgreen}{X_{3}\\sim\\mathcal{N}(1,2.5^2)}\\) As we can assess in the previous plot, the PDF of the Normal Distribution is ‚ÄúBell-shaped,‚Äù meaning that looks a little bit like a bell. Moreover, it is: Symmetric, i.e. it is not skewed either to the right or to the left. This also implies that the values of the density are the same for \\(x\\) and \\(-x\\), i.e \\(\\phi_{(\\mu,\\sigma)}(x) = \\phi_{(\\mu,\\sigma)}(-x)\\) Unimodal, meaning that it has only one mode and The Mean, Median and Mode are all equal, owing to the symmetry of the distribution. 5.4.2.1 The Standard Normal distribution First let us establish that \\(\\phi_{(\\mu,\\sigma)}(x)\\) can serve as a density function. To show that, we need to demonstrate that it integrates to 1 in all the domain \\(\\mathbb{R}\\) Integrating with respect to \\(x\\) using integration by substitution we obtain \\[\\begin{eqnarray*} \\int_{-\\infty}^{\\infty}\\phi_{(\\mu,\\sigma)}(x)dx&amp;=&amp; \\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\left\\{-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right\\}}dx \\\\ &amp;=&amp;\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{z^2}{2}\\right\\}}dz \\quad \\text{where: } z=\\frac{x-\\mu}{\\sigma} \\Leftrightarrow dz = \\frac{dx}{\\sigma}. \\end{eqnarray*}\\] But the second integral on the right hand side equals: \\[ \\int_{-\\infty}^{\\infty}\\exp{\\left\\{-\\frac{z^2}{2}\\right\\}}dz = 2\\underbrace{\\int_0^{\\infty}\\exp{\\left\\{-\\frac{z^2}{2}\\right\\}}dz}_{={\\sqrt{2\\pi}} \\big/ {2}} = \\sqrt{2\\pi} \\] which is a known result, known as the Gaussian Integral that cancels out with \\(1/\\sqrt{2\\pi}\\) yielding the integration to 1. Hence, the function \\(\\phi_{(\\mu,\\sigma)}(x)\\) does indeed define the PDF of a random variable with a mean \\(\\mu\\) and a variance of \\(\\sigma^2\\) Remark. Notice that to prove that \\(\\phi_{(\\mu,\\sigma)}(x)\\), we had to transform \\(X\\) to \\(Z\\) via the substitution \\(Z=(X-\\mu)/\\sigma\\). The variable \\(Z\\) is said to be standardised. Notice also that the resulting integrand: \\[ \\frac{1}{\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{z^2}{2}\\right\\}}=\\phi_{(0,1)}(z), \\] is the pdf of a random variable \\(Z\\sim \\mathcal{N}(0,1)\\). If \\(Z\\sim \\mathcal{N}(0,1)\\) then \\(Z\\) is called a standard normal random variable because: \\(E[Z]=0\\) and \\(Var(Z)=1\\) Because of the special role that the standard normal distribution has in calculations involving the normal distribution its pdf is given the special notation \\[\\phi(z)=\\phi_{(0,1)}(z).\\] The basic feature that underlies calculations involving the Normal distribution: \\[X\\sim \\mathcal{N}\\left( \\mu ,\\sigma ^{2}\\right)\\Leftrightarrow Z=\\frac{\\left( X-\\mu \\right) }{\\sigma }\\sim \\mathcal{N}\\left( 0,1\\right)\\] We can always transform from \\(X\\) to \\(Z\\) by ‚Äúshifting‚Äù and ‚Äúre-scaling‚Äù: \\[\\begin{equation*} Z=\\frac{X-\\mu }{\\sigma } \\ (\\text{for the random variable}) \\quad\\mbox{and}\\quad z=\\frac{x-\\mu }{\\sigma }\\, \\ (\\text{for its values}) , \\end{equation*}\\] and return back to \\(X\\) by a ‚Äúre-scaling‚Äù and ‚Äúshifting‚Äù: \\[\\begin{equation*} X=\\sigma Z+\\mu \\ (\\text{for the random variable}) \\quad\\mbox{and}\\quad x=\\sigma z+\\mu\\, \\ (\\text{for its values}) . \\end{equation*}\\] Thus statements about a Normal random variable can always be translated into equivalent statements about a standard Normal random variable, and vice versa. 5.4.2.2 The Normal CDF In pictures: Start from \\(X \\sim \\mathcal{N}(5,3)\\); then define \\(Y=X-5\\), which is a recentered/shifted \\(X\\) (it‚Äôs centered at 0 and has the same variance as \\(X\\)); finally define \\(Z\\), which is a recentered/shifted and rescaled \\(X\\) (it‚Äôs centered at 0 and has unit variance). In formulae: For \\(X\\sim \\mathcal{N}\\left( \\mu ,\\sigma ^{2}\\right)\\), the CDF is given by% \\[\\Phi_{(\\mu,\\sigma)}\\left( x\\right) =\\int_{-\\infty }^{x}\\frac{1}{\\sqrt{2\\pi \\sigma ^{2}}}\\exp{ \\left\\{ -\\frac{1}{2\\sigma ^{2}}\\left( t-\\mu \\right) ^{2}\\right\\}} dt\\] To calculate \\(\\Phi_{(\\mu,\\sigma)}\\left( x\\right)=P(\\{X\\leq x\\})\\) we use integration by substitution, once again, to give \\[\\begin{eqnarray*} P(\\{ X\\leq x\\} )&amp;=&amp;\\int_{-\\infty}^x\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\left\\{-\\frac{(t-\\mu)^2}{2\\sigma^2}\\right\\}}dt\\\\ &amp;=&amp;\\int_{-\\infty}^z\\phi(s)ds\\\\ &amp;=&amp;P(\\{Z\\leq z\\}) \\end{eqnarray*}\\] where \\(z=(x-\\mu)/\\sigma\\), \\(s=(t-\\mu)/\\sigma\\) and \\(ds=dt/\\sigma\\). - The required probability has been mapped into a corresponding probability for a standard Normal random variable. We can evaluate the probabilities \\[P(\\{Z\\leq z\\})=\\Phi(z)=\\int_{-\\infty}^z\\phi(s)ds\\] either directly using a computer or indirectly via Standard Normal Tables. - Standard Normal Tables give values of the integral \\(\\Phi(z)\\) for various values of \\(z\\geq 0\\). (The tables are themselves calculated using a computer, of course.) - For negative values of \\(z\\) the symmetry property of \\(\\phi(z)\\) ( \\(\\phi(z)=\\phi(-z)\\)) tells us that \\[\\Phi(-z)=1-\\Phi(z)\\,\\] - Similarly, if \\(X\\sim \\mathcal{N}\\left( \\mu ,\\sigma ^{2}\\right)\\) then \\[\\begin{eqnarray*} P(\\{x_1&lt;X\\leq x_2\\})&amp;=&amp;P(\\{z_1&lt;Z\\leq z_2\\})\\\\ &amp;=&amp;\\Phi(z_2)-\\Phi(z_1) \\end{eqnarray*}\\] where \\(z_1=(x_1-\\mu)/\\sigma\\) and \\(z_2=(x_2-\\mu)/\\sigma\\). 5.4.2.3 Standard Normal Tables Standard Normal Tables give values of the standard normal integral \\(\\Phi(z)\\) for various values of \\(z\\geq 0\\). Values for negative \\(z\\) are obtained via symmetry. ‚Ä¶. and you can use these tables to compute integrals/probabilities of the type: Example 5.6 [Prob of \\(Z\\)] 5.4.2.4 Some properties of the Normal distribution The shaded areas under the pdfs are (approximately) equivalent to \\(0.683\\), \\(0.954\\) and \\(0.997\\), respectively. So we state the following ‚Ä¶. ‚Ä¶ rule `68 ‚Äì 95 ‚Äì 99.7‚Äô: If \\(X\\) is a Normal random variable, \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), its realization has approximately a probability of \\ For \\(X\\sim \\mathcal{N}\\left( \\mu ,\\sigma ^{2}\\right)\\) \\[\\begin{equation*} E\\left[ X\\right] =\\mu \\text{ and }Var\\left( X\\right) =\\sigma ^{2}. \\end{equation*}\\] If \\(a\\) is a number, then \\[\\begin{eqnarray*} X+a &amp;\\sim &amp;\\mathcal{N}\\left( \\mu +a,\\sigma ^{2}\\right) \\\\ aX &amp;\\sim &amp;\\mathcal{N}\\left( a\\mu ,a^{2}\\sigma ^{2}\\right). \\end{eqnarray*}\\] If \\(X\\sim \\mathcal{N}\\left( \\mu ,\\sigma ^{2}\\right)\\) and \\(Y\\sim \\mathcal{N}\\left( \\alpha,\\delta ^{2}\\right)\\), and \\(X\\) and \\(Y\\) are independent then \\[\\begin{equation*} X+Y\\sim \\mathcal{N}\\left( \\mu +\\alpha ,\\sigma ^{2}+\\delta ^{2}\\right). \\end{equation*}\\] 5.4.2.5 The sum of two independent Normals Locations of \\(n=30\\) sampled values of \\(X,\\) \\(Y\\), and \\(X+Y\\) shown as tick marks under each respective density. 5.4.2.6 Normal: an example Example 5.7 On the highway A2 (in the Luzern area), the speed is limited to \\(80\\) \\(km/h\\). A radar measures the speeds of all the cars. Assuming that the registered speeds are distributed according to a Normal law with mean \\(72\\) \\(km/h\\) and standard error \\(8\\) \\(km/h\\): what is the proportion of the drivers who will have to pay a penalty for high speed? knowing that in addition to the penalty, a speed higher than \\(30\\) \\(km/h\\) (over the max allowed speed) implies a withdrawal of the driving license, what is the proportion of the drivers who will lose their driving license among those who will have a to pay a fine? Let \\(X\\) be the random variable expressing the registered speed: \\(X \\sim \\mathcal{N}(72,64)\\). Since a driver has to pay if its speed is above \\(80\\) \\(km/h\\), the proportion of drivers paying a penalty is expressed through \\(P(X&gt;80)\\): \\[\\begin{equation*} P(X&gt;80)= P\\left(Z&gt;\\frac{80-72}{8} \\right)=1-\\Phi(1) \\simeq 16 \\% \\end{equation*}\\] where \\(Z \\sim \\mathcal{N}(0,1)\\). We are looking for the conditional probability of a recorded speed greater than 110 the driver has had already to pay a fine: \\[\\begin{eqnarray*} P(X&gt;110 \\vert X&gt;80) &amp;=&amp; \\frac{P(\\{X&gt;110\\} \\bigcap \\{X&gt;80\\})}{P(X&gt;80)} \\\\ &amp;=&amp; \\frac{P(X&gt;110)}{P(X&gt;80)} = \\frac{1- \\Phi((110-72)/8)}{1-\\Phi(1)}\\approx \\frac{0}{16\\%}\\simeq 0. \\end{eqnarray*}\\] 5.4.3 The Chi-squared distribution Definition 5.7 If \\(Z_{1},Z_{2},\\ldots ,Z_{n}\\) are independent standard Normal random variables, then% \\[\\begin{equation*} X=Z_{1}^{2}+Z_{2}^{2}+\\cdots +Z_{n}^{2} \\end{equation*}\\]% has a chi-squared distribution with \\(n\\) degrees of freedom. Write as \\(X\\sim \\chi ^{2}(n)\\). \\(X\\sim \\chi ^{2}(n)\\) can take only positive values. Moreover, expected value and variance, for \\(X\\sim \\chi ^{2}(n)\\), are: \\[\\begin{eqnarray*} E\\left[ X\\right] &amp;=&amp;n \\\\ Var\\left( X\\right) &amp;=&amp;2n \\end{eqnarray*}\\] If \\(X\\sim \\chi ^{2}(n)\\) and \\(Y\\sim \\chi ^{2}(m)\\) are independent then \\(X+Y\\sim \\chi ^{2}(n+m)\\). 5.4.3.1 Some plots for the Chi-squared Probabilities for Chi-squared distributions may be obtained from a table 5.4.3.2 Chi-squared table 5.4.3.3 Chi-squared table (illustration of its use) Example 5.8 Let \\(X\\) be a chi-squared random variable with 10 degrees-of-freedom. What is the value of its ? By definition, the upper fifth percentile is the chi-squared value \\(x\\) (lower case!!!) such that the probability to the right of \\(x\\) is \\(0.05\\) (so the upper tail area is \\(5\\%\\)). To find such an \\(x\\) we use the chi-squared table: setting \\(\\mathcal{V} = 10\\) in the first column on the left and getting the corresponding row finding the column headed by \\(P(X \\geq x) = 0.05\\). Now, all we need to do is read the corresponding cell. What do we get? Well, the table tells us that the upper fifth percentile of a chi-squared random variable with 10 degrees of freedom is 18.30703. 5.4.4 The Student-t distribution Definition 5.8 If \\(Z\\sim \\mathcal{N}(0,1)\\) and \\(Y\\sim \\chi ^{2}(v)\\) are independent then% \\[\\begin{equation*} T=\\frac{Z}{\\sqrt{Y/v}} \\end{equation*}\\] has a Student-t distribution with \\(v\\) degrees of freedom. Write as \\(T\\sim t_{v}\\). \\(T\\sim t_{v}\\,\\) can take any value in \\(\\mathbb{R}\\). Expected value and variance for \\(T\\sim t_{v}\\) are \\[\\begin{eqnarray*} E\\left[ T\\right] &amp;=&amp;0\\text{, for }v&gt;1 \\\\ Var\\left( T\\right) &amp;=&amp;\\frac{v}{v-2}\\text{, for }v&gt;2. \\end{eqnarray*}\\] 5.4.4.1 Some Student-t distributions Remark. The pdf of \\(T\\sim t_{v}\\) is similar to a Normal (with mean zero) but with fatter tails. When \\(v\\) is large (typically, \\(v \\geq 120\\)) \\(t_{v}\\) approaches \\(\\mathcal{N}(0,1)\\). 5.4.4.2 Student-t table 5.4.5 The F distribution Definition 5.9 If \\(X\\sim \\chi ^{2}(v_{1})\\) and \\(Y\\sim \\chi ^{2}(v_{2})\\) are independent, then \\[\\begin{equation*} F=\\frac{\\frac{X}{v_{1}}}{\\frac{Y}{v_{2}}}, \\end{equation*}\\]% has an F distribution with \\(v_{1}\\) numerator' and $v_{2}$denominator‚Äô degrees of freedom. Write as \\(F\\sim F_{v_{1},v_{2}}\\). \\(F\\sim F_{v_{1},v_{2}}\\,\\) can take only positive values. Expected value and variance for \\(F\\sim F_{v_{1},v_{2}}\\) (note that the order of the degrees of freedom is important!). \\[\\begin{eqnarray*} E\\left[ F\\right] &amp;=&amp;\\frac{v_{2}}{v_{2}-2}\\text{, for }v_{2}&gt;2 \\\\ Var\\left( F\\right) &amp;=&amp;\\frac{2v_{2}^{2}\\left( v_{1}+v_{2}-2\\right) }{% v_{1}\\left( v_{2}-2\\right) ^{2}\\left( v_{2}-4\\right) }\\text{, for }v_{2}&gt;4. \\end{eqnarray*}\\] 5.4.5.1 Some F distributions 5.4.5.2 F distribution table (5% upper tail) 5.4.6 The lognormal distribution Definition 5.10 \\(Y\\) has a lognormal distribution when \\[\\ln \\left( Y\\right) =X\\] has a Normal distribution. We write \\(Y\\sim\\) $( ,^{2}) $. If \\(Y\\sim\\) \\(\\left( \\mu ,\\sigma ^{2}\\right)\\) then% \\[\\begin{eqnarray*} E\\left[ Y\\right] &amp;=&amp;\\exp^{ \\left( \\mu +\\frac{1}{2}\\sigma ^{2}\\right)} \\\\ Var(Y) &amp;=&amp;\\exp^{ \\left( 2\\mu +\\sigma ^{2}\\right)} \\left( \\exp^{ \\left( \\sigma ^{2}\\right)} -1\\right). \\end{eqnarray*}\\] Let us just see some plots‚Ä¶ more to come later‚Ä¶ 5.4.7 Exponential distribution Definition 5.11 Let \\(X\\) be a continuous random variable, having the following characteristics: [‚Äì] \\(X\\) is defined on the positive real numbers \\(\\left( 0;\\infty \\right)\\) ‚Äî namely \\(\\mathbb{R}^+\\); [‚Äì] the pdf and CDF are \\[\\begin{eqnarray} f_X(x)=\\lambda \\exp^{ -\\lambda x},\\lambda &gt;0; &amp; F_X(x)=1-\\exp (-\\lambda x); \\nn \\end{eqnarray}\\] then we say that \\(X\\) has an exponential distribution. We write \\(X\\sim\\) . For \\(X\\sim\\) we have that: \\[\\begin{eqnarray} E[X]=\\int_{0}^{\\infty }xf_X(x )dx= 1/\\lambda &amp; \\text{and} &amp; Var(X)=\\int_{0}^{\\infty }x^{2}f_X(x )dx-E^{2}(X)=1/\\lambda ^{2}. \\nn \\end{eqnarray}\\] Remark. \\(X\\) is typically applied to model the waiting time until an event occurs, when events are always occurring at a random rate \\(\\lambda &gt;0\\). Moreover, the sum of independent exponential random variables has a Gamma distribution (see tutorial). Example 5.9 Let \\(X\\sim\\) \\((\\lambda)\\), with \\(\\lambda =0.5\\). Thus \\[f_X(x) = \\left\\{ \\begin{array}{ll} 0.5 \\exp (-0.5x) &amp; x&gt;0\\\\ 0 &amp; \\text{otherwise} \\end{array} \\right.\\] Then, find the CDF. % For \\(x&gt;0\\), we have \\[\\begin{eqnarray*} F_{X}(x) &amp; = &amp; \\int_{0}^{x}f_{X}(u)du\\\\ &amp; = &amp; 0.5\\Big( -2\\exp (-0.5u)\\Big) \\bigl|_{u=0}^{u=x}\\\\ &amp; = &amp; 0.5(-2\\exp (-0.5x)+2\\exp (0))\\\\ &amp; = &amp; 1-\\exp (-0.5x) \\end{eqnarray*}\\] so, finally, \\[F_X(x) = \\left\\{ \\begin{array}{ll} 0 &amp; x \\leq 0 \\\\ 1-\\exp (-0.5x)&amp; x&gt;0 \\end{array} \\right.\\] ‚Ä¶and a graphical illustration, with varying \\(\\lambda\\) 5.5 Transformation of variables Consider a random variable \\(X\\) Suppose we are interested in \\(Y=\\psi(X)\\), where \\(\\psi\\) is a one to one function A function \\(\\psi \\left( x\\right)\\) is one to one (1-to-1) if there are no two numbers, \\(x_{1},x_{2}\\) in the domain of \\(\\psi\\) such that \\(\\psi \\left( x_{1}\\right) =\\psi \\left( x_{2}\\right)\\) but \\(x_{1}\\neq x_{2}\\). A sufficient condition for \\(\\psi \\left( x\\right)\\) to be 1-to-1 is that it be monotonically increasing (or decreasing) in \\(x\\). Note that the **inverse} of a 1-to-1 function \\(y=\\psi \\left(x\\right)\\) is a 1-to-1 function \\(\\psi^{-1}\\left( y\\right)\\) such that \\[\\begin{equation*} \\psi ^{-1}\\left( \\psi \\left( x\\right) \\right) =x\\text{ and }\\psi \\left( \\psi ^{-1}\\left( y\\right) \\right) =y. \\end{equation*}\\] To transform \\(X\\) to \\(Y\\), we need to consider all the values \\(x\\) that \\(% X\\) can take We first transform \\(x\\) into values \\(y=\\psi (x)\\) 5.5.1 Transformation of discrete random variables To transform a discrete random variable \\(X\\), into the random variable \\(Y=\\psi (X)\\), we transfer the probabilities for each \\(x\\) to the values $y=( x) $: \\[\\begin{equation*} \\begin{tabular}{l|cll|c} \\multicolumn{2}{l}{_Probability function for_$X$} &amp; &amp; \\multicolumn{2}{l}{_Probability function for_$X$} \\\\ &amp; &amp; &amp; &amp; \\\\ $X$ &amp; $P \\left(\\{ X=x_{i} \\}\\right) =p_{i}$ &amp; &amp; $Y$ &amp; $P \\left(\\{ X=x_{i} \\}\\right) =p_{i}$ \\\\ \\cline{1-2}\\cline{4-5} $x_{1}$ &amp; $p_{1}$ &amp; $\\qquad \\Rightarrow \\qquad$ &amp; $\\psi (x_{1})$ &amp; $p_{1}$ \\\\ $x_{2}$ &amp; $p_{2}$ &amp; &amp; $\\psi (x_{2})$ &amp; $p_{2}$ \\\\ $x_{3}$ &amp; $p_{3}$ &amp; &amp; $\\psi (x_{3})$ &amp; $p_{3}$ \\\\ $\\vdots$ &amp; $\\vdots$ &amp; &amp; $\\vdots$ &amp; $\\vdots$ \\\\ $x_{n}$ &amp; $p_{n}$ &amp; &amp; $\\psi (x_{n})$ &amp; $p_{n}$% \\end{tabular}% \\end{equation*}\\] Note that this is equivalent to applying the function \\(\\psi \\left(\\cdot \\right)\\) inside the probability statements: \\[\\begin{eqnarray*} P \\left( \\{ X=x_{i} \\}\\right) &amp;=&amp;P \\left( \\{\\psi \\left( X\\right) =\\psi \\left( x_{i}\\right) \\} \\right) \\\\ &amp;=&amp;P \\left( \\{ Y=y_{i} \\} \\right) \\\\ &amp;=&amp;p_{i} \\end{eqnarray*}\\] Example 5.10 [option pricing] Let us imagine that we are tossing a balanced coin (\\(p=1/2\\)), and when we get a Head'' ($H$) the stock price moves up of a factor $u$, but when we get aTail‚Äô‚Äô (\\(T\\)) the price moves down of a factor \\(d\\). We denote the price at time \\(t_1\\) by \\(S_1(H)=u S_0\\) if the toss results in head (\\(H\\)), and by \\(S_1(T)=d S_0\\) if it results in tail (\\(T\\)). After the second toss, the price will be one of: Indeed, after two tosses, there are four possible coin sequences, \\[ \\{HH,HT,TH,TT\\} \\] although not all of them result in different stock prices at time \\(t_2\\). Let us set \\(S_0=1\\), \\(u=2\\) and \\(d=1/2\\): we represent the price evolution by a tree: Now consider an European option call with maturity \\(t_2\\) and strike price \\(K=0.5\\), whose random pay-off at \\(t_2\\) is \\(C=\\max(0;S_2-0.5)\\). Thus, \\[\\begin{eqnarray*} C(HH)=\\max(0;4-0.5)=\\$ 3.5 &amp; C(HT)=\\max(0;1-0.5)=\\$ 0.5 \\\\ C(TH)=\\max(0;1-0.5)=\\$ 0.5 &amp; C(TT)=\\max(0;0.25-0.5)=\\$ 0. \\end{eqnarray*}\\] Thus at maturity \\(t_2\\) we have \\[\\begin{equation*} \\begin{tabular}{l|cll|c} \\multicolumn{2}{l}{_Probability function for_ $S_2$} &amp; &amp; \\multicolumn{2}{l}{_Probability function for_ $C$} \\\\ &amp; &amp; &amp; &amp; \\\\ $S_2$ &amp; $P \\left(\\{ X=x_{i} \\}\\right) =p_{i}$ &amp; &amp; $C$ &amp; $P \\left(\\{ C=c_{i} \\}\\right) =p_{i}$ \\\\ \\cline{1-2}\\cline{4-5} $\\$ u^2$ &amp; $p^2$ &amp; $\\qquad \\Rightarrow \\qquad$ &amp; $\\$ 3.5$ &amp; $p^2$ \\\\ $\\$ ud$ &amp; $2p(1-p)$ &amp; &amp; $\\$ 0.5$ &amp; $2p(1-p)$ \\\\ %$\\$ du$ &amp; $(1-p)p$ &amp; &amp; $\\$ 0.5$ &amp; $(1-p)p$ \\\\ $\\$ d^2$ &amp; $(1-p)^2$ &amp; &amp; $\\$ 0$ &amp; $(1-p)^2$% \\end{tabular} \\end{equation*}\\] {Since \\(ud=du\\) the corresponding values of \\(S_2\\) and \\(C\\) can be aggregated, without loss of info.} 5.5.2 Transformation of variables using the CDF We can use the same logic for CDF probabilities, whether the random variables are discrete or continuous Let \\(Y=\\psi \\left( X\\right)\\) with \\(\\psi \\left( x\\right)\\) 1-to-1 and monotone increasing. Then \\[\\begin{eqnarray*} F_{Y}\\left( y\\right) &amp;=&amp;P \\left( \\{ Y\\leq y \\}\\right) \\\\ &amp;=&amp;P \\left( \\{ \\psi \\left( X\\right) \\leq y \\} \\right) =P \\left( \\{ X\\leq \\psi ^{-1}\\left( y\\right) \\} \\right) \\\\ &amp;=&amp;F_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) \\end{eqnarray*}\\] Example 5.11 Let \\(Y=\\psi \\left( X\\right) =\\exp^{ X}\\) where$ XF_X$ on all values \\(x\\in\\mathbb{R}\\) \\[\\begin{eqnarray*} F_{Y}\\left( y\\right) &amp;=&amp;P \\left( \\{ Y\\leq y \\} \\right) \\\\ &amp;=&amp;P \\left( \\{ \\exp^{ X} \\leq y \\} \\right) =P \\left( \\{ X\\leq \\ln \\left( y\\right) \\} \\right) \\\\ &amp;=&amp;F_{X}\\left( \\ln \\left( y\\right) \\right) \\text{ only for }y&gt;0\\text{.} \\end{eqnarray*}\\] 5.5.3 Function 1-to-1 and monotone decreasing Monotone decreasing functions work in a similar way, but require changing of the inequality sign Let \\(Y=\\psi \\left( X\\right)\\) with \\(\\psi \\left( x\\right)\\) 1-to-1 and monotone decreasing. Then \\[\\begin{eqnarray*} F_{Y}\\left( y\\right) &amp;=&amp;P \\left( \\{ Y\\leq y \\} \\right) \\\\ &amp;=&amp;P \\left( \\{ \\psi \\left( X\\right) \\leq y \\} \\right) =P \\left( \\{ X\\geq \\psi ^{-1}\\left( y\\right) \\} \\right) \\\\ &amp;=&amp;1-F_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) \\end{eqnarray*}\\] Example 5.12 Example: let \\(Y=\\psi \\left( X\\right) =-\\exp^ X\\) where \\(% X\\sim F_X\\) on all values $x %TCIMACRO{ }% $% \\[\\begin{eqnarray*} F_{Y}\\left( y\\right) &amp;=&amp;P \\left( \\{ Y\\leq y \\}\\right) =P \\left( \\{ -\\exp ^ X \\leq y \\} \\right) \\\\ &amp;=&amp;P \\left( \\{ \\exp^ X \\geq -y \\} \\right) =P \\left( \\{ X\\geq \\ln \\left( -y\\right) \\} \\right) \\\\ &amp;=&amp;1-F_{X}\\left( \\ln \\left( -y\\right) \\right) \\text{ only for }y&lt;0\\text{.} \\end{eqnarray*}\\] 5.5.4 Transformation of continuous RV through pdf For continuous random variables, if \\(\\psi \\left( x\\right)\\) 1-to-1 and monotone increasing, we have \\[\\begin{equation*} F_{Y}\\left( y\\right) =F_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) \\end{equation*}\\] Notice this implies that the pdf of \\(Y=\\psi \\left( X\\right)\\) must satisfy% \\[\\begin{eqnarray*} f_{Y}\\left( y\\right) &amp;=&amp;\\frac{dF_{Y}\\left( y\\right) }{dy}=\\frac{dF_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) }{dy} \\\\ &amp;=&amp;\\frac{dF_{X}\\left( x\\right) }{dx}\\times \\frac{d\\psi ^{-1}\\left( y\\right) }{dy}\\qquad \\text{{\\small (chain rule)}} \\\\ &amp;=&amp;f_{X}\\left( x\\right) \\times \\frac{d\\psi ^{-1}\\left( y\\right) }{dy}\\qquad \\text{{\\small (derivative of CDF (of }}X\\text{){\\small \\ is pdf)}} \\\\ &amp;=&amp;f_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) \\times \\frac{d\\psi ^{-1}\\left( y\\right) }{dy}\\qquad \\text{{\\small (substitute }}x=\\psi ^{-1}\\left( y\\right) \\text{{\\small )}} \\end{eqnarray*}\\] What happens when \\(\\psi \\left( x\\right)\\) 1-to-1 and monotone **% decreasing}? We have% \\[\\begin{equation*} F_{Y}\\left( y\\right) =1-F_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) \\end{equation*}\\] So now the pdf of \\(Y=\\phi \\left( X\\right)\\) must satisfy \\[\\begin{eqnarray*} f_{Y}\\left( y\\right) &amp;=&amp;\\frac{dF_{Y}\\left( y\\right) }{dy}=-\\frac{% dF_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) }{dy} \\\\ &amp;=&amp;-f_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) \\times \\frac{d\\psi ^{-1}\\left( y\\right) }{dy}\\qquad \\text{{\\small (same reasons as before)}} \\end{eqnarray*}\\] but \\(\\frac{d\\psi ^{-1}\\left( y\\right) }{dy}&lt;0\\) since here \\(\\psi \\left( \\cdot \\right)\\) is monotone decreasing, hence we can write% \\[\\begin{equation*} f_{Y}\\left( y\\right) =f_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) \\times \\left\\vert \\frac{d\\psi ^{-1}\\left( y\\right) }{dy}\\right\\vert \\end{equation*}\\] This expression (called Jacobian-formula) is valid for \\(\\psi \\left( x\\right)\\) 1-to-1 and monotone (whether increasing or decreasing) 5.5.5 Example of transformation using pdf Example 5.13 - So what is the pdf for the lognormal distribution? Recall that \\(Y\\) has a lognormal distribution when \\(\\ln \\left( Y\\right) =X\\) has a Normal distribution \\(\\Rightarrow\\) if \\(X\\sim \\mathcal{N}\\left( \\mu ,\\sigma ^{2}\\right) ,\\) then $ Y=^X$ lognormal_ $( ,^{2}) $ Corresponding to \\(\\psi \\left( x\\right) =\\exp^x\\) and \\(\\psi ^{-1}\\left( y\\right) =\\ln (y)\\) The pdf of \\(X\\) is \\[\\begin{equation*} f_{X}\\left( x\\right) =\\frac{1}{\\sqrt{2\\pi \\sigma ^{2}}}\\exp^{ \\left\\{ -\\frac{1% }{2\\sigma ^{2}}\\left( x-\\mu \\right) ^{2}\\right\\}} \\end{equation*}\\]% for any $-&lt;x&lt;$ Using \\(\\psi \\left( x\\right) =\\exp^x\\) we know we will ll have possible values for \\(Y\\) only on $0&lt;y&lt;$ We know that \\[\\begin{equation*} f_{Y}\\left( y\\right) =f_{X}\\left( \\psi ^{-1}\\left( y\\right) \\right) \\times \\left\\vert \\frac{d\\psi ^{-1}\\left( y\\right) }{dy}\\right\\vert \\end{equation*}\\] And since \\(\\psi ^{-1}\\left( y\\right) =\\ln (y)\\) then \\[\\begin{equation*} \\left\\vert \\frac{d\\psi ^{-1}\\left( y\\right) }{dy}\\right\\vert =\\left\\vert \\frac{1}{y}\\right\\vert \\end{equation*}\\] \\(\\Rightarrow\\) the of \\(Y\\) is \\[\\begin{equation*} f_{Y}\\left( y\\right) =\\frac{1}{y\\sqrt{2\\pi \\sigma ^{2}}}\\exp^{ \\left\\{ -\\frac{1% }{2\\sigma ^{2}}\\left( \\ln (y)-\\mu \\right) ^{2}\\right\\}} \\end{equation*}\\] for any $0&lt;y&lt;$ Both the Normal and the lognormal are characterized by only two parameters (\\(\\mu\\) and \\(\\sigma\\)). The of the lognormal distribution is \\(\\exp^{ \\mu }\\), since \\[ P \\left( \\{ X\\leq \\mu \\} \\right) = 0.5, \\] and hence \\[\\begin{eqnarray*} 0.5 &amp;=&amp;P \\left(\\{ X\\leq \\mu \\}\\right) \\\\ &amp;=&amp;P \\left( \\{\\exp^{X} \\leq \\exp^{ \\mu }\\} \\right) \\\\ &amp;=&amp;P \\left( \\{Y\\leq \\exp^{ \\mu }\\} \\right). \\end{eqnarray*}\\] More generally, for \\(\\alpha\\in[0,1]\\), the \\(\\alpha\\)-th quantile of a r.v. \\(X\\) is the value \\(x_\\alpha\\) such that \\(P(\\{X \\leq x_\\alpha\\})\\geq\\alpha\\). If \\(X\\) si a continuous r.v. we can set \\(P(\\{X \\leq x_\\alpha\\})=\\alpha\\) (as we did, e.g., for the lognormal). 5.5.6 A caveat When \\(X\\) and \\(Y\\) are two random variables, we should pay attention to their transformations. For instance, let us consider \\[ X\\sim \\mathcal{N}(\\mu,\\sigma^2) \\quad \\text{and} \\quad Y\\sim Exp(\\lambda). \\] Then, let‚Äôs transform \\(X\\) and \\(Y\\) in a linear way: \\(Z=X+Y\\). We know that \\[ E[Z] = E[X+Y] = E[X] + E[Y] \\] %so we can rely on the linearity of the expected value. in a nonlinear way \\(W = X/Y\\). One can show that \\[\\color{red} E[W] = E\\left[\\frac{X}{Y}\\right] \\neq \\frac{E[X]}{E[Y]}.\\] 5.6 The big picture Despite exotic names, the common distributions relate to each other in intuitive and interesting ways. Several follow naturally from the Bernoulli distribution, for example. "]]
