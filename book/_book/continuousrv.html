<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 5 🔧 Continuous Random Variable | 🃏 Probability I</title>
<meta name="author" content="Dr. Daniel Flores Agreda (based on the Lecture by Prof. Davide La Vecchia)">
<meta name="description" content="Figure 5.1: ‘Statistical Cow’ by Enrico Chavez   5.1 Two Motivating Examples  Let’s try to count the relative frequency of each of these returns, in an attempt to estimate the probability of each...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 5 🔧 Continuous Random Variable | 🃏 Probability I">
<meta property="og:type" content="book">
<meta property="og:description" content="Figure 5.1: ‘Statistical Cow’ by Enrico Chavez   5.1 Two Motivating Examples  Let’s try to count the relative frequency of each of these returns, in an attempt to estimate the probability of each...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 5 🔧 Continuous Random Variable | 🃏 Probability I">
<meta name="twitter:description" content="Figure 5.1: ‘Statistical Cow’ by Enrico Chavez   5.1 Two Motivating Examples  Let’s try to count the relative frequency of each of these returns, in an attempt to estimate the probability of each...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="Course Notes">🃏 Probability I</a>:
        <small class="text-muted">Course Notes</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">About this lecture</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="settheory.html"><span class="header-section-number">2</span> Elements of Set Theory for Probability</a></li>
<li><a class="" href="axioms.html"><span class="header-section-number">3</span> Probability Axioms</a></li>
<li><a class="" href="discreterv.html"><span class="header-section-number">4</span> 🔧 Discrete Random Variables</a></li>
<li><a class="active" href="continuousrv.html"><span class="header-section-number">5</span> 🔧 Continuous Random Variable</a></li>
<li><a class="" href="limittheorems.html"><span class="header-section-number">6</span> 📝 Limit Theorems</a></li>
<li><a class="" href="bivariatediscreterv.html"><span class="header-section-number">7</span> 📝 Bivariate Discrete Random Variables</a></li>
<li><a class="" href="numericalmethods.html"><span class="header-section-number">8</span> 📝 Numerical Methods</a></li>
<li><a class="" href="exercise-solutions.html"><span class="header-section-number">9</span> 📝 Exercise Solutions</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="continuousrv" class="section level1" number="5">
<h1>
<span class="header-section-number">5</span> 🔧 Continuous Random Variable<a class="anchor" aria-label="anchor" href="#continuousrv"><i class="fas fa-link"></i></a>
</h1>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-119"></span>
<img src="img/fun/EC_Stat_Cow.png" alt="'Statistical Cow' by Enrico Chavez" width="80%"><p class="caption">
Figure 5.1: ‘Statistical Cow’ by Enrico Chavez
</p>
</div>
<div id="two-motivating-examples" class="section level2" number="5.1">
<h2>
<span class="header-section-number">5.1</span> Two Motivating Examples<a class="anchor" aria-label="anchor" href="#two-motivating-examples"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-121-1.png" width="80%" style="display: block; margin: auto;"></div>
<p>Let’s try to count the relative frequency of each of these returns, in an attempt to estimate the probability of each value of the return.</p>
<div class="inline-figure"><img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-122-1.png" width="80%" style="display: block; margin: auto;"></div>
<p>We notice that there are too many different values for the returns. This yields
a low value and “uniform” relative frequency which does not help us gain any insight on the probability distribution of the values. Moreover, we could be almost sure that a future return would not have any of the values already obtained.</p>
<p>However, we notice that there’s some <em>concentration</em>, i.e. there are more returns in the
neighborhood of zero, as there are in the extreme corners of the possible values. To quantify this concentration, we can create a <em>histogram</em> by splitting the space of possible values into a given number of intervals, or “bins,” and counting the number of observations within a bin.</p>
<p>If we consider 30 bins, we obtain the following plot:</p>
<div class="inline-figure"><img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-123-1.png" width="80%" style="display: block; margin: auto;"></div>
<p>If we consider separating the space of returns into 300 <code>bins</code>, we obtain the
following plot:</p>
<div class="inline-figure"><img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-124-1.png" width="80%" style="display: block; margin: auto;"></div>
<p>We could go further and affine the number of bins and the method of counting.
One of the options is to estimate a so-called <em>Density Curve</em>, which among other
considerations, chooses to split the space into an infinite amount of bins.</p>
<div class="inline-figure"><img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-125-1.png" width="80%" style="display: block; margin: auto;"></div>
<p>This plot is much more interesting, as it clearly shows that the returns are
quite concentrated around 0.</p>
<div class="inline-figure"><img src="img/05_continuous_rv/hist1.png" width="80%" style="display: block; margin: auto;"></div>
<p>And we can affine our analysis by increasing the number of bins.</p>
<div class="inline-figure"><img src="img/05_continuous_rv/hist2.png" width="80%" style="display: block; margin: auto;"></div>
<p>In the last one, we see that there is clearly a peak around 12h. If we would like
to allocate resources (e.g. hire more service workers to adapt to the demand),
we’d like to call the workers to provide service around those hours.</p>
<div class="inline-figure"><img src="img/05_continuous_rv/hist3.png" width="80%" style="display: block; margin: auto;"></div>
<p>These examples are illustrations of a class of random variables which are
different from what we have seen so far. Specifically, the examples emphasise
that, unlike discrete random variables, the considered variables are
<strong>continuous random variables</strong> i.e. they can take any value in an interval.</p>
<p>This means we cannot simply <em>list</em> all possible values of the random variable,
because there are (infinitely many) an uncountable number of possible outcomes
that might occur.</p>
<p>However, what we can do is to construct a probability distribution by assigning
a positive <strong>probability to each and every possible interval</strong> of values that
can occur. This is done by defining the <strong>Cumulative Distribution Function (CDF)</strong>,
which is sometimes called <strong>Probability Distribution Function</strong>.</p>
<p>So, graphically, we have that the <strong>Discrete Random Variables</strong> have a distribution that allocates the <strong>probability</strong> to the <strong>values</strong>, represented by <em>bars</em>, whereas <strong>Continuous Random Variables</strong> have a distribution that allocates probability to <strong>intervals</strong>, represented by the <strong>area below the density curve</strong> enclosed by the interval.</p>
<div class="inline-figure"><img src="img/05_continuous_rv/discr_vs_cont-1.png" width="80%" style="display: block; margin: auto;"></div>
</div>
<div id="cumulative-distribution-function-cdf" class="section level2" number="5.2">
<h2>
<span class="header-section-number">5.2</span> Cumulative Distribution Function (CDF)<a class="anchor" aria-label="anchor" href="#cumulative-distribution-function-cdf"><i class="fas fa-link"></i></a>
</h2>
<div class="inline-figure"><img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-131-1.png" width="80%" style="display: block; margin: auto;"></div>
<p>Graphically, if we represent the values of the PDF with a curve, the CDF will be the area beneath it in the interval considered.</p>
<div class="inline-figure"><img src="img/05_continuous_rv/generic_CDF2-1.png" width="80%" style="display: block; margin: auto;"></div>
<!-- % -->
<!-- %-  Let $X$ be a continuous random variable with continuous CDF $F_{X}\left( x\right)$. -->
<!-- % -->
<!-- %-  Then the Probability Density Function (% -->
<!-- %%TCIMACRO{\TeXButton{blue}{\color{blue}}}% -->
<!-- % -->
<!-- %\color{blue}% -->
<!-- % -->
<!-- %PDF% -->
<!-- %%TCIMACRO{\TeXButton{black}{}}% -->
<!-- % -->
<!-- %% -->
<!-- % -->
<!-- %) of $X$ at the point $x$ is defined as -->
<!-- %\begin{equation*} -->
<!-- %\color{blue}{f_{X}\left( x\right) =\frac{dF_{X}(x)}{dx}}\,. -->
<!-- %\end{equation*}% -->
<!-- %%for all $x\in \mathbb{R}$. -->
<!-- %-  For the illustrated CDF we have: -->
<!-- % -->
<!-- %%TCIMACRO{% -->
<!-- %%\FRAME{ftbpF}{4.67in}{2.5624in}{0pt}{}{}{generic_pdf.bmp}{% -->
<!-- %%\special{language "Scientific Word";type "GRAPHIC";maintain-aspect-ratio TRUE;display "USEDEF";valid_file "F";width 4.67in;height 2.5624in;depth 0pt;original-width 13.4859in;original-height 7.389in;cropleft "0";croptop "1";cropright "1";cropbottom "0";filename 'generic_pdf.bmp';file-properties "XNPEU";}}}% -->
<!-- % -->
<!-- %\begin{figure}[ptb]\centering -->
<!-- %\includegraphics[width=0.95\textwidth]{generic_pdf__1.pdf}% -->
<!-- %\end{figure}% -->
<p>Another way of seeing this relationship is with the following plot, where the value of the area undereneath the density is mapped into a new curve that will represent the CDF.</p>
<div class="inline-figure"><img src="img/05_continuous_rv/Diego_F-1.png" width="80%" style="display: block; margin: auto;"></div>
<p>From a formal mathematical standpoint, the relationship between PDF and CDF is given by the <strong><em>fundamental theorem of integral calculus</em></strong>. In the illustration, <span class="math inline">\(X\)</span> is a random variable taking values in the interval <span class="math inline">\((a,b]\)</span>, and the PDF <span class="math inline">\(f_{X}\left(x\right)\)</span> is non-zero only in <span class="math inline">\((a,b)\)</span>. More generally we have, for a variable taking values on the whole real line (<span class="math inline">\(\mathbb{R}\)</span>):</p>
<ul>
<li><p>the <strong><em>fundamental theorem of integral calculus</em></strong> yields
<span class="math display">\[\color{red}{F_{X}\left( x\right)} =P\left( X\leq x\right) =\int_{-\infty}^{x}\color{blue}{f_{X}\left(
t\right)}dt,\]</span>
the area under the CDF between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(x\)</span></p></li>
<li><p>or in terms of derivative
<span class="math display">\[\color{blue}{f_{X}\left( x\right)} = \frac{d\color{red}{F_{X}(x)}}{dx}\]</span></p></li>
</ul>
Most of the PDF’s that we are going to consider are bell-shaped. So, typically, we will have a density that looks like a bell:
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-134"></span>
<img src="img/05_continuous_rv/R_bell_pdf-1.png" alt="A typical bell-shaped Density Function" width="50%"><p class="caption">
Figure 5.2: A typical bell-shaped Density Function
</p>
</div>
and a CDF that has the shape of an S.
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-135"></span>
<img src="img/05_continuous_rv/R_bell_CDF-1.png" alt="A typical S-shaped Cumulative Distribution Function" width="50%"><p class="caption">
Figure 5.3: A typical S-shaped Cumulative Distribution Function
</p>
</div>
</div>
<div id="distributional-summaries" class="section level2" number="5.3">
<h2>
<span class="header-section-number">5.3</span> Distributional Summaries<a class="anchor" aria-label="anchor" href="#distributional-summaries"><i class="fas fa-link"></i></a>
</h2>
<div id="the-expectation" class="section level3" number="5.3.1">
<h3>
<span class="header-section-number">5.3.1</span> The Expectation<a class="anchor" aria-label="anchor" href="#the-expectation"><i class="fas fa-link"></i></a>
</h3>
<p>Recall that for <strong>Discrete</strong> random variables, the Expectation results from <strong>summing the product of <span class="math inline">\(x_{i}\)</span> and <span class="math inline">\(p_{i}=P(X=x_{i})\)</span></strong>, for all <strong>possible values <span class="math inline">\(x_{i}\)</span></strong></p>
<p><span class="math display">\[\begin{equation*}
  E\left[ X\right] =\sum_{i}x_{i}p_{i}
\end{equation*}\]</span></p>
</div>
<div id="the-variance" class="section level3" number="5.3.2">
<h3>
<span class="header-section-number">5.3.2</span> The Variance<a class="anchor" aria-label="anchor" href="#the-variance"><i class="fas fa-link"></i></a>
</h3>
<p>Recall that, for <strong>discrete</strong> random variables, we defined the variance as:</p>
<p><span class="math display">\[\begin{equation*}
Var\left( X\right) =\sum_{i}\left( x_{i}-E\left[ X\right] \right) ^{2}P
\left( X=x_{i}\right)
\end{equation*}\]</span></p>
<p>Very roughly speaking, we could say that we are replacing the sum (<span class="math inline">\(\sum\)</span>) by its continuous counterpart, namely the integral (<span class="math inline">\(\int\)</span>).</p>
<!-- %\frametitle{The expected value of a function of a random variable} -->
<!-- % -->
<!-- %Building on this intuition, we generalise to finding the expected value of $h(X)$.  -->
<!-- % -->
<!-- % -->
<!-- %For **discrete** random variables:% -->
<!-- %\begin{equation*} -->
<!-- %E\left[ h\left( X\right) \right] =\sum_{i}h\left( x_{i}\right) P \left( -->
<!-- %X=x_{i}\right) -->
<!-- %\end{equation*} -->
<!-- % -->
<!-- %  -->
<!-- % -->
<!-- %For **continuous** random variables:% -->
<!-- %\begin{equation*} -->
<!-- %Var\left( X\right) =\int_{a}^{b}h\left( x\right) \,f_{X}\left( x\right) dx -->
<!-- %\end{equation*} -->
</div>
<div id="important-properties-of-expectations" class="section level3" number="5.3.3">
<h3>
<span class="header-section-number">5.3.3</span> Important properties of expectations<a class="anchor" aria-label="anchor" href="#important-properties-of-expectations"><i class="fas fa-link"></i></a>
</h3>
<p>As with discrete random variables, the following properties hold when <span class="math inline">\(X\)</span> is a continuous random variable and <span class="math inline">\(c\)</span> is any real number (namely, <span class="math inline">\(c \in \mathbb{R}\)</span>):</p>
<ul>
<li><span class="math inline">\(E\left[ cX\right] =cE\left[ X\right]\)</span></li>
<li><span class="math inline">\(E\left[ c+X\right] =c+E\left[ X\right]\)</span></li>
<li><span class="math inline">\(Var\left( cX\right) =c^{2}Var\left( X\right)\)</span></li>
<li><span class="math inline">\(Var\left( c+X\right) =Var\left( X\right)\)</span></li>
</ul>
<p>Let us consider, for instance, the following proofs for first two properties.
To compute <span class="math inline">\(E\left[ cX\right]\)</span> we take advantage of the <em>linearity</em> of the
integral with respect to multiplication by a constant.
<span class="math display">\[\begin{eqnarray*}
E\left[ cX\right] &amp;=&amp;\int \left( cx\right) f_{X}\left( x\right) dx \\
&amp;=&amp;c\int xf_{X}\left( x\right) dx \\
&amp;=&amp;cE\left[ X\right].
\end{eqnarray*}\]</span></p>
<p>In the same way, to evaluate <span class="math inline">\(E\left[ c+X\right]\)</span>, we take advantage of the
linearity of integration and of the fact that <span class="math inline">\(f(x)\)</span> is a density and integrates to 1 over the
whole domain of integration.
<span class="math display">\[\begin{eqnarray*}
E\left[ c+X\right] &amp;=&amp;\int \left( c+x\right) f_{X}\left( x\right) dx \\
&amp;=&amp;\int cf_{X}\left( x\right) dx+\int xf_{X}\left( x\right) dx \\
&amp;=&amp;c\times 1+E\left[ X\right] \\
&amp;=&amp;c+E\left[ X\right].
\end{eqnarray*}\]</span></p>
</div>
<div id="mode-and-median" class="section level3" number="5.3.4">
<h3>
<span class="header-section-number">5.3.4</span> Mode and Median<a class="anchor" aria-label="anchor" href="#mode-and-median"><i class="fas fa-link"></i></a>
</h3>
<p>There are two other important distributional summaries that also characterise a sort of center for the distribution: the Mode and the Median.</p>
<p>The <strong>Mode</strong> of a continuous random variable having density <span class="math inline">\(f_{X}(x)\)</span> is the <strong>value of <span class="math inline">\(x\)</span> for which the PDF <span class="math inline">\(f_X(x)\)</span> attains its maximum</strong>, i.e.<span class="math display">\[\text{Mode}(X) = \text{argmax}_{x}\{f_X(x)\}.\]</span> Very roughly speaking, it is the <strong>point with the highest <em>concentration</em> of probability</strong>.</p>
<p>On the other hand, the <strong>Median</strong> of a continuous random variable having CDF <span class="math inline">\(F_{X}(x)\)</span> is the <strong>value <span class="math inline">\(m\)</span> such that <span class="math inline">\(F(m) = 1/2\)</span></strong>
<span class="math display">\[\text{Median}(X) = \text{arg}_{m}\left\{P(X\leq m) = F_X(m) = \frac{1}{2}\right\}.\]</span>
Again, very roughly speaking, the median is the value that <strong>splits the sample space in two intervals with equal cumulative probability</strong>.</p>
<p>Let’s illustrate these two location values in the PDF. We have purposefully chosen an asymmetric (right)<em>skewed</em> distribution to display the differences between these two values.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-138"></span>
<img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-138-1.png" alt="Mode and Median of an asymmetric distribution" width="80%"><p class="caption">
Figure 5.4: Mode and Median of an asymmetric distribution
</p>
</div>
</div>
</div>
<div id="some-important-continuous-distributions" class="section level2" number="5.4">
<h2>
<span class="header-section-number">5.4</span> Some Important Continuous Distributions<a class="anchor" aria-label="anchor" href="#some-important-continuous-distributions"><i class="fas fa-link"></i></a>
</h2>
<p>There is a wide variety of Probability Distributions. In this section, we will explore the properties of some of them, namely:</p>
<ul>
<li>Continuous Uniform</li>
<li>Gaussian a.k.a. “Normal”</li>
<li>Chi-squared</li>
<li>Student’s <span class="math inline">\(t\)</span>
</li>
<li>Fisher’s <span class="math inline">\(F\)</span>
</li>
<li>Log-Normal</li>
<li>Exponential</li>
</ul>
<div id="continuous-uniform-distribution" class="section level3" number="5.4.1">
<h3>
<span class="header-section-number">5.4.1</span> Continuous Uniform Distribution<a class="anchor" aria-label="anchor" href="#continuous-uniform-distribution"><i class="fas fa-link"></i></a>
</h3>
<!-- %-  The PDF is given by% -->
<!-- %\begin{equation*} -->
<!-- %f_{X}\left( x\right) =\left\{ -->
<!-- %\begin{array}{l} -->
<!-- %\frac{1}{b-a}\text{, for }a<x<b \\ -->
<!-- %0\text{, \quad otherwise}% -->
<!-- %\end{array}% -->
<!-- %\right. -->
<!-- %\end{equation*} -->
<!-- % -->
<!-- %% -->
<!-- % -->
<!-- % -->
<!-- %\frametitle{Continuous uniform distribution} -->
<!-- % -->
<!-- %```{definition} -->
<!-- %We say $X$ has a continuous **uniform** distribution over the -->
<!-- %interval $(a,b]$, denoted as $X\sim Unif(a,b)$, when the CDF and PDF are given by -->
<!-- %$$ -->
<!-- %\color{red}{F_X\left( x\right)}=\left\{ -->
<!-- %                           \begin{array}{ll} -->
<!-- %                             0, & \hbox{$x\leq a$;} \\ -->
<!-- %                             \frac{(x-a)}{(b-a)}, & \hbox{$a<x\leq b$;} \\ -->
<!-- %                             1, & \hbox{$x>b$.} -->
<!-- %                           \end{array} -->
<!-- %                         \right.\mbox{and}~\color{blue}{f_{X}\left( x\right)} =\left\{ -->
<!-- %\begin{array}{l} -->
<!-- %\frac{1}{b-a}\text{, for }a<x<b \\ -->
<!-- %0\text{, \quad otherwise}% -->
<!-- %\end{array}% -->
<!-- %\right. , -->
<!-- %$$ -->
<!-- %respectively. -->
<!-- %``` -->
<!-- %%-  The PDF is given by% -->
<!-- %%\begin{equation*} -->
<!-- %%f_{X}\left( x\right) =\left\{ -->
<!-- %%\begin{array}{l} -->
<!-- %%\frac{1}{b-a}\text{, for }a<x<b \\ -->
<!-- %%0\text{, \quad otherwise}% -->
<!-- %%\end{array}% -->
<!-- %%\right. -->
<!-- %%\end{equation*} -->
<!-- %% -->
As a graphical illustration, let us consider the case when <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=1\)</span>. So, we have that the density is 1 on the interval <span class="math inline">\((0,1)\)</span> and 0 everywhere else:
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-139"></span>
<img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-139-1.png" alt="Density of a Continuous Uniform Random Variable" width="80%"><p class="caption">
Figure 5.5: Density of a Continuous Uniform Random Variable
</p>
</div>
<div id="expectation-3" class="section level4" number="5.4.1.1">
<h4>
<span class="header-section-number">5.4.1.1</span> Expectation<a class="anchor" aria-label="anchor" href="#expectation-3"><i class="fas fa-link"></i></a>
</h4>
<p>The expected value of <span class="math inline">\(X\)</span> can be computed via integration:
<span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;\int_{a}^{b}\frac{x}{\left( b-a\right) }dx \\
&amp;=&amp;\frac{1}{\left( b-a\right)} \int_{a}^{b} x dx \\
&amp;=&amp; \frac{1}{\left( b-a\right)} \left[\frac{x^{2}}{2}\right] _{a}^{b} \\
&amp;=&amp;\frac{1}{\left(b-a\right)}\left[\frac{b^{2}}{2}-\frac{a^{2}}{2}\right]\\
&amp;=&amp;\frac{1}{2\left(b-a\right)}\left[(b-a)(b+a)\right]\\
&amp;=&amp;\frac{a+b}{2}
\end{eqnarray*}\]</span></p>
</div>
<div id="variance-3" class="section level4" number="5.4.1.2">
<h4>
<span class="header-section-number">5.4.1.2</span> Variance<a class="anchor" aria-label="anchor" href="#variance-3"><i class="fas fa-link"></i></a>
</h4>
<p>By definition, the variance of <span class="math inline">\(X\)</span> is given by:
<span class="math display">\[\begin{eqnarray*}
Var\left( X\right) 
&amp;=&amp;\int_{a}^{b}\left[] x-\left( \frac{a+b}{2}\right)
\right]^{2}\frac{1}{b-a}dx \\
&amp;=&amp;E\left[ X^{2}\right] -E\left[ X\right] ^{2}
\end{eqnarray*}\]</span></p>
<p>Since we know the values of the second term:
<span class="math display">\[\begin{equation*}
E\left[ X\right] ^{2}=\left( \frac{a+b}{2}\right) ^{2},
\end{equation*}\]</span>
we only need to work out the first term, i.e. 
<span class="math display">\[\begin{eqnarray*}
E\left[ X^{2}\right] &amp;=&amp;\int_{a}^{b}\frac{x^{2}}{b-a}dx =\left. \frac{x^{3}}{3\left( b-a\right) }\right\vert _{a}^{b} \\
&amp;=&amp;\frac{b^{3}-a^{3}}{3\left( b-a\right) } = \frac{(b-a)\left( ab+a^{2}+b^{2}\right)}{3\left( b-a\right) } \\
&amp;=&amp;\frac{\left( ab+a^{2}+b^{2}\right) }{3}.
\end{eqnarray*}\]</span></p>
<p>Putting both terms together, we get that the variance of <span class="math inline">\(X\)</span> is given by:
<span class="math display">\[\begin{eqnarray*}
Var\left( X\right) &amp;=&amp;\frac{\left( ab+a^{2}+b^{2}\right) }{3}-\left( \frac{%
a+b}{2}\right) ^{2} \\
&amp;=&amp;\frac{1}{12}\left( a-b\right) ^{2}
\end{eqnarray*}\]</span></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-143"></span>
<img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-143-1.png" alt="Density of $X \sim \mathcal{U}(0,10)$" width="80%"><p class="caption">
Figure 5.6: Density of <span class="math inline">\(X \sim \mathcal{U}(0,10)\)</span>
</p>
</div>
<p>We can compute the probabilities of different intervals:
<span class="math display">\[\begin{align}
P(0\leq x \leq 1) &amp;= \int_{0}^{1} 0.1x dx &amp;= &amp;0.1 x \left.\right\vert_{x=0}^{x=1}  \\
                  &amp;= 0.1\cdot(1-0)        &amp;= &amp;0.1  \\
P(0\leq x \leq 2) &amp;=  0.1\cdot (2-0)      &amp;= &amp;0.2  \\ 
P(2\leq x \leq 4) &amp;=  P(2\leq x \leq 4)   &amp;= &amp;0.2  \\ 
P(x \geq 2)       &amp;=  P(2 &lt; x \leq 10)    &amp;= &amp;0.1(10-2) = 0.8
\end{align}\]</span></p>
<p>Let’s represent graphically the <span class="math inline">\(P(X\geq 2)\)</span>. Indeed, we see that it is represented by the pink rectangle. If we compute the area, it is apparent that its value is <span class="math inline">\(0.8\)</span>, as we have obtained with the formula.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-144"></span>
<img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-144-1.png" alt="Cumulated Probability between 2 and 10" width="80%"><p class="caption">
Figure 5.7: Cumulated Probability between 2 and 10
</p>
</div>
</div>
</div>
<div id="normal-gaussian-distribution" class="section level3" number="5.4.2">
<h3>
<span class="header-section-number">5.4.2</span> Normal (Gaussian) distribution<a class="anchor" aria-label="anchor" href="#normal-gaussian-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>The Normal distribution was “discovered” in the eighteenth
century when scientists observed an astonishing degree of
regularity in the behavior of measurement errors.</p>
<p>They found that the patterns (distributions) that they observed, and which they
attributed to chance, could be <strong>closely approximated by continuous
curves</strong> which they christened the “normal curve of errors.”</p>
<p>The mathematical properties of these curves were first studied by:</p>
<ul>
<li>Abraham de Moivre (1667-1745),</li>
<li>Pierre Laplace (1749-1827), and then</li>
<li>Karl Gauss (1777-1855), who also lent his name to the distribution.</li>
</ul>
<p>There are couple of important <strong>properties</strong> worth mentioning:</p>
<ul>
<li>A Normal random variable <strong><span class="math inline">\(X\)</span> can take any value <span class="math inline">\(x\in\mathbb{R}\)</span></strong>.</li>
<li>A Normal distribution is <strong>completely defined by its mean <span class="math inline">\(\mu\)</span> and its variance <span class="math inline">\(\sigma ^{2}\)</span></strong>. This means that infinitely many different normal distributions are obtained by varying the <em>parameters</em> <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma ^{2}\)</span>.</li>
</ul>
<p>To illustrate this last property, we plot a series of densities that result from replacing different values of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> in the following figure:</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:normalDensities"></span>
<img src="img/05_continuous_rv/normals4-1.png" alt="Densities for $\color{blue}{X_{1}\sim\mathcal{N}(0,1)}$, $\color{red}{X_{2}\sim\mathcal{N}(0,1.5^2)}$, $\color{forestgreen}{X_{3}\sim\mathcal{N}(1,2.5^2)}$" width="80%"><p class="caption">
Figure 5.8: Densities for <span class="math inline">\(\color{blue}{X_{1}\sim\mathcal{N}(0,1)}\)</span>, <span class="math inline">\(\color{red}{X_{2}\sim\mathcal{N}(0,1.5^2)}\)</span>, <span class="math inline">\(\color{forestgreen}{X_{3}\sim\mathcal{N}(1,2.5^2)}\)</span>
</p>
</div>
<ul>
<li>As we can assess in the previous Figure <a href="#normalDensities"><strong>??</strong></a>, the PDF of the Normal Distribution is “Bell-shaped,” meaning that looks a little bit like a bell. Moreover, it is:
<ul>
<li>
<strong>Symmetric</strong>, i.e. it is not skewed either to the right or to the left. This also implies that the values of the density are the same for <span class="math inline">\(x\)</span> and <span class="math inline">\(-x\)</span>, i.e <span class="math inline">\(\phi_{(\mu,\sigma)}(x) = \phi_{(\mu,\sigma)}(-x)\)</span>
</li>
<li>
<strong>Unimodal</strong>, meaning that it has only one mode and</li>
<li>The <strong>Mean</strong>, <strong>Median</strong> and <strong>Mode</strong> are all <strong>equal</strong>, owing to the <strong>symmetry</strong> of the distribution.</li>
</ul>
</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-145"></span>
<img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-145-1.png" alt="The symmetry of the Gaussian Distribution" width="80%"><p class="caption">
Figure 5.9: The symmetry of the Gaussian Distribution
</p>
</div>
<div id="the-standard-normal-distribution" class="section level4" number="5.4.2.1">
<h4>
<span class="header-section-number">5.4.2.1</span> The Standard Normal distribution<a class="anchor" aria-label="anchor" href="#the-standard-normal-distribution"><i class="fas fa-link"></i></a>
</h4>
<p>First let us establish that <span class="math inline">\(\phi_{(\mu,\sigma)}(x)\)</span> can serve as a
<strong>density function</strong>. To show that, we need to demonstrate that it integrates to 1 in all the domain <span class="math inline">\(\mathbb{R}\)</span></p>
<p>Integrating with respect to <span class="math inline">\(x\)</span> using <em>integration by substitution</em> we obtain
<span class="math display">\[\begin{eqnarray*}
\int_{-\infty}^{\infty}\phi_{(\mu,\sigma)}(x)dx&amp;=&amp;
\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left\{-\frac{(x-\mu)^2}{2\sigma^2}\right\}}dx
\\
 &amp;=&amp;\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}\exp{\left\{-\frac{z^2}{2}\right\}}dz \quad \text{where: } z=\frac{x-\mu}{\sigma} \Leftrightarrow dz = \frac{dx}{\sigma}.
\end{eqnarray*}\]</span>
But the second integral on the right hand side equals to:
<span class="math display">\[
\int_{-\infty}^{\infty}\exp{\left\{-\frac{z^2}{2}\right\}}dz =  2\underbrace{\int_0^{\infty}\exp{\left\{-\frac{z^2}{2}\right\}}dz}_{={\sqrt{2\pi}} \big/ {2}} =
\sqrt{2\pi}
\]</span>
which is <strong>a known result</strong>, known as the <strong>Gaussian Integral</strong> that cancels out with <span class="math inline">\(1/\sqrt{2\pi}\)</span> yielding the integration to 1. Hence, the function <span class="math inline">\(\phi_{(\mu,\sigma)}(x)\)</span> does
indeed define the PDF of a random variable with a mean <span class="math inline">\(\mu\)</span> and a variance of <span class="math inline">\(\sigma^2\)</span></p>
<p>In shorthand notation, we can characterise this transformation in the following way.
<span class="math display">\[X\sim \mathcal{N}\left( \mu ,\sigma ^{2}\right)\Leftrightarrow Z=\frac{\left( X-\mu \right) }{\sigma }\sim \mathcal{N}\left( 0,1\right)\]</span>
This process of <em>standardisation</em> has very important implications when dealing with the Normal Distribution, one of them being the that one can <strong>compute all quantities related with a random variable <span class="math inline">\(X\)</span> by evaluating the related quantities over <span class="math inline">\(Z\)</span></strong>. This is an example of what is called called a <strong>variable transformation</strong></p>
<p>In particular, we can transform from <span class="math inline">\(X\)</span> to <span class="math inline">\(Z\)</span> by “shifting” and “re-scaling”:
<span class="math display">\[\begin{equation*}
Z=\frac{X-\mu }{\sigma } \ (\text{for the random variable}) \quad\mbox{and}\quad z=\frac{x-\mu }{\sigma }\,  \ (\text{for its values}) ,
\end{equation*}\]</span>
and return back to <span class="math inline">\(X\)</span> by a “re-scaling” and “shifting”:
<span class="math display">\[\begin{equation*}
X=\sigma Z+\mu  \ (\text{for the random variable}) \quad\mbox{and}\quad x=\sigma z+\mu\, \ (\text{for its values}) .
\end{equation*}\]</span></p>
<p>Thus, <strong>statements about any Normal random variable</strong> can always be translated into <strong>equivalent statements about a standard Normal random variable</strong>, and viceversa.</p>
</div>
<div id="the-normal-cdf" class="section level4" number="5.4.2.2">
<h4>
<span class="header-section-number">5.4.2.2</span> The Normal CDF<a class="anchor" aria-label="anchor" href="#the-normal-cdf"><i class="fas fa-link"></i></a>
</h4>
<p>For instance, we can concentrate on the consequences of this property on the computation of the CDF.</p>
<p>Analytically the CDF of a Normal Random Variable <span class="math inline">\(X\sim \mathcal{N}\left( \mu ,\sigma ^{2}\right)\)</span> is given by:<br><span class="math display">\[\Phi_{(\mu,\sigma)}\left( x\right) =\int_{-\infty }^{x}\frac{1}{\sqrt{2\pi \sigma ^{2}}}\exp{ \left\{ -\frac{1}{2\sigma ^{2}}\left( t-\mu \right) ^{2}\right\}} dt\]</span>
To calculate <span class="math inline">\(\Phi_{(\mu,\sigma)}\left( x\right)=P(\{X\leq x\})\)</span> we use integration with the substitution
<span class="math inline">\(s=(t-\mu)/\sigma\)</span>, which implies: <span class="math inline">\(z=(x-\mu)/\sigma\)</span> (for the domain of integration) and <span class="math inline">\(ds=dt/\sigma\)</span> (for the “differential”), yielding:
<span class="math display">\[\begin{eqnarray*}
P(\{ X\leq x\} )&amp;=&amp;\int_{-\infty}^x\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left\{-\frac{(t-\mu)^2}{2\sigma^2}\right\}}dt\\
 &amp;=&amp;\int_{-\infty}^z\phi(s)ds\\
P(\{ X\leq x\} )  &amp;=&amp;P(\{Z\leq z\})
\end{eqnarray*}\]</span></p>
<p>Hence, the <strong>probability of the interval <span class="math inline">\(X\leq x\)</span></strong> has been <strong>mapped into a corresponding probability</strong> for a <strong>standard Normal random variable</strong>. In other words:</p>
<p><span class="math display">\[P(\{ X\leq x\}) = P\left(\left\{\underbrace{\frac{X-\mu}{\sigma}}_Z\leq \underbrace{\frac{x-\mu}{\sigma}}_{z}\right\}\right) = P(\{ Z\leq z\})\]</span></p>
<p>The integral that defines the CDF of a Standard Normal random variable:
<span class="math display">\[P(\{Z\leq z\})=\Phi(z)=\int_{-\infty}^z\phi(s)ds\]</span>
<strong>does not have a closed-form expression</strong>. Yet, it is possible to provide very precise
<strong>numerical approximations</strong> using computers. Some of these values can be found in so-called <strong>Standard Normal Tables</strong>, which contain values of the integral <span class="math inline">\(\Phi(z)\)</span> for various values of <span class="math inline">\(z\geq 0\)</span>.</p>
<p>To compute the cumulative probability for <span class="math inline">\(z\leq 0\)</span> we take advantage of the symmetry property of <span class="math inline">\(\phi(z)\)</span> i.e. <span class="math inline">\(\phi(z)=\phi(-z)\)</span>, which also implies that:
<span class="math display">\[\Phi(-z)=1-\Phi(z)\,\]</span></p>
We can see this in the following plot.
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-147"></span>
<img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-147-1.png" alt="The Symmetry of the Standard Normal PDF and its consequences for the CDF" width="80%"><p class="caption">
Figure 5.10: The Symmetry of the Standard Normal PDF and its consequences for the CDF
</p>
</div>
<p>To compute the probability in a closed interval, if <span class="math inline">\(X\sim \mathcal{N}\left( \mu ,\sigma ^{2}\right)\)</span> we can proceed by substraction:
<span class="math display">\[\begin{eqnarray*}
P(\{x_1&lt;X\leq x_2\})&amp;=&amp;P(\{z_1&lt;Z\leq z_2\})\\
&amp;=&amp;P(\{Z\leq z_2\}) - P(\{Z\leq z_1\})\\ 
&amp;=&amp;\Phi(z_2)-\Phi(z_1)
\end{eqnarray*}\]</span>
where, again, the values <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> result from <em>standardisation</em>
<span class="math inline">\(z_1=(x_1-\mu)/\sigma\)</span> and <span class="math inline">\(z_2=(x_2-\mu)/\sigma\)</span>. As we can see from the plot below, there’s a clear
geometric interpretation to this relation.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-148"></span>
<img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-148-1.png" alt="Computing the Probability of an interval" width="80%"><p class="caption">
Figure 5.11: Computing the Probability of an interval
</p>
</div>
<!-- Graphical Explanation that I find a bit underwhelming -->
<!-- We can proceed graphically. Let's start with a variable $X \sim \mathcal{N}(5,3)$. The procedure of shifting and rescaling consists in defining a new variable $Y=X-5$, which is a recentered/shifted $X$ (it's centered at 0 and has the same variance as $X$) and finally defining $Z= Y/\sqrt{3}$, which is a recentered/shifted and rescaled $X$ (it's centered at 0 and has unit variance).  -->
</div>
<div id="standard-normal-tables" class="section level4" number="5.4.2.3">
<h4>
<span class="header-section-number">5.4.2.3</span> Standard Normal Tables<a class="anchor" aria-label="anchor" href="#standard-normal-tables"><i class="fas fa-link"></i></a>
</h4>
<p>Standard Normal Tables give values of the standard normal integral <span class="math inline">\(\Phi(z)\)</span> for <span class="math inline">\(z\geq 0\)</span>. This is done to , to save space, since values for negative <span class="math inline">\(z\)</span> are obtained via symmetry, as we mentioned in the previous section.
<!-- %-  Other Normal tables give either $1-\Phi(z)$ (tail area) or $\Phi(z)-0.5$! --></p>
Most of the tables are explicit in this choice and graphically show that they
contain the probabilities of the interval <span class="math inline">\(0\leq Z \leq z\)</span>, e.g. 
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-151"></span>
<img src="img/05_continuous_rv/bell_curve__5-1.png" alt="Description of the contents of a Standard Normal Table" width="80%"><p class="caption">
Figure 5.12: Description of the contents of a Standard Normal Table
</p>
</div>
<p>Typically, the dimension of the rows is nearest tenth to <span class="math inline">\(z\)</span>, while
the columns indicate the nearest hundreth.</p>
Say we would like to find the value of <span class="math inline">\(\Phi(0.46)\)</span>. We’d have to find the
row for <span class="math inline">\(0.4\)</span> and the column for <code>0.06</code>.<br><div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-152"></span>
<img src="img/05_continuous_rv/myTableGauss-1.png" alt="Probabilities for some values in the Standard Normal Table" width="80%"><p class="caption">
Figure 5.13: Probabilities for some values in the Standard Normal Table
</p>
</div>
<p>You can use these tables to compute integrals/probabilities of intervals <span class="math inline">\(Z&lt;=z\)</span> or
<span class="math inline">\(Z \in [z_1, z_2]\)</span> using the rules mentioned above.
<img src="img/05_continuous_rv/CDF_pr-1.png" width="100%" style="display: block; margin: auto;"></p>
</div>
<div id="further-properties-of-the-normal-distribution" class="section level4" number="5.4.2.4">
<h4>
<span class="header-section-number">5.4.2.4</span> Further properties of the Normal distribution<a class="anchor" aria-label="anchor" href="#further-properties-of-the-normal-distribution"><i class="fas fa-link"></i></a>
</h4>
<div id="the-rule-68-95-99.7" class="section level5 unnumbered">
<h5>The Rule ‘68 – 95 – 99.7’<a class="anchor" aria-label="anchor" href="#the-rule-68-95-99.7"><i class="fas fa-link"></i></a>
</h5>
<p>In the following plot, the shaded areas under the PDFs are (approximately)
equivalent to <span class="math inline">\(0.683\)</span>, <span class="math inline">\(0.954\)</span> and <span class="math inline">\(0.997\)</span>, respectively.
<img src="img/05_continuous_rv/Areas_Normal-1.png" width="100%" style="display: block; margin: auto;"></p>
<p>This leads to the rule: <strong>.’68 – 95 – 99.7’</strong> i.e. that if <span class="math inline">\(X\)</span> is a
Normal random variable, <span class="math inline">\(X \sim \mathcal{N}(\mu, \sigma^2)\)</span>, its realization
has approximately a probability of:</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="center">Probability</th>
<th align="center">Interval</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(68 \, \%\)</span></td>
<td align="center"><span class="math inline">\(\lbrack \mu - \sigma, \, \mu + \sigma \rbrack\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(95 \, \%\)</span></td>
<td align="center"><span class="math inline">\(\lbrack \mu - 2 \, \sigma, \, \mu + 2 \, \sigma \rbrack\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(99.7 \, \%\)</span></td>
<td align="center"><span class="math inline">\(\lbrack \mu - 3 \, \sigma, \, \mu + 3 \, \sigma\rbrack\)</span></td>
</tr>
</tbody>
</table></div>
<ul>
<li><p>For <span class="math inline">\(X\sim \mathcal{N}\left( \mu ,\sigma ^{2}\right)\)</span>
<span class="math display">\[\begin{equation*}
E\left[ X\right] =\mu \text{ and }Var\left( X\right) =\sigma ^{2}.
\end{equation*}\]</span></p></li>
<li><p>If <span class="math inline">\(a\)</span> is a number, then
<span class="math display">\[\begin{eqnarray*}
X+a &amp;\sim &amp;\mathcal{N}\left( \mu +a,\sigma ^{2}\right) \\
aX &amp;\sim &amp;\mathcal{N}\left( a\mu ,a^{2}\sigma ^{2}\right).
\end{eqnarray*}\]</span></p></li>
<li><p>If <span class="math inline">\(X\sim \mathcal{N}\left( \mu ,\sigma ^{2}\right)\)</span> and <span class="math inline">\(Y\sim \mathcal{N}\left( \alpha,\delta ^{2}\right)\)</span>, and <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent</strong> then
<span class="math display">\[\begin{equation*}
X+Y\sim \mathcal{N}\left( \mu +\alpha ,\sigma ^{2}+\delta ^{2}\right).
\end{equation*}\]</span></p></li>
</ul>
<p>To illustrate the last point, let’s use simulated
values for two independent normally-distributed random variables
<span class="math inline">\(X\sim \mathcal{N}(5,1)\)</span> and <span class="math inline">\(Y\sim \mathcal{N}(7,1)\)</span>, alongside their sum.</p>
We can see that if we plot the densities of these RV’s independently, we have the
expected normal density (two bell-shaped curves centered around 5 and 7 that are
similar in shape since they sport the same variance). Moreover the density of the
sum of the simulated values is also bell-shaped and appears to be centered around
12 and sports a wider variance, confirming visually what we expected, i.e. that
<span class="math inline">\(X + Y \sim \mathcal{N}(5+7,1+1)\)</span>, <span class="math inline">\(X+Y \sim \mathcal{N}(12,2)\)</span>.
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-156"></span>
<img src="img/05_continuous_rv/sum_of_two_independent_normals_with_rug__1-1.png" alt="Locations of $n=30$ sampled values of $X,$ $Y$, and $X+Y$ shown as tick marks under each respective density" width="80%"><p class="caption">
Figure 5.14: Locations of <span class="math inline">\(n=30\)</span> sampled values of <span class="math inline">\(X,\)</span> <span class="math inline">\(Y\)</span>, and <span class="math inline">\(X+Y\)</span> shown as tick marks under each respective density
</p>
</div>
</div>
</div>
<div id="an-example" class="section level4" number="5.4.2.5">
<h4>
<span class="header-section-number">5.4.2.5</span> An example<a class="anchor" aria-label="anchor" href="#an-example"><i class="fas fa-link"></i></a>
</h4>
<!-- %\frametitle{... it's all about normality...} -->
<!-- % -->
<!-- % -->
<!-- %\begin{figure}[ptb]\centering -->
<!-- %\includegraphics[natheight=7.389in, natwidth=13.4859in, height=2.4031in, width=4.7426in]{RU_Normal.PDF}% -->
<!-- %\end{figure} -->
<!-- % -->
<!-- % -->
<!-- %% -->
<!-- % -->
</div>
</div>
<div id="the-chi-squared-distribution" class="section level3" number="5.4.3">
<h3>
<span class="header-section-number">5.4.3</span> The Chi-squared distribution<a class="anchor" aria-label="anchor" href="#the-chi-squared-distribution"><i class="fas fa-link"></i></a>
</h3>
<p>A Chi-Squared-distributed Random Variable has the following properties:</p>
<ul>
<li>
<span class="math inline">\(X\sim \chi ^{2}(n)\)</span> can take only <strong>positive</strong> values.</li>
<li>The expectation and variance for <span class="math inline">\(X\sim \chi ^{2}(n)\)</span>, are given by:
<span class="math display">\[\begin{eqnarray*}
E\left[ X\right] &amp;=&amp;n \\
Var\left( X\right) &amp;=&amp;2n
\end{eqnarray*}\]</span>
</li>
<li>Finally, if <span class="math inline">\(X\sim \chi ^{2}(n)\)</span> and <span class="math inline">\(Y\sim \chi ^{2}(m)\)</span> are <strong>independent</strong> then <span class="math inline">\(X+Y\sim \chi ^{2}(n+m)\)</span>.</li>
</ul>
<p>The Chi-Squared distribution has a parameter that defines its shape, namely the
<span class="math inline">\(n\)</span> degrees of freedom. To illustrate their effect, let’s plot some densities
in the following plot.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-159"></span>
<img src="img/05_continuous_rv/chisquared_pdfs__2-1.png" alt="Chi-Squared densities for various values of $n$" width="80%"><p class="caption">
Figure 5.15: Chi-Squared densities for various values of <span class="math inline">\(n\)</span>
</p>
</div>
<div id="chi-squared-tables" class="section level4" number="5.4.3.1">
<h4>
<span class="header-section-number">5.4.3.1</span> Chi-squared tables<a class="anchor" aria-label="anchor" href="#chi-squared-tables"><i class="fas fa-link"></i></a>
</h4>
<p>As you can imagine, cumulated probabilities for Chi-squared distributions are not straightforward to compute. To do it, we require computational methods.</p>
<p>Given that the shape depends on the parameter <span class="math inline">\(n\)</span> there would be an infinite number of tables (as many as <span class="math inline">\(n\)</span>) should we want to carry along the values of the CDF. Instead, tables contain the values for the Upper-tail or Lower-tail <em>Quantiles</em> for some given probabilities. For instance, in the following plot, we have a typical table, containing the values of the quantiles for different upper-tail probabilities displayed according to the degrees of freedom (denoted as <span class="math inline">\(V\)</span>).</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-160"></span>
<img src="img/05_continuous_rv/chisq_table__3-1.png" alt="A typical Chi-square Table" width="80%"><p class="caption">
Figure 5.16: A typical Chi-square Table
</p>
</div>
<!-- To be included in the future: definition of upper-tail and lower-tail quantiles -->
</div>
</div>
<div id="the-student-t-distribution" class="section level3" number="5.4.4">
<h3>
<span class="header-section-number">5.4.4</span> The Student-t distribution<a class="anchor" aria-label="anchor" href="#the-student-t-distribution"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span class="math inline">\(T\sim t_{v}\,\)</span> can take any value in <span class="math inline">\(\mathbb{R}\)</span>.</li>
<li>Expected value and variance for <span class="math inline">\(T\sim t_{v}\)</span> are
<span class="math display">\[\begin{eqnarray*}
E\left[ T\right] &amp;=&amp;0\text{, for }v&gt;1 \\
Var\left( T\right) &amp;=&amp;\frac{v}{v-2}\text{, for }v&gt;2.
\end{eqnarray*}\]</span>
</li>
</ul>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-164"></span>
<img src="img/05_continuous_rv/student_t__4-1.png" alt="Densities for various Student-t densities alongside the Normal Density" width="80%"><p class="caption">
Figure 5.17: Densities for various Student-t densities alongside the Normal Density
</p>
</div>
<p>Again, since the distribution depends on the degrees of freedom <span class="math inline">\(v\)</span>, in the absence of
computers, we rely on tables for some values of upper-tail probabilities.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-165"></span>
<img src="img/05_continuous_rv/Student_t_table__5-1.png" alt="A typical Student's-t Table" width="80%"><p class="caption">
Figure 5.18: A typical Student’s-t Table
</p>
</div>
</div>
<div id="the-f-distribution" class="section level3" number="5.4.5">
<h3>
<span class="header-section-number">5.4.5</span> The F distribution<a class="anchor" aria-label="anchor" href="#the-f-distribution"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>
<span class="math inline">\(F\sim F_{v_{1},v_{2}}\,\)</span> can take only <strong>positive</strong> values.</li>
<li>Expected value and variance for <span class="math inline">\(F\sim F_{v_{1},v_{2}}\)</span> (note that <strong>the order of the degrees of freedom is important!</strong>) is given by:
<span class="math display">\[\begin{eqnarray*}
E\left[ F\right] &amp;=&amp;\frac{v_{2}}{v_{2}-2}\text{, for }v_{2}&gt;2 \\
Var\left( F\right) &amp;=&amp;\frac{2v_{2}^{2}\left( v_{1}+v_{2}-2\right) }{%
v_{1}\left( v_{2}-2\right) ^{2}\left( v_{2}-4\right) }\text{, for }v_{2}&gt;4.
\end{eqnarray*}\]</span>
</li>
</ul>
<p>As we can assess from the following plot, there is no uniform way in which the densities change
according to the degrees of freedom. However all of them seem to be concentrated towards the lower side of the <span class="math inline">\(\mathbb{R}^{+}\)</span> line.
<img src="img/05_continuous_rv/F-dist_pds__6-1.png" width="80%" style="display: block; margin: auto;"></p>
Again, when distributed in tables, we can only obtain information on the quantiles
for some upper-tail probabilites. However, since the distributions are defined by
two sets of degrees of freedom, a table can only pertain one value of the upper tail
probability.
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-168"></span>
<img src="img/05_continuous_rv/Fdist_table__7-1.png" alt="Critical Values for $F$ distribution and $\alpha = 0.05$" width="80%"><p class="caption">
Figure 5.19: Critical Values for <span class="math inline">\(F\)</span> distribution and <span class="math inline">\(\alpha = 0.05\)</span>
</p>
</div>
</div>
<div id="the-lognormal-distribution" class="section level3" number="5.4.6">
<h3>
<span class="header-section-number">5.4.6</span> The lognormal distribution<a class="anchor" aria-label="anchor" href="#the-lognormal-distribution"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>If <span class="math inline">\(Y\sim\)</span> <span class="math inline">\(\left( \mu ,\sigma ^{2}\right)\)</span> then
<span class="math display">\[\begin{eqnarray*}
E\left[ Y\right] &amp;=&amp;\exp^{ \left( \mu +\frac{1}{2}\sigma ^{2}\right)} \\
Var(Y) &amp;=&amp;\exp^{ \left( 2\mu +\sigma ^{2}\right)} \left( \exp^{ \left( \sigma
^{2}\right)} -1\right).
\end{eqnarray*}\]</span>
</li>
</ul>
<p>In the next plot, we display an estimation of the density of the realization of a Normal Random Variable, as well as the density of <span class="math inline">\(y = exp(x)\)</span>. Notice how the distribution is assymetric, positive and right-skewed.
<img src="img/05_continuous_rv/lognormal_with_rug__8-1.png" width="80%" style="display: block; margin: auto;"></p>
</div>
<div id="exponential-distribution" class="section level3" number="5.4.7">
<h3>
<span class="header-section-number">5.4.7</span> Exponential distribution<a class="anchor" aria-label="anchor" href="#exponential-distribution"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>The Expectation and Variance of <span class="math inline">\(X\sim \text{Exp}(\lambda)\)</span> are as follows:</li>
</ul>
<p><span class="math display">\[\begin{eqnarray*}
E[X]=\int_{0}^{\infty }xf_X(x )dx= 1/\lambda &amp; \text{and} &amp;   Var(X)=\int_{0}^{\infty }x^{2}f_X(x )dx-E^{2}(X)=1/\lambda ^{2}. \nn
\end{eqnarray*}\]</span></p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-174"></span>
<img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-174-1.png" alt="Graphical Illustration of Exponential distributions with varying $\lambda$" width="80%"><p class="caption">
Figure 5.20: Graphical Illustration of Exponential distributions with varying <span class="math inline">\(\lambda\)</span>
</p>
</div>
</div>
</div>
<div id="transformation-of-variables" class="section level2" number="5.5">
<h2>
<span class="header-section-number">5.5</span> Transformation of variables<a class="anchor" aria-label="anchor" href="#transformation-of-variables"><i class="fas fa-link"></i></a>
</h2>
<ul>
<li>Consider a random variable <span class="math inline">\(X\)</span>
</li>
<li>Suppose we are interested in <span class="math inline">\(Y=\psi(X)\)</span>, where <span class="math inline">\(\psi\)</span> is a <strong>one to one function</strong>
</li>
<li>A <strong>function</strong> <span class="math inline">\(\psi \left( x\right)\)</span> <strong>is one to one</strong> (1-to-1) if there are no two numbers, <span class="math inline">\(x_{1},x_{2}\)</span> in the domain of <span class="math inline">\(\psi\)</span> such that <span class="math inline">\(\psi \left( x_{1}\right) =\psi \left( x_{2}\right)\)</span> but <span class="math inline">\(x_{1}\neq x_{2}\)</span>.</li>
<li>A sufficient condition for <span class="math inline">\(\psi \left( x\right)\)</span> to be 1-to-1 is that it be monotonically increasing (or decreasing) in <span class="math inline">\(x\)</span>.</li>
<li>Note that the <strong>inverse</strong> of a 1-to-1 function <span class="math inline">\(y=\psi \left(x\right)\)</span> is a 1-to-1 function <span class="math inline">\(\psi^{-1}\left( y\right)\)</span> such that:</li>
</ul>
<p><span class="math display">\[\begin{equation*}
\psi ^{-1}\left( \psi \left( x\right) \right) =x\text{ and }\psi \left( \psi
^{-1}\left( y\right) \right) =y.
\end{equation*}\]</span></p>
<ul>
<li>To transform <span class="math inline">\(X\)</span> to <span class="math inline">\(Y\)</span>, we need to consider all the values <span class="math inline">\(x\)</span> that <span class="math inline">\(% X\)</span> can take</li>
<li>We first transform <span class="math inline">\(x\)</span> into values <span class="math inline">\(y=\psi (x)\)</span>
</li>
</ul>
<div id="transformation-of-discrete-random-variables" class="section level3" number="5.5.1">
<h3>
<span class="header-section-number">5.5.1</span> Transformation of discrete random variables<a class="anchor" aria-label="anchor" href="#transformation-of-discrete-random-variables"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>To transform a discrete random variable <span class="math inline">\(X\)</span>, into the random variable <span class="math inline">\(Y=\psi (X)\)</span>, we transfer the probabilities for <strong>each</strong> <span class="math inline">\(x\)</span> to the values $y=( x) $:</li>
</ul>
<!-- \multicolumn{2}{l}{_Probability function for_$X$} |  |  --><!-- \multicolumn{2}{l}{_Probability function for_$X$} \\  --><div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th align="center"><span class="math inline">\(X\)</span></th>
<th align="center"><span class="math inline">\(P \left(\{ X=x_{i} \}\right) =p_{i}\)</span></th>
<th align="center"></th>
<th align="center"><span class="math inline">\(Y\)</span></th>
<th align="center"><span class="math inline">\(P \left(\{X=x_{i} \}\right) =p_{i}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{1}\)</span></td>
<td align="center"><span class="math inline">\(p_{1}\)</span></td>
<td align="center"><span class="math inline">\(\qquad \Rightarrow \qquad\)</span></td>
<td align="center"><span class="math inline">\(\psi (x_{1})\)</span></td>
<td align="center"><span class="math inline">\(p_{1}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(x_{2}\)</span></td>
<td align="center"><span class="math inline">\(p_{2}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\psi (x_{2})\)</span></td>
<td align="center"><span class="math inline">\(p_{2}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{3}\)</span></td>
<td align="center"><span class="math inline">\(p_{3}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\psi (x_{3})\)</span></td>
<td align="center"><span class="math inline">\(p_{3}\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
<td align="center"><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(x_{n}\)</span></td>
<td align="center"><span class="math inline">\(p_{n}\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\psi (x_{n})\)</span></td>
<td align="center"><span class="math inline">\(p_{n}\)</span></td>
</tr>
</tbody>
</table></div>
<p>Notice that this is equivalent to applying the function <span class="math inline">\(\psi \left(\cdot \right)\)</span> inside the probability statements:</p>
<p><span class="math display">\[\begin{eqnarray*}
P \left( \{ X=x_{i}  \}\right) &amp;=&amp;P \left(  \{\psi \left( X\right) =\psi \left(
x_{i}\right)  \} \right) \\
&amp;=&amp;P \left( \{ Y=y_{i} \} \right) \\
&amp;=&amp;p_{i}
\end{eqnarray*}\]</span></p>
<p><img src="img/05_continuous_rv/Shreve_Bin-1.png" width="80%" style="display: block; margin: auto;">
Indeed, after two tosses, there are four possible coin sequences,
<span class="math display">\[
\{HH,HT,TH,TT\}
\]</span>
although not all of them result in different stock prices at time <span class="math inline">\(t_2\)</span>.
<!-- % -->
<!-- %% Define styles for bags and leafs -->
<!-- %\tikzstyle{bag} = [text width=2em, text centered] -->
<!-- %\tikzstyle{end} = [] -->
<!-- %\begin{tikzpicture}[sloped] -->
<!-- %   \node (a) at ( 0,0) [bag] {$\$ A$}; -->
<!-- %   \node (b) at ( 4,-1.5) [bag] {B}; -->
<!-- %   \node (c) at ( 4,1.5) [bag] {C}; -->
<!-- %   \node (d) at ( 8,-3) [bag] {D}; -->
<!-- %   \node (e) at ( 8,0) [bag] {E}; -->
<!-- %   \node (f) at ( 8,3) [bag] {F}; -->
<!-- %   \draw [->] (a) to node [below] {$(1-p)$} (b); -->
<!-- %   \draw [->] (a) to node [above] {$P$} (c); -->
<!-- %   \draw [->] (c) to node [below] {$P^2$} (f); -->
<!-- %   \draw [->] (c) to node [above] {$(1-p)p$} (e); -->
<!-- %   \draw [->] (b) to node [below] {$(1-p)p$} (e); -->
<!-- %   \draw [->] (b) to node [above] {$(1-p)^2$} (d); -->
<!-- %\end{tikzpicture} --></p>
<p>Let us set <span class="math inline">\(S_0=1\)</span>, <span class="math inline">\(u=2\)</span> and <span class="math inline">\(d=1/2\)</span>: we represent the price evolution by a tree:</p>
<div class="inline-figure"><img src="Prob1-GSEM-UNIGE_files/figure-html/unnamed-chunk-177-1.png" width="80%" style="display: block; margin: auto;"></div>
<p>Now consider an European option call with maturity <span class="math inline">\(t_2\)</span> and strike price <span class="math inline">\(K=0.5\)</span>, whose random pay-off at <span class="math inline">\(t_2\)</span> is <span class="math inline">\(C=\max(0;S_2-0.5)\)</span>. Thus,
<span class="math display">\[\begin{eqnarray*}
C(HH)=\max(0;4-0.5)=\$ 3.5 &amp; C(HT)=\max(0;1-0.5)=\$ 0.5 \\
C(TH)=\max(0;1-0.5)=\$ 0.5 &amp; C(TT)=\max(0;0.25-0.5)=\$ 0.
\end{eqnarray*}\]</span>
Thus at maturity <span class="math inline">\(t_2\)</span> we have</p>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="11%">
<col width="54%">
<col width="11%">
<col width="11%">
<col width="11%">
</colgroup>
<thead><tr class="header">
<th align="center"><span class="math inline">\(S_2\)</span></th>
<th align="center"><span class="math inline">\(P \left(\{ X=x_{i} \}\right) =p_{i}\)</span></th>
<th align="center"></th>
<th align="center"><span class="math inline">\(C\)</span></th>
<th align="center"><span class="math inline">\(P \left(\{C=c_{i} \}\right) =p_{i}\)</span></th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\$ u^2\)</span></td>
<td align="center"><span class="math inline">\(p^2\)</span></td>
<td align="center"><span class="math inline">\(\qquad \Rightarrow \qquad\)</span></td>
<td align="center"><span class="math inline">\(\$ 3.5\)</span></td>
<td align="center"><span class="math inline">\(p^2\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(\$ ud\)</span></td>
<td align="center"><span class="math inline">\(2p(1-p)\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\$ 0.5\)</span></td>
<td align="center"><span class="math inline">\(2p(1-p)\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(\$ d^2\)</span></td>
<td align="center"><span class="math inline">\((1-p)^2\)</span></td>
<td align="center"></td>
<td align="center"><span class="math inline">\(\$ 0\)</span></td>
<td align="center"><span class="math inline">\((1-p)^2\)</span></td>
</tr>
</tbody>
</table></div>
<p>Since <span class="math inline">\(ud=du\)</span> the corresponding values of <span class="math inline">\(S_2\)</span> and <span class="math inline">\(C\)</span> can be aggregated, without loss of info.</p>
</div>
<div id="transformation-of-variables-using-the-cdf" class="section level3" number="5.5.2">
<h3>
<span class="header-section-number">5.5.2</span> Transformation of variables using the CDF<a class="anchor" aria-label="anchor" href="#transformation-of-variables-using-the-cdf"><i class="fas fa-link"></i></a>
</h3>
<p>We can use the same logic for CDF probabilities, whether the random variables are <strong>discrete or continuous</strong>.</p>
<p>For instance, Let <span class="math inline">\(Y=\psi \left( X\right)\)</span> with <span class="math inline">\(\psi \left( x\right)\)</span> 1-to-1 and <strong>monotone increasing</strong>. Then:</p>
<p><span class="math display">\[\begin{eqnarray*}
F_{Y}\left( y\right) &amp;=&amp;P \left( \{ Y\leq y \}\right) \\
&amp;=&amp;P \left( \{ \psi \left( X\right) \leq y \} \right) =P \left( \{ X\leq \psi
^{-1}\left( y\right) \} \right) \\
&amp;=&amp;F_{X}\left( \psi ^{-1}\left( y\right) \right)
\end{eqnarray*}\]</span></p>
<p>You can notice that Monotone decreasing functions work in a similar way, but require
changing of the sense of the inequality, e.g. let <span class="math inline">\(Y=\psi \left( X\right)\)</span> with <span class="math inline">\(\psi \left( x\right)\)</span> 1-to-1 and
<strong>monotone decreasing</strong>. Then:</p>
<p><span class="math display">\[\begin{eqnarray*}
F_{Y}\left( y\right) &amp;=&amp;P \left( \{ Y\leq y \} \right) \\
&amp;=&amp;P \left( \{ \psi \left( X\right) \leq y \} \right) =P \left( \{ X\geq \psi
^{-1}\left( y\right) \} \right) \\
&amp;=&amp;1-F_{X}\left( \psi ^{-1}\left( y\right) \right)
\end{eqnarray*}\]</span></p>
</div>
<div id="transformation-of-continuous-rv-through-pdf" class="section level3" number="5.5.3">
<h3>
<span class="header-section-number">5.5.3</span> Transformation of continuous RV through PDF<a class="anchor" aria-label="anchor" href="#transformation-of-continuous-rv-through-pdf"><i class="fas fa-link"></i></a>
</h3>
<p>For continuous random variables, if <span class="math inline">\(\psi \left( x\right)\)</span> 1-to-1 and
monotone <strong>increasing</strong>, we have:
<span class="math display">\[\begin{equation*}
F_{Y}\left( y\right) =F_{X}\left( \psi ^{-1}\left( y\right) \right)
\end{equation*}\]</span>
Notice this implies that the PDF of <span class="math inline">\(Y=\psi \left( X\right)\)</span> must
satisfy%
<span class="math display">\[\begin{eqnarray*}
f_{Y}\left( y\right) &amp;=&amp;\frac{dF_{Y}\left( y\right) }{dy}=\frac{dF_{X}\left(
\psi ^{-1}\left( y\right) \right) }{dy} \\
&amp;=&amp;\frac{dF_{X}\left( x\right) }{dx}\times \frac{d\psi ^{-1}\left( y\right) 
}{dy}\qquad \text{{\small (chain rule)}} \\
&amp;=&amp;f_{X}\left( x\right) \times \frac{d\psi ^{-1}\left( y\right) }{dy}\qquad 
\text{{\small (derivative of CDF (of }}X\text{){\small \ is PDF)}} \\
&amp;=&amp;f_{X}\left( \psi ^{-1}\left( y\right) \right) \times \frac{d\psi
^{-1}\left( y\right) }{dy}\qquad \text{{\small (substitute }}x=\psi
^{-1}\left( y\right) \text{{\small )}}
\end{eqnarray*}\]</span></p>
<p>What happens when <span class="math inline">\(\psi \left( x\right)\)</span> 1-to-1 and monotone <strong>decreasing</strong>? We have:
<span class="math display">\[\begin{equation*}
F_{Y}\left( y\right) =1-F_{X}\left( \psi ^{-1}\left( y\right) \right)
\end{equation*}\]</span>
So now the PDF of <span class="math inline">\(Y=\phi \left( X\right)\)</span> must satisfy:
<span class="math display">\[\begin{eqnarray*}
f_{Y}\left( y\right) &amp;=&amp;\frac{dF_{Y}\left( y\right) }{dy}=-\frac{%
dF_{X}\left( \psi ^{-1}\left( y\right) \right) }{dy} \\
&amp;=&amp;-f_{X}\left( \psi ^{-1}\left( y\right) \right) \times \frac{d\psi
^{-1}\left( y\right) }{dy}\qquad \text{{\small (same reasons as before)}}
\end{eqnarray*}\]</span></p>
<p>but <span class="math inline">\(\frac{d\psi ^{-1}\left( y\right) }{dy}&lt;0\)</span> since here <span class="math inline">\(\psi \left(\cdot \right)\)</span> is monotone decreasing, hence we can write:
<span class="math display">\[\begin{equation*}
f_{Y}\left( y\right) =f_{X}\left( \psi ^{-1}\left( y\right) \right) \times
\left\vert \frac{d\psi ^{-1}\left( y\right) }{dy}\right\vert
\end{equation*}\]</span></p>
<p>This expression (called <strong>Jacobian-formula</strong>) is valid for <span class="math inline">\(\psi \left( x\right)\)</span> 1-to-1 and
monotone (whether increasing or decreasing)</p>
<p>More generally, for <span class="math inline">\(\alpha\in[0,1]\)</span>, the <span class="math inline">\(\alpha\)</span>-th quantile of a r.v. <span class="math inline">\(X\)</span> is the value <span class="math inline">\(x_\alpha\)</span> such that <span class="math inline">\(P(\{X \leq x_\alpha\})\geq\alpha\)</span>. If <span class="math inline">\(X\)</span> si a continuous r.v. we can set <span class="math inline">\(P(\{X \leq x_\alpha\})=\alpha\)</span> (as we did, e.g., for the lognormal).</p>
</div>
<div id="a-caveat" class="section level3" number="5.5.4">
<h3>
<span class="header-section-number">5.5.4</span> A caveat<a class="anchor" aria-label="anchor" href="#a-caveat"><i class="fas fa-link"></i></a>
</h3>
<p>When <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are two random variables, we should pay attention to their transformations. For instance, let us consider
<span class="math display">\[
X\sim \mathcal{N}(\mu,\sigma^2) \quad \text{and}  \quad Y\sim Exp(\lambda).
\]</span>
Then, let’s transform <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span></p>
<ul>
<li>in a linear way: <span class="math inline">\(Z=X+Y\)</span>. We know that
<span class="math display">\[
E[Z] = E[X+Y] = E[X] + E[Y] 
\]</span>
%so we can rely on the linearity of the expected value.</li>
<li>in a nonlinear way <span class="math inline">\(W = X/Y\)</span>. One can show that</li>
</ul>
<p><span class="math display">\[\color{red} E[W] = E\left[\frac{X}{Y}\right] \neq \frac{E[X]}{E[Y]}.\]</span></p>
<!-- %so, we cannot rely on the linearity of the expected value. -->
</div>
</div>
<div id="the-big-picture" class="section level2" number="5.6">
<h2>
<span class="header-section-number">5.6</span> The big picture<a class="anchor" aria-label="anchor" href="#the-big-picture"><i class="fas fa-link"></i></a>
</h2>
<p>Despite exotic names, the common distributions relate to each other in intuitive and interesting ways. Several follow naturally from the Bernoulli distribution, for example.</p>
<div class="inline-figure"><img src="img/05_continuous_rv/RelRVs_Diego-1.png" width="80%" style="display: block; margin: auto;"></div>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="discreterv.html"><span class="header-section-number">4</span> 🔧 Discrete Random Variables</a></div>
<div class="next"><a href="limittheorems.html"><span class="header-section-number">6</span> 📝 Limit Theorems</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#continuousrv"><span class="header-section-number">5</span> 🔧 Continuous Random Variable</a></li>
<li><a class="nav-link" href="#two-motivating-examples"><span class="header-section-number">5.1</span> Two Motivating Examples</a></li>
<li><a class="nav-link" href="#cumulative-distribution-function-cdf"><span class="header-section-number">5.2</span> Cumulative Distribution Function (CDF)</a></li>
<li>
<a class="nav-link" href="#distributional-summaries"><span class="header-section-number">5.3</span> Distributional Summaries</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#the-expectation"><span class="header-section-number">5.3.1</span> The Expectation</a></li>
<li><a class="nav-link" href="#the-variance"><span class="header-section-number">5.3.2</span> The Variance</a></li>
<li><a class="nav-link" href="#important-properties-of-expectations"><span class="header-section-number">5.3.3</span> Important properties of expectations</a></li>
<li><a class="nav-link" href="#mode-and-median"><span class="header-section-number">5.3.4</span> Mode and Median</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#some-important-continuous-distributions"><span class="header-section-number">5.4</span> Some Important Continuous Distributions</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#continuous-uniform-distribution"><span class="header-section-number">5.4.1</span> Continuous Uniform Distribution</a></li>
<li><a class="nav-link" href="#normal-gaussian-distribution"><span class="header-section-number">5.4.2</span> Normal (Gaussian) distribution</a></li>
<li><a class="nav-link" href="#the-chi-squared-distribution"><span class="header-section-number">5.4.3</span> The Chi-squared distribution</a></li>
<li><a class="nav-link" href="#the-student-t-distribution"><span class="header-section-number">5.4.4</span> The Student-t distribution</a></li>
<li><a class="nav-link" href="#the-f-distribution"><span class="header-section-number">5.4.5</span> The F distribution</a></li>
<li><a class="nav-link" href="#the-lognormal-distribution"><span class="header-section-number">5.4.6</span> The lognormal distribution</a></li>
<li><a class="nav-link" href="#exponential-distribution"><span class="header-section-number">5.4.7</span> Exponential distribution</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#transformation-of-variables"><span class="header-section-number">5.5</span> Transformation of variables</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#transformation-of-discrete-random-variables"><span class="header-section-number">5.5.1</span> Transformation of discrete random variables</a></li>
<li><a class="nav-link" href="#transformation-of-variables-using-the-cdf"><span class="header-section-number">5.5.2</span> Transformation of variables using the CDF</a></li>
<li><a class="nav-link" href="#transformation-of-continuous-rv-through-pdf"><span class="header-section-number">5.5.3</span> Transformation of continuous RV through PDF</a></li>
<li><a class="nav-link" href="#a-caveat"><span class="header-section-number">5.5.4</span> A caveat</a></li>
</ul>
</li>
<li><a class="nav-link" href="#the-big-picture"><span class="header-section-number">5.6</span> The big picture</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>🃏 Probability I</strong>: Course Notes" was written by Dr. Daniel Flores Agreda (based on the Lecture by Prof. Davide La Vecchia). </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
