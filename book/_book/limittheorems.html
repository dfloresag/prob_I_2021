<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 6 üìù Limit Theorems | üÉè Probability I</title>
<meta name="author" content="Dr.¬†Daniel Flores Agreda (based on the Lecture by Prof.¬†Davide La Vecchia)">
<meta name="description" content="Figure 6.1: ‚ÄòAreUnormal‚Äô by Enrico Chavez   6.1 Markov‚Äôs and Chebyshev‚Äôs Inequalities We will start by overviewing two inequalities that allow the computation of upper bounds for probability...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 6 üìù Limit Theorems | üÉè Probability I">
<meta property="og:type" content="book">
<meta property="og:description" content="Figure 6.1: ‚ÄòAreUnormal‚Äô by Enrico Chavez   6.1 Markov‚Äôs and Chebyshev‚Äôs Inequalities We will start by overviewing two inequalities that allow the computation of upper bounds for probability...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 6 üìù Limit Theorems | üÉè Probability I">
<meta name="twitter:description" content="Figure 6.1: ‚ÄòAreUnormal‚Äô by Enrico Chavez   6.1 Markov‚Äôs and Chebyshev‚Äôs Inequalities We will start by overviewing two inequalities that allow the computation of upper bounds for probability...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="Course Notes">üÉè Probability I</a>:
        <small class="text-muted">Course Notes</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">About this lecture</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="settheory.html"><span class="header-section-number">2</span> Elements of Set Theory for Probability</a></li>
<li><a class="" href="axioms.html"><span class="header-section-number">3</span> Probability Axioms</a></li>
<li><a class="" href="discreterv.html"><span class="header-section-number">4</span> üîß Discrete Random Variables</a></li>
<li><a class="" href="continuousrv.html"><span class="header-section-number">5</span> üîß Continuous Random Variable</a></li>
<li><a class="active" href="limittheorems.html"><span class="header-section-number">6</span> üìù Limit Theorems</a></li>
<li><a class="" href="bivariatediscreterv.html"><span class="header-section-number">7</span> üìù Bivariate Discrete Random Variables</a></li>
<li><a class="" href="numericalmethods.html"><span class="header-section-number">8</span> üìù Numerical Methods</a></li>
<li><a class="" href="exercise-solutions.html"><span class="header-section-number">9</span> üìù Exercise Solutions</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="limittheorems" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> üìù Limit Theorems<a class="anchor" aria-label="anchor" href="#limittheorems"><i class="fas fa-link"></i></a>
</h1>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:unnamed-chunk-182"></span>
<img src="img/fun/areUnormal.png" alt="'AreUnormal' by Enrico Chavez" width="80%"><p class="caption">
Figure 6.1: ‚ÄòAreUnormal‚Äô by Enrico Chavez
</p>
</div>
<div id="markovs-and-chebyshevs-inequalities" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Markov‚Äôs and Chebyshev‚Äôs Inequalities<a class="anchor" aria-label="anchor" href="#markovs-and-chebyshevs-inequalities"><i class="fas fa-link"></i></a>
</h2>
<p>We will start by overviewing two inequalities that allow the computation of upper bounds for probability statements and play an important role in stablishing the convergence results we‚Äôll see further in this chapter.</p>
<div id="markovs-inequality" class="section level3" number="6.1.1">
<h3>
<span class="header-section-number">6.1.1</span> Markov‚Äôs inequality<a class="anchor" aria-label="anchor" href="#markovs-inequality"><i class="fas fa-link"></i></a>
</h3>
<p>Chebychev‚Äôs inequality can be used to construct crude bounds on the probabilities associated with deviations of a random variable from its mean.</p>
</div>
</div>
<div id="sequences-of-random-variables" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Sequences of Random Variables<a class="anchor" aria-label="anchor" href="#sequences-of-random-variables"><i class="fas fa-link"></i></a>
</h2>
<p>We would like to say something about how these random variables behave as <span class="math inline">\(n\)</span> gets larger and larger (i.e.¬†as <span class="math inline">\(n\)</span> tends towards infinity, denoted by <span class="math inline">\(n\rightarrow\infty\)</span> )</p>
<p>The study of such limiting behaviour is commonly called a study of `asymptotics‚Äô ‚Äî after the word asymptote used in standard calculus.</p>
<div id="example-bernoulli-trials-and-their-sum" class="section level3" number="6.2.1">
<h3>
<span class="header-section-number">6.2.1</span> Example: Bernoulli Trials and their sum<a class="anchor" aria-label="anchor" href="#example-bernoulli-trials-and-their-sum"><i class="fas fa-link"></i></a>
</h3>
<p>Let <span class="math inline">\(\tilde Z\)</span> denote a dichotomous random variable with <span class="math inline">\(\tilde Z\sim \mathcal{B}(p)\)</span>. A sequence of Bernoulli trials provides us with a sequence of values <span class="math inline">\(\tilde Z_{1},\tilde Z_{2},...,\tilde Z_{n},...\)</span> %where each <span class="math inline">\(\tilde {Z}_{i}\)</span> is such that</p>
<p><span class="math display">\[\begin{eqnarray*}
\Pr("Success")=\Pr \left( \tilde{Z}_{i}=1\right) = p &amp; \text{and} &amp; \Pr("Failure")=\Pr \left( \tilde Z_{i}=0\right)  = 1-p
\end{eqnarray*}\]</span></p>
<p>Now let
<span class="math display">\[S_n=\sum_{s=1}^n \tilde Z_s,\]</span> the number of ‚ÄúSuccesses‚Äù in the first <span class="math inline">\(n\)</span> Bernoulli trials. This yields a new sequence of random variables</p>
<p><span class="math display">\[\begin{eqnarray*}
S_{1} &amp;=&amp; \tilde Z_{1} \\
S_{2} &amp;=&amp;\left( \tilde Z_{1}+ \tilde Z_{2}\right)\\
&amp;&amp;\vdots  \\
S_{n} &amp;=&amp;\left( \tilde Z_{1}+ \tilde Z_{2}+\cdots + \tilde Z_{n}\right) = \sum_{i=1}^n \tilde Z_i
\end{eqnarray*}\]</span></p>
<p>This new sequence is such that <span class="math inline">\(S_n\sim B(n,p)\)</span> for each <span class="math inline">\(n\)</span>.</p>
<p>Now consider the sequence:<br><span class="math display">\[{P}_n=S_n/n,\]</span>
for <span class="math inline">\(n=1,2,\ldots\)</span>, corresponds to the proportion of `Successes‚Äôin the first <span class="math inline">\(n\)</span> Bernoulli trials.</p>
<p>It is natural to ask how the behaviour of <span class="math inline">\({P}_n\)</span> is related to the true probability of a `Success‚Äô (<span class="math inline">\(p\)</span>).</p>
<!-- %- We can provide a reasonably complete description of the behaviour of $\bar{P}_n$: -->
<!-- % -->
<!-- %- $\bar{P}_n\sim \frac{1}{n}Binomial(n,p)$, -->
<!-- %- $E[\bar{P}_n]=p$ and $Var(\bar{P}_n)=p(1-p)/n$ for all $n$, and -->
<!-- %- as $n\rightarrow\infty$ the probability distribution of -->
<!-- %$$ -->
<!-- %\frac{\sqrt{n}(\bar{P}_n-p)}{\sqrt{p(1-p)}} -->
<!-- %$$ -->
<!-- %is closely approximated by a standard normal. -->
<!-- % -->
<p>Specifically, the open question at this point is: \</p>
<p> ‚ÄúDo these results imply that <span class="math inline">\({P}_n\)</span> collapses onto the true <span class="math inline">\(p\)</span> as <span class="math inline">\(n\)</span> increases, and if
so, in what way?‚Äù  \</p>
<p>To gain a clue, let us consider the simulated values of <span class="math inline">\({P}_n\)</span>.</p>
</div>
<div id="example-bernoulli-trials-and-limit-behaviour" class="section level3" number="6.2.2">
<h3>
<span class="header-section-number">6.2.2</span> Example: Bernoulli Trials and limit behaviour<a class="anchor" aria-label="anchor" href="#example-bernoulli-trials-and-limit-behaviour"><i class="fas fa-link"></i></a>
</h3>
<p>So, informally, we can claim that a sequence of random variables <span class="math inline">\(X_{1},X_{2},...,X_{n},...\)</span> is thought to converge if the probability distribution of <span class="math inline">\(X_{n}\)</span> gets more and more concentrated around a single point as <span class="math inline">\(n\)</span> tends to infinity.</p>
</div>
</div>
<div id="convergence-in-probability-oversetprightarrow" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> Convergence in Probability (<span class="math inline">\(\overset{p}{\rightarrow }\)</span>)<a class="anchor" aria-label="anchor" href="#convergence-in-probability-oversetprightarrow"><i class="fas fa-link"></i></a>
</h2>
<p>More formally,</p>
<div id="operational-rules-for-oversetprightarrow" class="section level3" number="6.3.1">
<h3>
<span class="header-section-number">6.3.1</span> Operational Rules for <span class="math inline">\(\overset{p}{\rightarrow }\)</span><a class="anchor" aria-label="anchor" href="#operational-rules-for-oversetprightarrow"><i class="fas fa-link"></i></a>
</h3>
<p>Let us itemize some rules. To this end, let <span class="math inline">\(a\)</span> be any (nonrandom) number so:</p>
<ul>
<li><p>If <span class="math inline">\(X_{n}\overset{p}{\rightarrow } \alpha\)</span> then</p></li>
<li><p><span class="math inline">\(aX_{n}\overset{p}{\rightarrow }a\alpha\)</span> and</p></li>
<li><p><span class="math inline">\(a+X_{n}\overset{p}{\rightarrow }a+\alpha\)</span>,</p></li>
<li>
<p>If <span class="math inline">\(X_{n}\overset{p}{\rightarrow }X\)</span> then</p>
<ul>
<li>
<span class="math inline">\(aX_{n}\overset{p}{\rightarrow }aX\)</span> and</li>
<li><span class="math inline">\(a+X_{n}\overset{p}{\rightarrow }a+X\)</span></li>
</ul>
</li>
<li>
<p>If <span class="math inline">\(X_{n}\overset{p}{\rightarrow }\alpha\)</span> and <span class="math inline">\(Y_{n}\overset{p}{\rightarrow }\gamma\)</span> then</p>
<ul>
<li>
<span class="math inline">\(X_{n}Y_{n}\overset{p}{\rightarrow }\alpha \gamma\)</span> and</li>
<li>
<span class="math inline">\(X_{n}+Y_{n}\overset{p}{\rightarrow }\alpha +\gamma\)</span>.</li>
</ul>
</li>
<li>
<p>If <span class="math inline">\(X_{n}\overset{p}{\rightarrow }X\)</span> and <span class="math inline">\(Y_{n}\overset{p}{\rightarrow }Y\)</span> then</p>
<ul>
<li>
<span class="math inline">\(X_{n}Y_{n}\overset{p}{\rightarrow }X Y\)</span> and<br>
</li>
<li><span class="math inline">\(X_{n}+Y_{n}\overset{p}{\rightarrow }X +Y\)</span></li>
</ul>
</li>
<li><p>Let <span class="math inline">\(g\left( x\right)\)</span> be any (non-random) continuous function. If <span class="math inline">\(X_{n}\overset{p}{\rightarrow }\alpha\)</span> then
<span class="math display">\[g\left( X_{n}\right) \overset{p}{\rightarrow }g\left( \alpha \right),\]</span> and if <span class="math inline">\(X_{n}\overset{p}{\rightarrow }X\)</span> then</p></li>
</ul>
<p><span class="math display">\[g\left( X_{n}\right) \overset{p}{\rightarrow }g\left( X \right).\]</span></p>
<p>Suppose <span class="math inline">\(X_{1},X_{2},...,X_{n},...\)</span> is a sequence of  random variables with common distribution <span class="math inline">\(F_X(x)\)</span> and moments <span class="math inline">\(\mu_r=E [X^r]\)</span>. At any given point along the sequence, <span class="math inline">\(X_{1},X_{2},...,X_{n}\)</span> constitutes a simple random sample of size <span class="math inline">\(n\)</span>. \</p>
<p>For each fixed sample size <span class="math inline">\(n\)</span>, the <span class="math inline">\(r\)</span>th sample moment is (using an obvious notation)
<span class="math display">\[\begin{equation*}
M_{(r,n)}=\frac{1}{n}\left( X_{1}^r+X_{2}^r+\cdots +X_{n}^r\right)=\frac{1}{n}\sum_{s=1}^nX_s^r\,,
\end{equation*}\]</span>
and we know that
<span class="math display">\[E[M_{(r,n)}]=\mu_r\quad\text{and}\quad Var(M_{(r,n)})=\frac{1}{n}(\mu_{2r}-\mu_r^2)\,.\]</span></p>
<p>Now consider the sequence of sample moments <span class="math inline">\(M_{(r,1)},M_{(r,2)},...,M_{(r,n)},...\)</span> or, equivalently, <span class="math inline">\(\{M_{(r,i)}\}_{i=1}^{n}\)</span>.</p>
<!-- % -->
<!-- % -->
<!-- % -->
<!-- % -->
<!-- % -->
<!-- %\frametitle{An informal example (continued)} -->
<!-- % -->
<!-- % -->
<!-- %- The expected value of the sample mean is% -->
<!-- %\begin{eqnarray*} -->
<!-- %E\left[ \overline{X}_{n}\right]  &=&E\left[ \frac{1}{n}\left( -->
<!-- %X_{1}+X_{2}+\cdots +X_{n}\right) \right]  \\ -->
<!-- %&=&\frac{1}{n}\left( E\left[ X_{1}\right] +E\left[ X_{2}\right] +\cdots +E% -->
<!-- %\left[ X_{n}\right] \right)  \\ -->
<!-- %&=&\frac{1}{n}\left( \mu +\mu +\cdots +\mu \right) =\mu -->
<!-- %\end{eqnarray*} -->
<!-- % -->
<!-- %- The variance of the sample mean is% -->
<!-- %\begin{eqnarray*} -->
<!-- %Var\left( \overline{X}_{n}\right)  &=&Var\left( \frac{1}{n}\left( -->
<!-- %X_{1}+X_{2}+\cdots +X_{n}\right) \right)  \\ -->
<!-- %&=&\frac{1}{n^{2}}\left( Var\left( X_{1}\right) +Var\left( X_{2}\right) -->
<!-- %+\cdots +Var\left( X_{n}\right) \right)  \\ -->
<!-- %&=&\frac{1}{n^{2}}\left( \sigma ^{2}+\sigma ^{2}+\cdots +\sigma ^{2}\right) -->
<!-- %\\ -->
<!-- %&=&\frac{n\sigma ^{2}}{n^{2}}=\frac{\sigma ^{2}}{n} -->
<!-- %\end{eqnarray*} -->
<!-- % -->
<!-- % -->
<!-- % -->
<!-- % -->
<!-- % -->
<!-- % -->
</div>
<div id="convergence-of-sample-moments-as-a-motivation" class="section level3" number="6.3.2">
<h3>
<span class="header-section-number">6.3.2</span> Convergence of Sample Moments as a motivation‚Ä¶<a class="anchor" aria-label="anchor" href="#convergence-of-sample-moments-as-a-motivation"><i class="fas fa-link"></i></a>
</h3>
<p>The distribution of <span class="math inline">\(M_{(r,n)}\)</span> (which is unknown because <span class="math inline">\(F_X(x)\)</span> has not been specified) is thus concentrated around <span class="math inline">\(\mu_r\)</span> for all <span class="math inline">\(n\)</span>,
with a variance which tends to zero as <span class="math inline">\(n\)</span> increases. \</p>
<p>So the distribution of <span class="math inline">\(M_{(r,n)}\)</span> becomes more and more concentrated around <span class="math inline">\(\mu_r\)</span> as $n
$ increases and therefore we might  that
<span class="math display">\[\begin{equation*}
M_{(r,n)}\overset{p}{\rightarrow }\mu_r.
\end{equation*}\]</span></p>
<p>In fact, this result follows from what is known as the <strong>Weak Law of Large Numbers</strong> (WLLN).</p>
<!-- %- N.B. If we define the sample MGF as -->
<!-- %$$ -->
<!-- %\bar{M}_X(t)=\frac{1}{n}\sum_{s=1}^ne^{tX_s}=\sum_{r=0}^\infty M_{(r,n)}\frac{t^r}{r!}\,, -->
<!-- %$$ -->
<!-- %then -->
<!-- %$$ -->
<!-- %\bar{M}_X(t)-M_X(t)=\sum_{r=0}^\infty (M_{(r,n)}-\mu_r)\frac{t^r}{r!}\,. -->
<!-- %$$ -->
<!-- %Using the operational rules for probability limits indicates that $p\lim\bar{M}_X(t)=M_X(t)$. This suggests \textit{estimating} $F_X(x)$ from "the data"! -->
<!-- %%- But how can we formally prove this WLLN? -->
</div>
<div id="the-weak-law-of-large-numbers-wlln" class="section level3" number="6.3.3">
<h3>
<span class="header-section-number">6.3.3</span> The Weak Law of Large Numbers (WLLN)<a class="anchor" aria-label="anchor" href="#the-weak-law-of-large-numbers-wlln"><i class="fas fa-link"></i></a>
</h3>
</div>
<div id="the-wlln-and-chebyshevs-inequality" class="section level3" number="6.3.4">
<h3>
<span class="header-section-number">6.3.4</span> The WLLN and Chebyshev‚Äôs Inequality<a class="anchor" aria-label="anchor" href="#the-wlln-and-chebyshevs-inequality"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>First note that <span class="math inline">\(E[\overline{Y}_n]=\mu_Y\)</span> and <span class="math inline">\(Var(\overline{Y}_n)=\sigma_Y^2/n\)</span>.</li>
<li>Now, according to <strong>Chebyshev‚Äôs inequality</strong>
<span class="math display">\[\begin{eqnarray*}
\Pr \left( |\overline{Y}_{n}-\mu_Y| &lt;\varepsilon\right)  &amp;\geq &amp;1-\frac{E\left[ \left(
\overline{Y}_{n}-\mu_Y \right) ^{2}\right] }{\varepsilon^{2}} \\
&amp;=&amp;1-\frac{\sigma_Y ^{2}/n}{\varepsilon^{2}} \\
&amp;=&amp;1-\frac{\sigma_Y ^{2}}{n\varepsilon^{2}}\geq 1-\delta
\end{eqnarray*}\]</span>
for all <span class="math inline">\(n&gt;\sigma_Y^2/(\varepsilon^2\delta)\)</span>.</li>
<li>Thus the WLLN is proven, provided we can verify <strong>Chebyshev‚Äôs inequality</strong>.</li>
<li>Note that by considering the limit as <span class="math inline">\(n\rightarrow \infty\)</span> we also have%
<span class="math display">\[\begin{equation*}
\lim_{n\rightarrow \infty }\Pr \left( \left\vert \overline{Y}_{n}-\mu_Y\right\vert &lt;\varepsilon\right) \geq \lim_{n\rightarrow \infty }\left( 1-\frac{\sigma^{2}}{n\varepsilon^{2}}\right) =1\,,
\end{equation*}\]</span>
again implying that <span class="math inline">\(\left( \overline{Y}_{n}-\mu_Y \right) \overset{p}{\rightarrow }0\)</span>.</li>
</ul>
<!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- %\frametitle{Proof of Chebyshev's inequality (continuous case)} --><!-- % --><!-- % --><!-- %- We can rewrite the RHS of Chebyshev's inequality as --><!-- %\begin{eqnarray*} --><!-- %1-\frac{E\left( Z^{2}\right) }{t^{2}} &=&E\left[ 1-\frac{Z^{2}}{t^{2}}\right] --><!-- %\\ --><!-- %&=&\int_{-\infty }^{\infty }\left( 1-\frac{z^{2}}{t^{2}}\right) f_{Z}\left( --><!-- %z\right) dz \\ --><!-- %&=&\int_{-\infty }^{-t}\left( 1-\frac{z^{2}}{t^{2}}\right) f_{Z}\left( --><!-- %z\right) dz \\ --><!-- %&&+\int_{-t}^{t}\left( 1-\frac{z^{2}}{t^{2}}\right) f_{Z}\left( z\right) dz --><!-- %\\ --><!-- %&&+\int_{t}^{\infty }\left( 1-\frac{z^{2}}{t^{2}}\right) f_{Z}\left( --><!-- %z\right) dz --><!-- %\end{eqnarray*} --><!-- % --><!-- % --><!-- %- i.e. just break up the integral into three parts --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- %\frametitle{Proof of Chebyshev's inequality (continued)} --><!-- % --><!-- % --><!-- %- Notice that both of the outer integrals are non-positive, --><!-- % --><!-- % --><!-- %- i.e.% --><!-- %\begin{equation*} --><!-- %\int_{-\infty }^{-t}\left( 1-\frac{z^{2}}{t^{2}}\right) f_{Z}\left( z\right) --><!-- %dz\leq 0 --><!-- %\end{equation*} --><!-- % --><!-- %- and% --><!-- %\begin{equation*} --><!-- %\int_{t}^{\infty }\left( 1-\frac{z^{2}}{t^{2}}\right) f_{Z}\left( z\right) --><!-- %dz\leq 0 --><!-- %\end{equation*} --><!-- % --><!-- %- since in both cases $\left( 1-\frac{z^{2}}{t^{2}}\right) \leq 0$ --><!-- % --><!-- % --><!-- %- And so --><!-- %\begin{eqnarray*} --><!-- %E\left[ 1-\frac{Z^{2}}{t^{2}}\right]  &=&\int_{-t}^{t}\left( 1-\frac{z^{2}}{% --><!-- %t^{2}}\right) f_{Z}\left( z\right) dz+\text{something negative} \\ --><!-- %&\leq &\int_{-t}^{t}\left( 1-\frac{z^{2}}{t^{2}}\right) f_{Z}\left( z\right) --><!-- %dz --><!-- %\end{eqnarray*} --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- %\frametitle{Proof of Chebyshev's inequality (continued)} --><!-- % --><!-- % --><!-- %- So therefore we have that% --><!-- %\begin{eqnarray*} --><!-- %E\left[ 1-\frac{Z^{2}}{t^{2}}\right]  &\leq &\int_{-t}^{t}\left( 1-\frac{% --><!-- %z^{2}}{t^{2}}\right) f_{Z}\left( z\right) dz \\ --><!-- %&=&\int_{-t}^{t}f_{Z}\left( z\right) dz-\int_{-t}^{t}\frac{z^{2}}{t^{2}}% --><!-- %f_{Z}\left( z\right) dz \\ --><!-- %&=&\Pr \left( -t<Z<t\right) -\text{something positive} \\ --><!-- %&\leq &\Pr \left( -t<Z<t\right) --><!-- %\end{eqnarray*} --><!-- % --><!-- %- and hence Chebyshev's inequality holds --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><!-- % --><ul>
<li><p>If <span class="math inline">\(p\lim_{n\rightarrow\infty}(X_n-X)=0\)</span> then <span class="math inline">\(X_{n}\overset{D}{\rightarrow }X\)</span>.</p></li>
<li><p>Let <span class="math inline">\(a\)</span> be any real number. If <span class="math inline">\(X_{n}\overset{D}{\rightarrow }X\)</span>, then
<span class="math inline">\(aX_{n}\overset{D}{\rightarrow }aX\)</span></p></li>
<li><p>If <span class="math inline">\(Y_{n}\overset{p}{\rightarrow }\phi\)</span> and <span class="math inline">\(X_{n}\overset{D}{% \rightarrow }X\)</span>, then</p></li>
<li><p><span class="math inline">\(Y_{n}X_{n}\overset{D}{\rightarrow }\phi X,\)</span> and</p></li>
<li><p><span class="math inline">\(Y_{n}+X_{n}\overset{D}{\rightarrow }\phi +X\)</span></p></li>
<li><p>If <span class="math inline">\(X_{n}\overset{D}{\rightarrow }X\)</span> and <span class="math inline">\(g\left( x\right)\)</span> is any
continuous function, then <span class="math inline">\(g\left( X_{n}\right) \overset{D}{\rightarrow }% g\left( X\right)\)</span></p></li>
</ul>
<!-- %- If $Y_{n}\overset{D}{\rightarrow }Y$ and $X_{n}\overset{D}{\rightarrow --><!-- %}X$, then --><!-- % --><!-- %- $Y_{n}X_{n}\overset{D}{\rightarrow }YX,$ and --><!-- % --><!-- %- $Y_{n}+X_{n}\overset{D}{\rightarrow }Y +X$ --><!-- % --><p>The following theorem is often said to be one of the most important results. Its significance lies in the fact that it allows accurate probability calculations to be made without knowledge of the underlying distributions!</p>
<p>%</p>
<!-- % -->
<!-- %\begin{figure}[ptb]\centering -->
<!-- %\includegraphics[height=3in, width=3.6in]{Poisson.pdf}% -->
<!-- %\end{figure}% -->
<!-- % -->
<p>%</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="continuousrv.html"><span class="header-section-number">5</span> üîß Continuous Random Variable</a></div>
<div class="next"><a href="bivariatediscreterv.html"><span class="header-section-number">7</span> üìù Bivariate Discrete Random Variables</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#limittheorems"><span class="header-section-number">6</span> üìù Limit Theorems</a></li>
<li>
<a class="nav-link" href="#markovs-and-chebyshevs-inequalities"><span class="header-section-number">6.1</span> Markov‚Äôs and Chebyshev‚Äôs Inequalities</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#markovs-inequality"><span class="header-section-number">6.1.1</span> Markov‚Äôs inequality</a></li></ul>
</li>
<li>
<a class="nav-link" href="#sequences-of-random-variables"><span class="header-section-number">6.2</span> Sequences of Random Variables</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#example-bernoulli-trials-and-their-sum"><span class="header-section-number">6.2.1</span> Example: Bernoulli Trials and their sum</a></li>
<li><a class="nav-link" href="#example-bernoulli-trials-and-limit-behaviour"><span class="header-section-number">6.2.2</span> Example: Bernoulli Trials and limit behaviour</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#convergence-in-probability-oversetprightarrow"><span class="header-section-number">6.3</span> Convergence in Probability (\(\overset{p}{\rightarrow }\))</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#operational-rules-for-oversetprightarrow"><span class="header-section-number">6.3.1</span> Operational Rules for \(\overset{p}{\rightarrow }\)</a></li>
<li><a class="nav-link" href="#convergence-of-sample-moments-as-a-motivation"><span class="header-section-number">6.3.2</span> Convergence of Sample Moments as a motivation‚Ä¶</a></li>
<li><a class="nav-link" href="#the-weak-law-of-large-numbers-wlln"><span class="header-section-number">6.3.3</span> The Weak Law of Large Numbers (WLLN)</a></li>
<li><a class="nav-link" href="#the-wlln-and-chebyshevs-inequality"><span class="header-section-number">6.3.4</span> The WLLN and Chebyshev‚Äôs Inequality</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>üÉè Probability I</strong>: Course Notes" was written by Dr.¬†Daniel Flores Agreda (based on the Lecture by Prof.¬†Davide La Vecchia). </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
