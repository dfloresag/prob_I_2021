# Introduction {#introduction}

<!-- You can label chapter and section titles using `{#label}` after them, e.g., we can reference Chapter \@ref(intro). If you do not manually label them, there will be automatic labels anyway, e.g., Chapter \@ref(methods). -->

<!-- Figures and tables with captions will be placed in `figure` and `table` environments, respectively. -->

<!-- # ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center'} -->
<!-- # [Probability, union, and complement]par(mar = c(4, 4, .1, .1)) -->
<!-- # plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Reference a figure by its code chunk label with the `fig:` prefix, e.g., see Figure \@ref(fig:nice-fig). Similarly, you can reference tables generated from `knitr::kable()`, e.g., see Table \@ref(tab:nice-tab). -->

<!-- # ```{r nice-tab, tidy=FALSE} -->
<!-- # knitr::kable( -->
<!-- #   head(iris, 20), caption = 'Here is a nice table!', -->
<!-- #   booktabs = TRUE -->
<!-- # ) -->
<!-- ``` -->

```{r ,echo = FALSE, fig.cap='_Taking statistics to schools_ by Enrico Chavez', fig.align='center', out.width="75%"}
  knitr::include_graphics("img/fun/EC_Talking_statistics_to_schools.png")
```

## A World of Data

**Data is the most important commodity in our connected world of today**. Generated at a breakneck pace and related to practically all of the aspects of human life, Data has become a valuable resource for the creation of products and services designed to improve people's lives. Gathering this data can potentially help companies design better policies and companies design better products.

To illustrate this point, take a look at the following interactive map of the internal migration in Switzerland in 2016. Seizing historical data like this could help decision makers, such as politicians or managers in housing firms, identify migration trends and inform their decisions in what pertains, say housing projects or other infrastructure investments.

<!-- Insert a better explanation of this -->

```{r, echo = FALSE, fig.align='center', fig.cap = 'Inter-cantonal migration in Switzerland in 2016, an interactive map by Ilya Bobadin with data from the Swiss Federal Statistical Office. [https://flowmap.blue/](https://flowmap.blue/).'}
knitr::include_url("https://flowmap.blue/15kwLB4baXZ7jpip8q0JjgR6zDoS5Gt3gMLCTUAboQxk?v=46.760927,8.215964,6.51,0,0&a=0&as=1&b=1&bo=75&c=0&ca=1&d=0&fe=1&lt=1&lfm=ALL&col=Default&f=45")
```

In order to make sense of the immense amounts of information generated every millisecond, computational tools are needed. Thankfully, in this day and age, computing power allows easy and fast manipulation of that enormous amount of information, and more and more sophisticated algorithms are designed to analyse it, **yet it is not only by "brute force" i.e. algorithmic power that appropriate insights are found**. To make sense of the data, we also need to consider its features from a **mathematical standpoint**. 

The field of Mathematics that provides a formal framework of the data generation process the field of **Probability**, and it is the building block for **Statistics**, the domain of knowledge that deals with the creation of mathematical tools to analyse data.  

## What to expect from this Lecture?

In this course, **we study Probability as a way to introduce Statistics**.

Thus we do not study Probability as a subject of interest in itself. This means that we do not develop deep probability theory using all the formal theoretical arguments, but rather **we set up a few principles and methods** which will be **helpful to study Statistics**. 

<!-- \includegraphics[width=0.6\textwidth,height=0.7\textheight]{y.pdf} -->

Hence, for the purposes of this lecture, we shall consider:  

- **Statistics**: as the discipline that deals with the **collection**, **presentation**, **analysis** and **interpretation** of data. In future Classes, you will learn classical Statistical methods as well as their modern reincarnation (or "rebranding"): algorithmic tools known as Machine Learning, Deep Learning, etc.

- **Probability** as the mathematical formalisation of randomness and uncertainty the main building block of Statistical methods. 

```{r ,echo = FALSE, fig.cap='_Thinking of learning ML this weekend, is there math?_ (2020). Retrieved from [r/DataScienceMemes](https://www.reddit.com/r/DataScienceMemes/comments/hqi8sv/thinking_of_learning_ml_this_weekend_is_there_math/?utm_source=share&utm_medium=web2x&context=3)',fig.align='center', out.width="75%"}
  knitr::include_graphics("https://i.redd.it/8r2fd78pgna51.png")
```

### One intuitive illustration

Before jumping into the core of the subject, I would like to make a little detour that helps illustrate how prevalent  probabilities are in practically every aspect of daily life. Oftentimes, in Movies and TV shows, we hear characters facing uncertain outcomes. When our protagonists are about to make life altering decisions they often ask themselves: 

> What are my **chances** of winning? 

To which some other character replies with some phrase like:

> "I'd say more like one out of a million."

In your mind, this number becomes an indication of the _likelihood_ of the event happening. 

> "So, you're telling me that THERE IS A CHANCE!"

Intuitively, you understand this as the result of the following rationale: were you to _repeat_ the _experiment_, your "chances of success" result from a ratio between the number of times you succeed, versus the number of attempts.

$$\text{my chances} = \frac{\text{number of times I succeed}}{\text{number of times I fail}}$$
In a bit of an abuse of language, this value, constitutes the **probability** of success. 

When it's small, success is very _unlikely_, but if it is large (or at least closer to 1), your success seems more likely, as the times you succeed in the experiments are more frequent. 

Probability constitutes then the **measure of the "likelihood"** of an event or random outcome.

<!-- Hence, we can measure uncertainty in units of probability. -->

 <!-- Statistics is concerned with **study of uncertainty** and with the _study of data-driven decision making in the face of uncertainty_. -->

<!-- To answer this question we make use of Probability. Indeed, Probability is all about the certainty/uncertainty and the prediction of something happening. Some events are impossible, other events are certain to occur, while many are possible, but not certain to occur... -->

>   "In this world there is nothing certain but death and taxes."
>
> ---  Benjamin Franklin

<!-- \includegraphics[width=0.4\textwidth,height=0.5\textheight]{profit.pdf} -->

<!-- \includegraphics[width=0.3\textwidth,height=0.3\textheight]{tipoS.pdf} -->


#### Another illustration: Stock price evolution

Let us see how to characterise the uncertainty of the price of a financial asset, such as a Stock, with a simple probabilistic model. To account for the fluctuations in the price, we can make the following simple assumptions:

Let $S_0$ denote the price of the Stock at time $t_0$, and consider that:

- with probability $p$ the stock price increases by a factor $u>1$, and 
- with probability $1-p$ the price goes down a factor $d<1$. 

We can therefore infer the price at time $t_1$  by $S_1 = uS_0$ if the price goes up, and by $S_1=dS_0$  if the price goes down. 

Let us set $S_0=1$ (US Dollar) and the factors $u=2$ (i.e. doubling the price) and $d=1/2$ (i.e. halving the price). **What can we say about the price at time $t_2$?**

To answer this last question we could represent the evolution of the price using a **Probability Tree Diagram**. In such charts, we organise the different **outcomes in branches**. At the end of each branch (in the _nodes_) , we place the outcomes while, hovering over each branch (on the _edges_) we place the probability associated with the outcome. 

```{r, echo=FALSE, engine='tikz', fig.cap='Stock Price Evolution as a Probability Tree Diagram', fig.align='center'}
\tikzstyle{bag} = [text width=8em, text centered]
\tikzstyle{end} = []
\begin{tikzpicture}[sloped]
   \node (a) at ( 0,0) [bag] {$\$ 1$};
   \node (b) at ( 4,-1.5) [bag] {$\$ d =\$ 0.5$};
   \node (c) at ( 4,1.5) [bag] {$\$ u= \$ 2$};
   \node (d) at ( 8,-3) [bag] {$\$ d^2= \$ 0.25$};
   \node (e) at ( 8,0) [bag] {$\$ ud= \$ du = \$ 1$};
   \node (f) at ( 8,3) [bag] {$\$ u^2=\$ 4$};
   \draw [->] (a) to node [below] {$(1-p)$} (b);
   \draw [->] (a) to node [above] {$p$} (c);
   \draw [->] (c) to node [below] {$p^2$} (f);
   \draw [->] (c) to node [above] {$(1-p)p$} (e);
   \draw [->] (b) to node [below] {$(1-p)p$} (e);
   \draw [->] (b) to node [above] {$(1-p)^2$} (d);
\end{tikzpicture}
```


- To compute the probability of successive outcomes, we **_multiply_ the probabilities** on the edges leading to this outcome. 
- If there is more than one path leading to a given outcome, we **sum the total probabilty** of the edges yielding the outcome.  

Hence: 

- The probability of $S_2 = u^2 = 4$ is $p^2$
- The probability of $S_2 = d^2 = 4$ is $(1-p)^2$
- The probability of $S_2 = d^2 = 1$ is $(1-p)p+p(1-p)$


<!-- #### Example: Quanta -->


<!-- \includegraphics[width=0.3\textwidth,height=0.2\textheight]{Quantum.pdf} -->

<!-- \includegraphics[width=0.4\textwidth,height=0.9\textheight, angle = -90]{cern.pdf} -->


<!-- Probability models are the basis for quantum physics: they characterize the **uncertainty** of properties of single energy ``quanta'' emitted by a ``perfect radiator" with a given temperature ($T$). Specifically, recall the famous Einstein's equation $$\boxed{\mathcal{E} = m c^2}$$ which expresses the energy $\mathcal{E}$  in terms of the mass \underline{$m$, a random quantity,} and $c$, the speed of light. Moreover, consider the geometric energy mean defined by ${\mathcal{E}_G} = c_0 k_B T$, where $c_0 \approx 2.134$ and $k_B$ is Boltzmann's constant. Thus, one can define the random quantity -->

<!-- $$W = \frac{\mathcal{E}}{{\mathcal{E}_G}},$$ -->
<!-- which is called _quantum mass ratio_. -->


<!-- The random behaviour of $W$ can be described by a probability density function: -->

<!-- > Insert Illustration or animation of the quantum mass ratio.  -->

<!-- \includegraphics[width=0.4\textwidth,height=0.6\textheight, angle = 0]{W.pdf} -->


## A quick reminder of Mathematics

Probability theory is a mathematical tool. Hence, it is important to review some elemental concepts. Here are some of the formulae that we will use throughout the course. 
 
### Powers and Logarithms

- $a^m \times a^n = a^{m+n}$;
- $(a^n)^m = a^{m \times n}$;
- $\ln(\exp^{a}) = a$;
- $a=\ln(\exp^{a}) = \ln(e^a)$;
- $\ln(a^n) = n \times \ln a$;
- $\ln (a \times b) = \ln (a) + \ln (b)$;


### Differentiation 

**Derivatives** will also play a pivotal role. Start by remembering some of the basic derivation operations:

- Derivative of _$x$ to the power $n$_, $f(x)= x^n$
$$\frac{d x^n}{dx} = n \cdot x^{n-1}$$ 

- Derivative of the exponential function, $f(x) = \exp(x)$ 
$$\frac{d \exp^{x}}{dx} = \exp^{x}$$ 

- Derivative of the natural logarithm, $f(x) = \ln(x)$ 
$$ \frac{d \ln({x})}{dx} = \frac{1}{x}$$

Moreover, we will make use of some fundamental derivation rules, such as:

#### Product rule

The derivative of the product between two functions is given as follows:

\begin{align} 
\frac{d [f(x)\cdot g(x)]}{dx} &= \frac{df(x)}{dx} g(x) + \frac{dg(x)}{dx} f(x) \\ 
&= f'(x) g(x)+ f(x) g'(x)
\end{align}

#### Chain rule

Let $f(u)$ and $g(x)$ two functions. Let $f\circ g(x) = f[g(x)]$ be the **composite function**. The derivative of the composite is given by the product:

$$\frac{d f[g(x)]}{dx} =  (f\circ g)'(x) = f'[g(x)] \cdot g'(x)$$

### Integration

Integrals will be crucial in many tasks. For instance, recall that integration is _linear_ over the sum, i.e. $\forall c, d \in \mathbb{R}$ 

$$\int_{a}^{b} \left[c \times f(x) + d \times g(x) \right]dx = c  \times \int_{a}^{b}   f(x) dx + d \times \int_{a}^{b}   g(x) dx; $$

<!-- %-  A special case of Leibnitz's rule: $$ \frac{d}{dx} \int_{-\infty}^{x} f(s) ds = f(x);$$   -->
<!-- %-  -->
<!-- %\begin{eqnarray}  -->
<!-- %\int_{a}^{b} f(x) dx = \int_{a}^{m} f(x) dx + \int_{m}^{b}  f(x) dx,  \quad { \text{\ for \ } m \in [a,b]; }\nn -->
<!-- %\end{eqnarray}  -->

- If the function is _positive_ $f(x) \geq 0, \forall x \in \mathbb{R}$, then its integral is also positive. 

$$\int_{\mathbb{R}} f(x) dx \geq 0.$$

- For a continuous function $f(x)$, the **indefinite integral** is 

$$\int f(x) dx = F(x) + \text{const}$$

- while the **definite integral** is

$$F(b)-F(a)= \int_{a}^{b} f(x) dx, \quad b \geq a.$$
And in both cases: $f(x) = F'(x)$

### Sums

Besides integrals we are also going to use **sums**:

- Sums are denoted with a $\Sigma$ operator and an _index_ $i$, as in:

$$\sum_{i=1}^{n} X_{i} = X_1 + X_2 +....+ X_n,$$

- Moreover, for every $\alpha_i \in \mathbb{R}$,  
$$\sum_{i=1}^{n} \alpha_i X_{i} = \alpha_1 X_1 + \alpha_2 X_2 +....+ \alpha_n X_n;$$ 
<!-- %whose special case is -->
<!-- %$ -->
<!-- %\sum_{i=1}^{n} \alpha X_{i} = \alpha X_{i} = \alpha X_1 + \alpha X_2 +....+ \alpha X_n = \alpha \sum_{i=1}^{n} X_{i} -->
<!-- %$ -->

- A **double sum** is a sum operated over two _indices_. For instance consider the sum of the product of two sequences $\{x_1, \dots, x_n\}$ and $\{y_1, \dots, y_m\}$,

$$\sum_{i=1}^{n} \sum_{j=1}^{m}  x_{i}y_{j}  = x_1y_1 + x_1 y_2 +... +x_2y_1+ x_2y_2 + \dots$$

- by carefully arranging the terms in the sum, we can establish the following identity: 

\begin{align}
\sum_{i=1}^{n} \sum_{j=1}^{m}  x_{i}y_{j}  &= x_1y_1 + x_1 y_2 +... +x_2y_1+ x_2y_2 + \dots \\
&= \left(\sum_{i=1}^{n} x_i\right) y_1 +  \left(\sum_{i=1}^{n} x_i\right) y_2 + \dots + \left(\sum_{i=1}^{n} x_i\right) y_m  \\
&= \sum_{i=1}^{n} x_i \sum_{j=1}^{m} y_j.
\end{align}



### Combinatorics

Finally, we will also rely on some **combinatorial formulas**. Specifically,

#### Factorial 

\begin{equation} 
n! = n \times (n-1) \times (n-2) \times \dots \times 1;
(\#eq:factorial)
\end{equation}

where $0! =1$, by convention. 

#### The Binomial Coefficient 

The **Binomial coefficient**, for $n \geq k$ is defined by the following ratio:
\begin{equation}
\binom n k =\frac{n!}{k!(n-k)!}{\color{blue}{=C^{k}_n}}.
(\#eq:binomialcoef)
\end{equation}

In English, the symbol $\binom n k$ is read as "$n$ choose $k$". We will see why in a few paragraphs.  

Combinatorial formulas are very useful when studying **permutations** and **combinations** both very recurrent concepts.

##### Permutations {-}

A permutation is an **ordered rearrangement** of the elements of a set. 

```{example, name = "Three friends and three chairs"}
Suppose we have three friends Aline ($A$), Bridget ($B$) and Carmen ($C$) that want to sit on three chairs during lunchtime.

- One day, you see Aline on the left, Bridget in the center and Carmen on the right, i.e. $(A, B, C)$
- The next day, Bridget sits first, Aline second and Carmen right, i.e. $(B, A, C)$
```

These rearrangements constitute two **Permutations** of the set of three friends $\{A, B, C\}$.

You then start wondering in how many ways these three friends can sit. With such a small set, it is easy to take a pen and some paper to write down all the possible permutations:

$$(A, B, C), (A, C, B), (B, A, C), (B, C, A), (C, A, B), (C, B, A)$$

As you can see, the total number of permutations is : $N = 6.$ But also $6= 3 \times 2 \times 1 = 3!$. This is by no means a coincidence. To see in detail why let us consider each chair as an "experiment" and its occupant an "outcome". 

- **Chair (Experiment) 1** : has 3 possible occupants (outcomes): {$A$, $B$, $C$}. 
- **Chair (Experiment) 2** : has 2 possible occupants (outcomes): either $\{B,C\}$, $\{A,C\}$ or $\{A,B\}$
- **Chair (Experiment) 3** : has 1 possible occupant (outcomes) : $\{A\}$, $\{B\}$ or $\{C\}$

Here, we can apply the Fundamental Counting Principle, i.e. from @ross2014first:

> If $r$ experiments that are to be performed are such that the first one may result
in any of $n_1$ possible outcomes; and if, for each of these $n_1$ possible outcomes,
there are $n_2$ possible outcomes of the second experiment; and if, for each of the
possible outcomes of the first two experiments, there are $n_3$ possible outcomes
of the third experiment; and so on, then there is a total of $n_1 \times n_2 \times \cdots \times n_r$ possible
outcomes of the $r$ experiments.

Hence: $3 \times 2 \times 1 = 3! = 6$

```{definition} 
**Permutations** help you answer the question: _How many different ways can we arrange $n$ objects?_

- In the 1st place: $n$ possibilities
- In the 2nd place: $(n-1)$ possibilities 
- ...
- Finally, $1$ possibility

Thus, in total we have $n\times(n-1)\times(n-2)\times...\times1 = n!$
``` 

```{exercise, name = "Gifts on the Shelf"}
Imagine that you own a gift store and you have a shelf where you want to display 5 products:
```

- A gorgeous sunflower `r emo::ji("sunflower")` 
- An old-timey Radio `r emo::ji("radio")` 
- A best-selling book `r emo::ji("book")` 
- An elegant pen `r emo::ji("pen")` 
- An incredibly charismatic turtle `r emo::ji("turtle")` 

In how many ways can you arrange them on the shelf? 

<!-- Response: $5!$ gives you the total number of possible choices when you can select 5 presents. These are called  **permutations** of the set of gifts and owe that name to the fact that we are just _reordering_ or _permuting_ the elements of the set; -->

<!-- Imagine that, due to budgetary constraints, one chair has been stolen from our friends and they make turns on who stays standing. There are of course 3 configurations in which they sit $(A,B)$, $(A,C)$ and $(B,C)$ , but we can achieve the same result with combinatorial formulae.  -->

Let us use this Exercise to go further. Suppose now that the shelf is now a bit more narrow, and you can only show 3 products instead of 5. How many ways can we select $3$ items among the $5$? 

By the Fundamental Counting Principle you have $5\times4\times3$ ways of selecting a the 3 elements. If we write them in factorial notation, this represents 
$$
5\times4\times3 = \frac{5\times4\times3\times2\times1}{2\times1} = \frac{5!}{2!} = \frac{5!}{(5-3)!}
$$
This formula can be interpreted as follows: 

_If you select $3$ presents from the list, then you have $(5-3)$ other presents that you won't select. The latter set has $(5-3)!$ possibilities which have to be **factored out** from the $5!$ possible permutations._

As you can see, we are slowly but surely arriving to the definition of $\binom n k$ in \@ref(eq:binomialcoef). However, there is still one more element... 

##### Combinations {-}

Implicit in this example, is the notion that **the order** in which these elements are displayed is **important**. This means, that the set (`r emo::ji("sunflower")`, `r emo::ji("book")`,  `r emo::ji("turtle")`) is different from the set ( `r emo::ji("book")`,  `r emo::ji("turtle")`, `r emo::ji("sunflower")`) which, as you can assess, is a **permutation** of the original draw.

Let us suppose that the **order is not important**. This implies that once the 3 gifts are chosen  **all the permutations of this subset are deemed equivalent**. Hence, the subset (`r emo::ji("sunflower")`, `r emo::ji("book")`,  `r emo::ji("turtle")`), is deemed equivalent as all its $3! = 6$ permutations and we have to *factor out* this amount from the previous result, by dividing the $3!$ different ways you can order the selected presents. 
<!-- Think of how Aline, Bridget and Carmen were sitting from your perspective. -->
<!-- - If the order left to right is important for you $(A, C, B)$ is a different configuration from $(B, C, A)$. -->
<!-- - However, if you don't care about the order, these two configurations are equivalent, as they can be read in the same way if you read the second from right to left. -->


If we put this in a formula, we have $$\frac{5!/(5-3)!}{3!}$$ ways to select the $3$ presents:

$$\frac{5!/(5-3)!}{3!}= \frac{5!}{3!2!} = \binom 5 3 = C_5^3.$$
This gives you the total number of possible ways to select the $3$ presents when the order does not matter. There are thus *"$5$ choose $k$" ways of choosing 3 elements among 5*. 

```{exercise}
What if you have only room for 2 products in your shelf? 
```


In general, $n \choose k$ results from the following computations.

- In the 1st place: $n$
- In the 2nd place: $(n-1)$
- ...
- In the $k$-th place: $(n-k+1)$


- We have $k!$ ways to permute the $k$ objects that we selected 
- The number of possibilities (without considering the order) is:

$$\frac{n!/(n-k)!}{k!} = \frac{n!}{k!(n-k)!}{\color{gray}{=C^{k}_n}}$$

For the Problem Set $2$, you will have to make use of $C^{k}_n$ in Ex2-Ex3-Ex5. Indeed, 
to compute the probability for an event $E$, will have to make use of the formula:

\begin{equation} 
P(E)=\dfrac{\text{number of cases in E}}{\text{number of possible cases}}.
(\#eq:pe)
\end{equation}

This is a first intuitive definition of probability, which we will justify in the next chapter. For the time being, let us say that the combinatorial calculus will be needed to express both the quantities (numerator and denominator) in \@ref(eq:pe).  

The name "Binomial Coefficient" is closely associated with the **Binomial Theorem**, which provides the expression for the power $n$ of the sum of two variables $(x+y)$ as a sum:

```{theorem, name = "Binomial Theorem - Pascal 1654"}

For $n=0,1, 2,\dots$,  we have:

$$(x+y)^n = {n \choose 0}x^n y^0 + {n \choose 1}x^{n-1}y^1 + \cdots + {n \choose n-1}x^1 y^{n-1} + {n \choose n}x^0 y^n;$$

The sum is finite and stops after n+1 terms. 
```

Equivalently, making use of the sum notation,

$$(x+y)^n = \sum_{k=0}^n {n \choose k}x^{n-k}y^k = \sum_{k=0}^n {n \choose k}x^{k}y^{n-k}.$$ 

```{example}
Let us compute the binomial coefficients for polynomials of different degrees $n$:

- for $n=1$ 

\begin{align}
(x+y)^1 &= {1 \choose 0}x^1 y^0 +  {1 \choose 1}x^0 y^{1}\\  
&=  x +  y 
\end{align}

- for $n=2$ we have 

\begin{align}
(x+y)^2 &= {2 \choose 0}x^2 y^0 + {2 \choose 1}x^{1}y^1 +  {2 \choose 2}x^0 y^{2}  \\  
&=  x^2 +  2 x y +  y^{2} 
\end{align} 


- for $n=3$ we have 
\begin{align}
(x+y)^3 &= {3 \choose 0}x^3 y^0 + {3 \choose 1}x^{2}y^1 + \cdots + {3 \choose 2}x^1 y^{2} + {3 \choose 3}x^0 y^3 \\
&= y^3 + 3xy^2 + 3x^2y+x^3 
\end{align}
```

### Limits

The study of **limits** will be crucial in many tasks:

- The limit of a finite sum is an infinite series:
$$\lim_{n \to \infty} \sum_{i=1}^n x_i = \sum_{i=1 }^\infty x_i \nonumber$$
- The Exponential function, characterised as a limit:
$$ e^x = \lim_{n \rightarrow \infty} \left(1 + \frac{x}{n}\right)^n \nonumber$$
- The Limit of a Negative Exponential function: Let $\alpha >0$
$$\lim_{x \to \infty} {\alpha e^{-\alpha x}} = 0 \nonumber$$ 
- The Exponential function, characterised as an infinite series:
$$e^x = \sum_{i = 0}^{\infty} {x^i \over i!} = 1 + x + {x^2 \over 2!} + {x^3 \over 3!} + {x^4 \over 4!} + \dots$$


