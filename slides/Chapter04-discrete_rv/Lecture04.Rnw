\documentclass[smaller, handout]{beamer}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{epstopdf}
\usepackage{color}


\setcounter{MaxMatrixCols}{10}
\newtheorem{remark}{Remark}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{interpretation}{Interpretation}[section]
\newtheorem{goal}{Goal}[section]
\newtheorem{statement}{Statement}[section]
\newtheorem{aes}{Aim \& Scope}[section]
\newtheorem{exercise}{Exercise}[section]
\renewcommand{\Pr}{P}

\newcommand{\mbf}[1]{\mathbf{#1}}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}}
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{array}}
\newcommand{\ea}{\end{array}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\nn}{\nonumber}

\newenvironment{stepenumerate}{\begin{enumerate}[<+->]}{\end{enumerate}}
\newenvironment{stepitemize}{\begin{itemize}[<+->]}{\end{itemize} }
\newenvironment{stepenumeratewithalert}{\begin{enumerate}[<+-| alert@+>]}{\end{enumerate}}
\newenvironment{stepitemizewithalert}{\begin{itemize}[<+-| alert@+>]}{\end{itemize} }
\usetheme{Madrid}


\definecolor{Mygreen}{rgb}{0.0,0.5,0.0}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% GSEM COLORS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{darkGSEM}{RGB}{70,95,127}
\definecolor{darkGSEM2}{RGB}{40,80,150}
\definecolor{GSEM}{RGB}{96,121,153} % GSEM 10% lighter

%%% Global colors
\setbeamercolor*{palette primary}{use=structure,fg=white,bg=darkGSEM}
\setbeamercolor*{palette quaternary}{use=structure,fg=white,bg=darkGSEM!90}
\setbeamercolor{frametitle}{fg=white,bg=GSEM!80}

%%% TOC colors
\setbeamercolor{section in toc}{fg=darkGSEM}

%%% itemize colors
\setbeamertemplate{itemize items}[circle]
\setbeamercolor{itemize item}{fg=darkGSEM2}
\setbeamercolor{itemize subitem}{fg=darkGSEM2}
\setbeamercolor{itemize subsubitem}{fg=darkGSEM2}


%%% enumerate colors
\setbeamercolor{item projected}{fg=white,bg=GSEM}
\setbeamertemplate{enumerate item}{\insertenumlabel.}
\setbeamercolor{enumerate item}{fg=darkGSEM2}
\setbeamercolor{enumerate subitem}{fg=darkGSEM2}
\setbeamercolor{enumerate subsubitem}{fg=darkGSEM2}


\AtBeginSection[]
{
  \begin{frame}{\secname}
    %\frametitle{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<setup, include = FALSE>>=
knitr::opts_chunk$set(echo=FALSE,
                      out.width  = '0.5\\linewidth',
                      fig.align  = 'center',
                      fig.width = 4*1.5,
                      fig.height = 3*1.5)
@


\begin{document}

\title[S110015]{Probability 1}
\subtitle{Lecture 4: Discrete Random Variables - Part 1}
\author[Flores-Agreda, La Vecchia]{Dr. Daniel Flores-Agreda, \\[0.5em] \tiny{(based on the notes of Prof. Davide La Vecchia)}}
\date{Spring Semester 2021}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Objectives}
\begin{itemize}
\item Define the concept of a Random Variable
\bigskip
\item Explore the features of Discrete Random Variables
\bigskip
\begin{itemize}
\item Distribution and Probability Mass Function (PMF)
\bigskip
\item Cumulative Distribution Function (CDF)
\bigskip
\item Expectation and Variance
\end{itemize}
\bigskip
\item (If time allows it) Start presenting some important Discrete Distributions.
\end{itemize}
\end{frame}

\begin{frame}{Outline}
\tableofcontents
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{What is a Random Variable?}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{\secname}

  % Until now we considered probabilities of concrete events e.g.
  %
  % \begin{itemize}
  % \item events for card flip (e.g. the card may be `hearts or diamonds')
  % \item events associated with coin tosses (e.g. the coins may show two heads` $HH$')
  % \end{itemize}
  %
  % We've characterised these events as sets and
  %
  % \item[-] events defined as combinations of sets (an event in `$A\cup B^{c}$')
  % \item[-]...
  % \end{stepitemize}

  Let's say we have a Random Experiment with different outcomes.

  \begin{definition}[Informal]
  A \textbf{Random Variable} $X$ is a variable that takes on different
  \textbf{numerical values} according to the outcomes of a random experiment.\\

  The probability of the numerical values will result from the probabilities of
  the outcomes.
  \end{definition}

  To define a random variable, we need:

  \begin{stepenumerate}
  \item a list of \textbf{all possible numerical values}
  \item the \textbf{probability of each numerical value}
  \end{stepenumerate}
\end{frame}

\begin{frame}{\secname}
  \begin{example}[Rolling the dice - Again]
  \begin{itemize}
  \item Roll a single die, and record the number of dots on the top side.
  \item The list of all possible outcomes is the number shown on the die.
  \begin{itemize}
  \item i.e. the possible outcomes are 1, 2, 3, 4, 5 and 6
  \end{itemize}
  \item If we say each outcome is equally likely, then the probability of each outcome must be 1/6
  \end{itemize}
  \vspace{3cm}
  \end{example}
\end{frame}

\begin{frame}{\secname}
  \begin{example}[Flipping coins - Again]
  \begin{itemize}
  \item Flip a coin 10 times, and record the number of times T (tail) occurs
  \item The possible outcomes are
  \begin{equation*}
  \text{0, 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10}
  \end{equation*}
  \item For each number we associate a probability
  \vspace{2cm}
  \pause
  \item The probabilities are determined by the assumptions made about the
  coin flips, e.g.
  \begin{itemize}
    \item what is the probability of a `tail' on a single coin flip
    \item whether this probability is the same for every coin flip
    \item whether the 10 coin flips are `independent' of each other
  \end{itemize}
  \end{itemize}

  \end{example}
\end{frame}

\begin{frame}{\secname}
  \begin{example}
  \begin{stepitemize}
  \item Measure the time taken by school students to complete a test.
  \item Every student has a maximum of 2 hours to finish the test.
  \pause
  \item Let $X=$ completion time (in minutes).
  \item The possible values of the random variable $X$ are contained in the interval
  $$(0,120]=\{x:0<x\leq 120\}.$$
  \item We then need to associate probabilities with all events we may wish to
  consider, such as
  $$P\left(\{ X\leq 15\}\right) \quad  \text{or} \quad P\left(\{ X>60\}\right).$$
  \end{stepitemize}
  \end{example}
\end{frame}

\begin{frame}{\secname}
\framesubtitle{A more formal definition}
  \begin{itemize}
  \item Suppose we have:

  \begin{enumerate}[a.]
  \item[a.] A sample space $\color{Mygreen} S \color{black}$ ``for the events''
  \item[b.] A probability measure ($\color{Mygreen} Pr \color{black}$) ``for the events'' in $\color{Mygreen} S \color{black}$
  \end{enumerate}
  \pause
  \item Let $\color{blue} X \color{black}(\color{Mygreen} s \color{black})$ be a function that takes an element $\color{Mygreen}  s\in S\color{black}$ to a number $x$

  \end{itemize}
  \pause
  \begin{minipage}{1.0\textwidth}
  \begin{figure}[h!]
  \centering
  \includegraphics[scale=0.15]{img/charts.010.png}
  \end{figure}
  \end{minipage}
\end{frame}


\begin{frame}{\secname}

  \begin{example}[Rolling two dice]
  \textbf{Experiment:} We already know that the sample space $\color{Mygreen}S\color{black}$ is given by:

  \begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{img/C.pdf}
  \end{figure}

  \begin{itemize}
  \item let $i \in \{1, \dots, 6\}$ denote the outcome of Die 1
  \item let $j \in \{1, \dots, 6\}$ denote the outcome of Die 2
  \end{itemize}

  \begin{center}
  Every pair $(i,j) = \color{Mygreen}{s_{ij}} \in \color{Mygreen}{S}$ has a probability 1/36

   \medskip

  For every element or subset of $S$ we can compute a probability with $\color{Mygreen}{Pr(\cdot)}$
  \vspace{1cm}
  \end{center}
  \end{example}
\end{frame}

\begin{frame}{\secname}
  \begin{example}[continued]
  Let us define $X(\color{Mygreen}{s_{ij}})$ as the sum of the outcomes in both dice:
  \begin{eqnarray*}
  X(\color{Mygreen}{s_{ij}})= X(i,j)= i+j, & \textit{for} & i=1,...,6,  \textit{and  }   j=1,...,6
  \end{eqnarray*}

  \textbf{Consequences:}
  \begin{itemize}
  \item $X(\cdot)$ maps $\color{Mygreen} S \color{black}$ into $\color{blue}D\color{black}$.
  \item The sample space $\color{blue}D\color{black}$ is given by
  \begin{equation*}
  \color{blue}D=\left\{2,3,4,5,6,7,8,9,10,11,12\right\}\color{black}
  \end{equation*}
  \end{itemize}
  \vspace{-0.3cm}
  where, for instance:
  \begin{center}
  $\color{blue}2\color{black}$ is related to the pair $(1,1)$,

  $\color{blue}3\color{black}$ is related to the pairs $(1,2)$ and $(2,1)$, etc etc.

  Every element or subset of $\color{blue}{D}$ we can compute a probability with $\color{blue}{P(\cdot)}$
  \end{center}

  \end{example}
\end{frame}

\begin{frame}{\secname}
  \begin{example}[continued]
  \begin{itemize}
  \item To each element (event) in $\color{blue}D\color{black}$ we can attach a probability, using the probability of the corresponding event(s) in $S$. For instance,
  $$
  P(\color{blue}2\color{black})=Pr(1,1)=1/36, \quad \text{or} \quad P(\color{blue}3\color{black})=Pr(1,2)+Pr(2,1)=2/36.
  $$
  \vspace{-0.3cm}
  \item How about the $P(\color{blue}7\color{black})$?
  \begin{equation*}
  P(\color{blue}7\color{black})=Pr(3,4)+Pr(2,5)+Pr(1,6)+Pr(4,3)+Pr(5,2)+Pr(6,1)=6/36.
  \end{equation*}
  \vspace{-0.3cm}
  \item The latter equality can also be re-written as
  \[P(\color{blue}7\color{black})=2(Pr(3,4)+Pr(2,5)+Pr(1,6))=6 \ Pr(3,4),\]
  \vspace{-0.3cm}
  \end{itemize}
  \end{example}
  \begin{exercise}[]
  What is $P(\color{blue} 9 \color{black})$? What is $P(\color{blue} 13 \color{black})$? [Hint: does \color{blue} 13 \color{black} belong to $\color{blue} D \color{black}$?]
  \end{exercise}
\end{frame}

\begin{frame}{\secname}
\framesubtitle{A(n) even more formal characterisation}

  Let us formalise all these ideas:

  \begin{itemize}
  \item Let $D$ be the set of all values $x$ that can be obtained by $X\left(
  s\right) $, for all $s\in S$:%
  \begin{equation*}
  D=\left\{ x:x=X\left( s\right) ,\text{ }s\in S\right\}
  \end{equation*}
  \item $D$ is a \textbf{list of all possible numbers $x$} that can be obtained, and
  thus is a \textbf{sample space for $X$}. \textit{Remark that the random variable is $X$
  while $x$ represents its realization (non random).}
  \item $D$ can be either an \textbf{uncountable interval}
  \begin{itemize}
  \item $X$ is a \textbf{continuous} random variable, or
  \end{itemize}
  \item $D$ can be \textbf{discrete} or \textbf{countable}
  \begin{itemize}
  \item $X$ is a \textbf{discrete} random variable
  \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{\secname}

Moreover, because $\color{blue}{P}$ is defined from $\color{Mygreen}{Pr}$, it is also {a probability measure} on $\color{blue}{D}$.
\pause
For each $A$:
  \begin{equation*}
  \color{blue}P\color{black}\left( A\right) = \color{Mygreen} {Pr} \color{black}( \left\{ s\in S :X\left(  s  \right) \in
  A\right\})
  \end{equation*}
  where $\color{blue}P \color{black}$ and $\color{Mygreen} Pr \color{black}$ stand for ``probability'' on $\color{blue} D \color{black}$
  and on $\color{Mygreen} S \color{black}$, respectively.
  Hence:
  \begin{enumerate}

  \item $P \left( A\right) \geq 0$
  \vspace{0.15cm}
  \item $\color{blue}P \left( D\right) \color{black}=\color{Mygreen}Pr (\left\{ s\in S:X\left( s\right)
  \in D\right\}) =Pr \left( S\right) =1 \color{black}$
  \vspace{0.15cm}
  \item If $A_{1},A_{2},A_{3}...$ is a sequence of events
  such that $$A_{i}\cap A_{j}=\varnothing $$ for all $i\neq j$ then
  $$\color{blue}P \color{black} \left(
  \bigcup _{i=1}^{\infty }A_{i}\right) =\sum_{i=1}^{\infty } \color{blue} P  \color{black} \left(
  A_{i}\right) . $$
  \end{enumerate}
  (From now on, we'll be dropping the colors.)
\end{frame}%


\begin{frame}{\secname}
  \begin{example}[Geometric random variable]
  \begin{footnotesize}
  \textbf{Experiment:} rolling a die until a 6 appears.
  \begin{itemize}
  \item Let $X=$ ``number of rolls until we get a 6''
  \item $D = \{1, 2, 3,\ldots,n, \ldots \} \equiv \mathbb{N}$.
  \end{itemize}
  \begin{align*}
  P(\{X=1\}) &=Pr(\text{`6' on the 1st roll})= \frac{1}{6}\\
  P(\{X=2\})&=Pr \left( \text{no `6' on the 1st roll and `6' on the 2nd roll}\right) =\frac{5}{6}\cdot \frac{1}{6}=\frac{5}{36}\\
  P(\{X=3\})&=Pr \left( \text{no `6' on either the 1st nor 2nd roll and `6' on the third roll}\right)\\
  &=\frac{5}{6}\cdot \frac{5}{6}\cdot \frac{1}{6}=\frac{25}{216}\\
  &\vdots \\
  P(\{X=n\})&=Pr( \text{no `}6\text{' on the first }n-1\text{ rolls and '6' on the last roll})\\
  &=\left(\frac{5}{6}\right)^{n-1}\cdot \frac{1}{6} \\
  &\vdots
  \end{align*}
  \end{footnotesize}
  \end{example}
\end{frame}

\begin{frame}{\secname}
\begin{example}[continued]

<<include = FALSE, message = FALSE, echo = FALSE>>=
library(tidyverse)
library(hrbrthemes)
sim_geom <-
  rgeom(10000000,1/6) %>%
  as_tibble %>%
  count(value) %>%
  mutate(p = n/sum(n)) %>%
  rename(x = value)
@

<<echo = FALSE, out.width = '0.5\\linewidth', fig.align = "right" >>=
sim_geom %>%
  filter(x <= 50) %>%
  ggplot() +
  geom_col(aes(x, p), fill = "lightblue") +
  theme_minimal() +
  labs(title = "Geometric distribution")
@
\end{example}
\end{frame}

\begin{frame}{\secname}
  \begin{example}[continued]
  \begin{footnotesize}
  \begin{stepitemize}

  \item Rather than list the possible values of $X$ along with the associated
  probabilities in a table, we can provide a formula that gives the required
  probabilities.

  \begin{equation*}
  P(\left\{ X=n \right\})=\left(\frac{5}{6}\right)^{n-1}\frac{1}{6}\quad\text{for}
  \quad n=1,2,\ldots
  \end{equation*}
  \end{stepitemize}
  \end{footnotesize}
  \end{example}
  \begin{exercise}
  Show that
  \begin{equation*}
  \sum_{n=1}^\infty\left(\frac{5}{6}\right)^{n-1}\frac{1}{6}=1.
  \end{equation*}
  \end{exercise}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discrete random variables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}{\secname}
% \begin{figure}[h!]
%   \centering
%   \includegraphics[scale=0.20]{../../book/img/fun/EC_Latin.png}
%   \end{figure}
% \end{frame}


\begin{frame}{\secname}
  Discrete random variables are often associated with the process of counting.

  More generally:
  \begin{definition}[Probability of a Discrete Random Variable]
  \begin{footnotesize}
  Suppose $X$ can take the values $x_{1},x_{2},x_{3},\ldots ,x_{n}$.

  The probability of $x_{i}$ is $$p_{i}= P(\left\{ X=x_i\right\})$$

  and we must have $p_{1}+p_{2}+p_{3}+\cdots +p_{n}=1$ and all $p_{i}\geq 0$. These probabilities may be put in a table%
  \begin{equation*}
  \begin{tabular}{|c|c|}
  \hline
  $x_i $ & $P(\left\{ X=x_i\right\}) $ \\ \hline\hline
  $x_{1}$ & $p_{1}$ \\ \hline
  $x_{2}$ & $p_{2}$ \\ \hline
  $x_{3}$ & $p_{3}$ \\ \hline
  $\vdots $ & $\vdots $ \\ \hline
  $x_{n}$ & $p_{n}$ \\ \hline\hline
  Total & $1$ \\ \hline
  \end{tabular}
  \end{equation*}
  \end{footnotesize}
  \end{definition}
\end{frame}

\begin{frame}{\secname}
  \begin{itemize}
  \item For a \textbf{discrete random variable $X$}, any table listing all
  possible nonzero probabilities provides the entire \textbf{Probability
  Distribution}.
  \pause
  \item The \textbf{probability mass function} $p(a)$ of $X$ is defined by
  $$ p_a = p(a)= P(\{X=a \})$$
  and this is positive for at most a countable number of values of $a$.
  \end{itemize}
  \pause
  For instance,
  $p_{1} = P(\left\{ X=x_1\right\})$, $p_{2} = P(\left\{ X=x_2\right\})$, and so on.
  That is, if $X$ must assume
  one of the values $x_1,x_2,...$, then
  \bea
   p(x_i) \geq 0 & \text{for \ \ } i=1,2,... \nn \\
   p(x) = 0 & \text{otherwise.}
  \eea

  Clearly, we must have
  $$\sum_{i=1}^{\infty} p(x_i) = 1.$$
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cumulative Distribution Function}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{\secname}
\begin{footnotesize}
  The \textbf{Cumulative Distribution Function} (\textbf{CDF}) is a
  table listing the values that $X$ can take, along with
  $$
  F_X(a) = P \left(\{ X\leq a\}\right)= \sum_{\text{all \ \ } x \leq a } p(x).
  $$
  If the random variable $X$ takes on values $x_{1},x_{2},x_{3},\ldots .,x_{n}$ \emph{listed in
  increasing order } $
  x_{1}<x_{2}<x_{3}<\cdots <x_{n}
  $, the CDF is a step function, that it its value is constant in the intervals $(x_{i-1},x_i]$ and takes a step/jump of size $p_i$
  at each $x_i$:
  \vspace{0.1cm}

  \begin{equation*}
  \begin{tabular}{|c|c|}
  \hline
  $x_i $ & $F_X(x_i) =P \left(\{ X\leq x_i\}\right) $ \\ \hline\hline
  $x_{1}$ & $p_{1}$ \\ \hline
  $x_{2}$ & $p_{1}+p_{2}$ \\ \hline
  $x_{3}$ & $p_{1}+p_{2}+p_{3}$ \\ \hline
  $\vdots $ & $\vdots $ \\ \hline
  $x_{n}$ & $p_{1}+p_{2}+\cdots +p_{n}=1$ \\ \hline
  \end{tabular}
  \end{equation*}
  \vspace{0.2cm}
  \end{footnotesize}
\end{frame}

\begin{frame}{\secname}
  \begin{example}
  \noindent
  \begin{center}
  \begin{tabular}{c|cccc|c}
  $x_i$ & 0 & 1 & 2 & 3 & Tot \\ \hline
  $P(\{X=x_i\})$ & 4/35 & 18/35 & 12/35 & 1/35 & 1 \\
  $P(\{X\leq x_i\})$ & 4/35 & 22/35 & 34/35 & 35/35 &
  \end{tabular}
  \end{center}
  \bigskip
  $$F_X(x) = \left\{
  \begin{array}{ll}
  0 & x<0 \\
  4/35 & 0 \leq x < 1\\
  22/35 & 1 \leq x < 2\\
  34/35 & 2 \leq x < 3 \\
  1 & x \geq 3.
  \end{array} \right.$$
  \end{example}
\end{frame}


\begin{frame}{\secname}
  \begin{example}[continued]
  ... or graphically, you get \textit{a step function} ...
  \begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth,height=0.5\textheight]{img/repartbis.pdf}
  \end{figure}
  \end{example}
\end{frame}



\begin{frame}{\secname}
  \begin{remark}
  Suppose $a\leq b$. Then, because the event $\{X\leq a \}$ is contained in the
  event $\{X\leq b \}$, namely
  $$
  \{X\leq a \} \subseteq \{X\leq b \},
  $$
   it follows that
   $$F_X(a) \leq F_X(b),
   $$
  so, the probability of the former is less than or equal to the probability
  of the latter. \\
  \vspace{0.5cm}
  \begin{center}
  \textbf{In other words, $F_X(x)$ is a nondecreasing function of $x$.}
  \end{center}
  \end{remark}
\end{frame}

\begin{frame}{\secname}
  \begin{definition} [Quantiles]

  The CDF can be inverted to define the value $x$ of $X$ that corresponds to a given probability $\alpha$, namely $\alpha = P (X \leq x )$, for $\alpha \in [0,1]$.

  \medskip

  The inverse CDF $F_X^{-1}(\alpha)$ or \textbf{quantile of order $\alpha$}, $Q(\alpha)$, is the smallest realisation of $X$ associated to a CDF greater or equal to $\alpha$

  In formula, the $\alpha$-quantile $Q(\alpha)$ is the smallest number satisfying:

  $$
  F_X [F^{-1}_X (\alpha)] = P[X \leq \underbrace{F^{-1}_X (\alpha)}_{Q(\alpha)}] \geq \alpha, \quad \text{for} \quad \alpha\in[0,1].
  $$

  By construction, a quantile of a discrete random variable is a realization of $X$\footnote{More to come in future chapters}
  \end{definition}
\end{frame}

\begin{frame}{\secname}
\begin{example}[cont'd, graphically]
  \begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth,height=0.95\textheight, angle = 90]{img/Quantiles_Discr.pdf}
  \end{figure}
  ...calling (only for this slide) $R$ the rv, $r$ its realizations and $F_R(r)$ its CDF at $r$...
  \end{example}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Distributional Summaries}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{\secname}
For a discrete random variable, it is useful to provide values that describe some
  \textbf{attributes} of its distribution
  \pause
  \begin{definition}
  \begin{footnotesize}
  The \textbf{Expectation}, a.k.a. \textbf{Expected} or \textbf{Mean} value, of the
  distribution is (roughly speaking) its \emph{center}.
  \begin{equation*}
  E\left[ X\right] =p_{1}x_{1}+p_{2}x_{2}+\cdots + p_{n}x_{n} = \sum_{i=1}^{n} p_i x_i,
  \end{equation*}%
  and constitutes a measure of \emph{location}.
  \end{footnotesize}
  \end{definition}
  \pause
  <<fig.align = 'left'>>=
  library(hrbrthemes)
  dist1 <- tibble(
    x = c(0, 1, 2, 3),
    p = c(4/35, 18/35, 12/35, 1/35),
    p_txt = c('4/35', '18/35', '12/35', '1/35')
  ) %>%
    mutate(px_i = p*x,
           sd_i = p*(x-sum(px_i))^2
           )

  sum_px_txt = paste(paste(dist1$p_txt,dist1$x,sep="*"), collapse = "+")

  dist1 %>%
    ggplot()+
    geom_col(aes(x = x, y=p), fill = "lightblue", width =0.4)+
    theme_minimal() +
    geom_vline(mapping = aes(xintercept = sum(px_i)), color = "green", lwd=2)+
    geom_text(aes(x =x , y=p+0.02, label = str_glue("{p_txt}"))) +
    labs(title = str_glue("Distribution 1:E(X)={round(sum(dist1$px_i), digits = 4)}"),
         x = "x", y = "P(X=x)")
  @
\end{frame}

\begin{frame}{\secname}
  \begin{definition}
  \begin{footnotesize}
   The \textbf{Square root of the Variance}, or \textbf{Standard
  Deviation,} of the distribution
  \begin{eqnarray*}
  s.d\left( X\right) &=&\sqrt{Var\left( X\right) } \\
  &=&\sqrt{p_{1}\left( x_{1}-E\left[ X\right] \right) ^{2}+p_{2}\left( x_{2}-E%
  \left[ X\right] \right) ^{2}+\cdots + p_{n}\left( x_{n}-E\left[ X\right]
  \right)^{2}}
  \end{eqnarray*}
  is a measure of \emph{spread} a.k.a. `variability' or `dispersion').
  \end{footnotesize}
  \end{definition}
\pause
<<fig.width = 8*1.5, out.width='0.95\\linewidth', fig.align='left'>>=
library(patchwork)
dist2 <- tibble(
  x = c(0, 1, 2, 3),
  p = c(18/35,4/35, 1/35, 12/35),
  p_txt = c('18/35','4/35', '1/35', '12/35')
)%>%
  mutate(px_i = p*x,
         sd_i = p*(x-sum(px_i))^2)

dist1_sd <-
  dist1 %>%
  ggplot()+
  geom_col(aes(x = x, y=p), fill = "lightblue", width =0.4)+
  geom_vline(mapping = aes(xintercept = sum(px_i)), color = "green", lwd=2)+
  geom_text(aes(x =x , y=p+0.02, label = str_glue("{p_txt}"))) +
  labs(
    title = str_glue("Distribution 1: E(X)={round(sum(dist1$px_i), digits = 4)}, s.d(X)={round(sqrt(sum(dist1$sd_i)), digits = 4)}"),
    x = "x",
    y = "P(X=x)"
    )

dist2_sd <-
  dist2 %>%
  ggplot()+
  geom_col(aes(x = x, y=p), fill = "lightgreen", width =0.4)+
  geom_vline(mapping = aes(xintercept = sum(px_i)), color = "green", lwd=2)+
  geom_text(aes(x =x , y=p+0.02, label = str_glue("{p_txt}"))) +
  labs(
    title = str_glue("Distribution 2: E(X)={round(sum(dist2$px_i), digits = 4)}, s.d(X)={round(sqrt(sum(dist2$sd_i)), digits = 4)}"),
    x = "x",
    y = "P(X=x)",
    caption = "Distribution 2 is 'more spread' than Distribution 1"
    )

dist1_sd + dist2_sd  & theme_minimal()
@
\end{frame}

\begin{frame}{\secname}%
  If $X$ is a discrete random variable and $a$ is any real number, then:
  \begin{align*}
  E\left[  \alpha X\right]  &=\alpha E\left[ X\right]\\
  E\left[  \alpha+X\right]  &=\alpha+E\left[ X\right] \\
  Var\left(\alpha X\right)  &=\alpha^{2}Var\left( X\right)\\
  Var\left( \alpha+X\right) &=Var\left( X\right)
  \end{align*}
  \pause
  \begin{exercise}
  \begin{footnotesize}
  Let us very the first property:
  $E\left[ \alpha X \right] =\alpha E\left[ X\right].$
  From the intro lecture we know that, for every $\alpha_i \in \mathbb{R}$,
  $$\sum_{i=1}^{n} \alpha_i X_{i} = \alpha_1 X_1 + \alpha_2 X_2 +....+ \alpha_n X_n.$$ So, the
  required result follows as a special case, setting $\alpha_i= \alpha$, for every $i$, and applying the definition of Expectation. Verify this and the other properties as an exercise. [Hint: set $\alpha_i = \alpha p_i$.]
  \end{footnotesize}
  \end{exercise}
\end{frame}

\begin{frame}{\secname}
  \begin{definition}
  Consider two discrete random variables
  % \footnote{Technically speaking, $X$ and $Y$ should be defined on the same probability space .} $X$ and $Y$.
  \medskip
  Then, $X$ and $Y$ are \textbf{independent} if%
  \begin{equation*}
  P \left(\left\{ \ X=x\right\} \cap \left\{ Y=y\right\} \right) =P \left(\{
  X=x\}\right) \cdot P \left(\{ Y=y \}\right)
  \end{equation*}
  for all values $x$ that $X$ can take and all values $y$ that $Y$ can take.
  \end{definition}
\end{frame}%


\begin{frame}{\secname}%
    \begin{stepitemize}
  \item If $X$ and $Y$ are two discrete random variables, then%
  \begin{equation*}
  E\left[ X+Y\right] =E\left[ X\right] +E\left[ Y\right]
  \end{equation*}

  \item If $X$ and $Y$ are also \emph{independent}, then%
  \begin{equation}
  Var\left( X+Y\right) =Var\left( X\right) +Var\left( Y\right) \label{Eq. Var}
  \end{equation}

  \begin{remark}
  Note that Eq. (\ref{Eq. Var})  does not (typically) hold if $X$ and $Y$ are NOT independent---more to come on this later on...
  \end{remark}
  \end{stepitemize}
\end{frame}

\begin{frame}{\secname}%

  Recall that the expectation of X was defined as
  \begin{equation*}
  E\left[ X\right] = \sum_{i=1}^{n} p_i x_i
  \end{equation*}

  Now, suppose we are interested in a function $m$ of the random variable $X$, say $m(X)$. We define
  \begin{equation*}
  E\left[ m\left( X\right) \right] =p_{1}m\left( x_{1}\right) +p_{2}m\left(
  x_{2}\right) +\cdots p_{n}m\left( x_{n}\right).
  \end{equation*}
  \begin{remark}
  \begin{footnotesize}
  Notice that the variance is a special case of expectation where,
  \begin{equation*}
  m(X)=(X-E\left[ X\right] )^{2}.
  \end{equation*}
  Indeed,
  \begin{equation*}
  Var\left( X\right) =E\left[ (X-E\left[ X\right] )^{2}\right].
  \end{equation*}
  \end{footnotesize}
  \vspace{-0.3cm}
  \end{remark}

  \begin{exercise}
  \begin{footnotesize}
  Show that
  \begin{equation*}
  Var\left( X\right) =E\left[ X^{2}\right] -E\left[ X\right] ^{2}.
  \end{equation*}
  \end{footnotesize}
  \vspace{-0.3cm}
  \end{exercise}
\end{frame}

\begin{frame}{Wrap-up}
  \begin{itemize}
  \item A \textbf{Random Variable} \emph{maps} events from the \textbf{Sample Space of the Events} onto \textcolor{blue}{a set of numerical values}.
  \medskip
  \item This entails a new sample space and a new probability measure $\color{blue}P(\cdot)$
  \medskip
  \item When the mapping is onto a \textbf{countable set} the Random Variable is \textbf{Discrete}.
  \medskip
  \item A Discrete Random Variable is endowed of a \textbf{Probability Distribution}, which is a \textbf{listing of the probabilities of each value}.
  \medskip
  \item This Distribution can be displayed as a \textbf{table} or as \textbf{functions} : Probability Mass Function (PMF) or Cumulative Distribution Function (CDF).
  \medskip
  \item A Distribution has a ``gravity center'' The \textbf{Expectation}.
  \item A Distribution has a measure of ``spread'' with respect to this center:
  the Standard Deviation, which depends on the \textbf{Variance}.
  \end{itemize}
\end{frame}


\begin{frame}
  \begin{center}
  \Large{Thank You for your Attention!}
  \pause
  \Large{``See you'' Next Week}
  \end{center}
\end{frame}

\end{document}
